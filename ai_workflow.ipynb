{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T05:00:45.912550Z",
     "start_time": "2020-01-04T05:00:45.908112Z"
    }
   },
   "source": [
    "<div style=\"text-align:center; font-size:24px\">Capstone AI Workflow</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem\n",
    "\n",
    "Given a MNIST dataset, we find the best model to this topic: Recognition handwritten digits. \n",
    "\n",
    "The MNIST database is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets.\n",
    "\n",
    "Please see more detail in [1].\n",
    "\n",
    "**References**\n",
    "\n",
    "[1] The MNIST database of handwritten digits (http://yann.lecun.com/exdb/mnist/) \n",
    "\n",
    "[2] https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "[3] https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py\n",
    "\n",
    "[4] https://towardsdatascience.com/unit-testing-and-logging-for-data-science-d7fb8fd5d217\n",
    "\n",
    "[5] https://github.com/CoreyMSchafer/code_snippets/blob/master/Decorators/decorators.py\n",
    "\n",
    "[6] https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html?highlight=mnist\n",
    "\n",
    "[7] https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "\n",
    "# Data Setttings\n",
    "\n",
    "In [1], the authors described the MNIST corpus as follows:\n",
    "\n",
    "+ train-images-idx3-ubyte.gz:  training set images (9912422 bytes)\n",
    "+ train-labels-idx1-ubyte.gz:  training set labels (28881 bytes)\n",
    "+ t10k-images-idx3-ubyte.gz:   test set images (1648877 bytes)\n",
    "+ t10k-labels-idx1-ubyte.gz:   test set labels (4542 bytes) \n",
    "\n",
    "The images are normalized in 28x28, that are preserved the aspect ratio from original black and white images (NIST, 20x20), by computing the center of mass of the pixels and translating the image to position this point at the center of 28x28 field.\n",
    "\n",
    "\n",
    "# Approach\n",
    "\n",
    "## Baseline\n",
    "\n",
    "In this baseline, we choose random result, that is in [0, 9], for each test vector input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:30:50.234534Z",
     "start_time": "2020-01-04T19:30:49.309163Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import logging\n",
    "from functools import wraps\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "COL_NAME_METHOD_NAME = \"Testcase\"\n",
    "COL_NAME_ACCURACY = \"Accuracy\"\n",
    "COL_NAME_DURATION = \"Duration\"\n",
    "\n",
    "COL_NAMES = [COL_NAME_METHOD_NAME, COL_NAME_ACCURACY]\n",
    "\n",
    "\n",
    "class decorator_class(object):\n",
    "\n",
    "    def __init__(self, original_function):\n",
    "        self.original_function = original_function\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        print('call method before {}'.format(self.original_function.__name__))\n",
    "        self.original_function(*args, **kwargs)\n",
    "\n",
    "\n",
    "def logger(original_func):\n",
    "    logging.basicConfig(filename='{}.log'.format(original_func.__name__),\n",
    "                        level=logging.INFO)\n",
    "\n",
    "    @wraps(original_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        logging.info('Ran with args: {}, and kwargs: {}'.format(args, kwargs))\n",
    "        return original_func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def cls_timer(original_func):\n",
    "    @wraps(original_func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time.time()\n",
    "        result = original_func(*args, **kwargs)\n",
    "        t2 = time.time() - t1\n",
    "        print('{} ran in: {} sec'.format(original_func.__name__, t2))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def get_dataset_mnist():\n",
    "    # new version: fetch_openml; old version: fetch_mldata('MNIST original')\n",
    "    mnist = fetch_openml('mnist_784')\n",
    "    X = mnist.data.astype('float64')\n",
    "    y = mnist.target\n",
    "    return (X, y)\n",
    "\n",
    "class Normalize(object):\n",
    "    def normalize(self, X_train, X_test):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_test  = self.scaler.transform(X_test)\n",
    "        return (X_train, X_test)\n",
    "\n",
    "    def inverse(self, X_train, X_test):\n",
    "        X_train = self.scaler.inverse_transform(X_train)\n",
    "        X_test  = self.scaler.inverse_transform(X_test)\n",
    "        return (X_train, X_test)\n",
    "\n",
    "def split(X, y, splitRatio):\n",
    "    X_train = X[:splitRatio]\n",
    "    y_train = y[:splitRatio]\n",
    "    X_test = X[splitRatio:]\n",
    "    y_test = y[splitRatio:]\n",
    "    return (X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "class algo_logistic_regression(object):\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = X_train, y_train, X_test, y_test\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def fit(self):\n",
    "        normalizer = Normalize()\n",
    "        self.X_train, self.X_test = normalizer.normalize(self.X_train, self.X_test)\n",
    "        train_samples = self.X_train.shape[0]\n",
    "        self.classifier = LogisticRegression(\n",
    "            C=50. / train_samples,\n",
    "            multi_class='multinomial',\n",
    "            penalty='l1',\n",
    "            solver='saga',\n",
    "            tol=0.1,\n",
    "            class_weight='balanced')\n",
    "        self.classifier.fit(self.X_train, self.y_train)\n",
    "        self.train_y_predicted = self.classifier.predict(self.X_train)\n",
    "        self.train_accuracy = round(np.mean(self.train_y_predicted.ravel() == self.y_train.ravel()) * 100, 2)\n",
    "        self.train_confusion_matrix = confusion_matrix(self.y_train, self.train_y_predicted)\n",
    "\n",
    "        return self.train_accuracy\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def predict(self):\n",
    "        self.test_y_predicted = self.classifier.predict(self.X_test)\n",
    "        self.test_accuracy = round(np.mean(self.test_y_predicted.ravel() == self.y_test.ravel()) * 100, 2)\n",
    "        self.test_confusion_matrix = confusion_matrix(self.y_test, self.test_y_predicted)\n",
    "        self.report = classification_report(self.y_test, self.test_y_predicted)\n",
    "        print(\"Classification report for classifier Logistic Regression:\\n %s\\n\" % (self.report))\n",
    "        return self.test_accuracy\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def predict_dummy(self):\n",
    "        \"\"\"Dummy prediction with baseline obtained result between [0,9]\n",
    "        \"\"\"\n",
    "        self.test_y_predicted_baseline = np.array([str(x) for x in np.random.randint(10, size=len(self.X_test))])\n",
    "        self.test_accuracy_baseline = round(np.mean(self.test_y_predicted_baseline.ravel() == self.y_test.ravel() * 100), 2)\n",
    "        self.test_confusion_matrix_baseline = confusion_matrix(self.y_test, self.test_y_predicted_baseline)\n",
    "        self.report_baseline = classification_report(self.y_test, self.test_y_predicted_baseline)\n",
    "        print(\"Classification report for classifier - Baseline:\\n %s\\n\" % (self.report_baseline))\n",
    "        return self.test_accuracy_baseline\n",
    "\n",
    "\n",
    "class algo_multilayer_perceptron(object):\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def __init__(self, X_train, y_train, X_test, y_test):\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = X_train, y_train, X_test, y_test\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def fit(self):\n",
    "        normalizer = Normalize()\n",
    "        self.X_train, self.X_test = normalizer.normalize(self.X_train, self.X_test)\n",
    "        train_samples = self.X_train.shape[0]\n",
    "        self.classifier = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                                        solver='sgd', verbose=10, random_state=1,\n",
    "                                        learning_rate_init=.1)\n",
    "        self.classifier.fit(self.X_train, self.y_train)\n",
    "        self.train_y_predicted = self.classifier.predict(self.X_train)\n",
    "        self.train_accuracy = round(np.mean(self.train_y_predicted.ravel() == self.y_train.ravel()) * 100, 2)\n",
    "        self.train_confusion_matrix = confusion_matrix(self.y_train, self.train_y_predicted)\n",
    "        return self.train_accuracy\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def predict(self):\n",
    "        self.test_y_predicted = self.classifier.predict(self.X_test)\n",
    "        self.test_accuracy = round(np.mean(self.test_y_predicted.ravel() == self.y_test.ravel()) * 100, 2)\n",
    "        self.test_confusion_matrix = confusion_matrix(self.y_test, self.test_y_predicted)\n",
    "        self.report = classification_report(self.y_test, self.test_y_predicted)\n",
    "        print(\"Classification report for classifier Logistic Regression:\\n %s\\n\" % (self.report))\n",
    "        return self.test_accuracy\n",
    "\n",
    "    @logger\n",
    "    @cls_timer\n",
    "    def predict_dummy(self):\n",
    "        \"\"\"Dummy prediction with baseline obtained result between [0,9]\n",
    "        \"\"\"\n",
    "        self.test_y_predicted_baseline = np.array([str(x) for x in np.random.randint(10, size=len(self.X_test))])\n",
    "        self.test_accuracy_baseline = round(np.mean(self.test_y_predicted_baseline.ravel() == self.y_test.ravel() * 100), 2)\n",
    "        self.test_confusion_matrix_baseline = confusion_matrix(self.y_test, self.test_y_predicted_baseline)\n",
    "        self.report_baseline = classification_report(self.y_test, self.test_y_predicted_baseline)\n",
    "        print(\"Classification report for classifier - Baseline:\\n %s\\n\" % (self.report_baseline))\n",
    "        return self.test_accuracy_baseline\n",
    "\n",
    "# Main functions\n",
    "\n",
    "### Unittest for logger and cls_timer\n",
    "# @logger\n",
    "# @cls_timer\n",
    "# def display_info(name, age):\n",
    "#     time.sleep(1)\n",
    "#     print('display_info ran with arguments ({}, {})'.format(name, age))\n",
    "# display_info('Tom', 22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:31:22.213354Z",
     "start_time": "2020-01-04T19:30:50.236300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (70000, 784) (70000,)\n",
      "__init__ ran in: 2.1457672119140625e-06 sec\n",
      "fit ran in: 15.04773211479187 sec\n",
      "------------------------------------------------------------------------\n",
      "Classification report for classifier - Baseline:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.12      0.11       980\n",
      "           1       0.11      0.10      0.10      1135\n",
      "           2       0.12      0.11      0.11      1032\n",
      "           3       0.10      0.10      0.10      1010\n",
      "           4       0.11      0.11      0.11       982\n",
      "           5       0.09      0.10      0.09       892\n",
      "           6       0.09      0.10      0.10       958\n",
      "           7       0.10      0.10      0.10      1028\n",
      "           8       0.10      0.10      0.10       974\n",
      "           9       0.11      0.11      0.11      1009\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.10      0.10      0.10     10000\n",
      "weighted avg       0.10      0.10      0.10     10000\n",
      "\n",
      "\n",
      "predict_dummy ran in: 0.16110515594482422 sec\n",
      "\n",
      "Test Accuracy - Baseline: 0.0 \n",
      "\n",
      "Test confusion matrix - Baseline:\n",
      "[[113  97  94 102 102  94 108 103  83  84]\n",
      " [118 108 113 100 119 119 115 128  98 117]\n",
      " [104 102 115 112 102  98 107  87 103 102]\n",
      " [ 95  97  92 101  98 107 103  96 106 115]\n",
      " [104  98 102 104 106  96 107  90  88  87]\n",
      " [ 96  85  75  92  90  86  92  92  89  95]\n",
      " [ 94 113  85  87  83 106  96 102 113  79]\n",
      " [ 97  98  99 111  98 108 101 102 108 106]\n",
      " [ 97  97  90  93  94 100  96  96 100 111]\n",
      " [ 89  97 112 110 103  82  94 103 109 110]]\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Train Accuracy - Logistic Regression: 72.65 \n",
      "\n",
      "Train confusion matrix:\n",
      "[[5465    7   31   25   45   18  196   59   71    6]\n",
      " [   3 6481   98   45    2   29   25   40   18    1]\n",
      " [ 339  478 3603  165  286   19  689  210  115   54]\n",
      " [ 143  255  224 4521   60  256  102  146  241  183]\n",
      " [ 106  136   23   51 4521  331  198  150   84  242]\n",
      " [ 414  214   85 1032  402 2261  292  408  186  127]\n",
      " [ 189   96   91   53  157  230 5038   29   35    0]\n",
      " [ 209  203  183   38  142   16   25 5108   50  291]\n",
      " [  72  770  181  692   72  191  352   48 3317  156]\n",
      " [ 167  182   48  257  625  265  120  782  227 3276]]\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Classification report for classifier Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       980\n",
      "           1       0.76      0.96      0.85      1135\n",
      "           2       0.80      0.62      0.69      1032\n",
      "           3       0.67      0.76      0.71      1010\n",
      "           4       0.71      0.76      0.74       982\n",
      "           5       0.62      0.41      0.50       892\n",
      "           6       0.69      0.84      0.76       958\n",
      "           7       0.74      0.81      0.77      1028\n",
      "           8       0.77      0.60      0.67       974\n",
      "           9       0.77      0.57      0.65      1009\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.73      0.73      0.72     10000\n",
      "weighted avg       0.73      0.73      0.72     10000\n",
      "\n",
      "\n",
      "predict ran in: 0.35492682456970215 sec\n",
      "\n",
      "Test Accuracy - LogisticRegression: 73.18 \n",
      "\n",
      "Test confusion matrix:\n",
      "[[ 922    2    2    3    3    1   36    4    7    0]\n",
      " [   0 1090   18   11    0    0    5    5    6    0]\n",
      " [  66   94  636   30   40    2   98   33   25    8]\n",
      " [  21   30   31  766    7   44   18   32   38   23]\n",
      " [  15   20    2    8  748   58   45   26   18   42]\n",
      " [  61   27   12  185   74  368   58   65   23   19]\n",
      " [  36   10   16    8   26   42  804    8    7    1]\n",
      " [  23   43   47    6   16    3    6  830   10   44]\n",
      " [  14   98   27  100   10   33   70    9  583   30]\n",
      " [  21   29    9   33  129   43   28  108   38  571]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "\n",
    "# Logistic Regression\n",
    "X,y = get_dataset_mnist()\n",
    "print('MNIST:', X.shape, y.shape)\n",
    "\n",
    "splitRatio = 60000\n",
    "X_train, y_train, X_test, y_test = split(X, y, splitRatio)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "t0 = time.time()\n",
    "np.random.seed(31337)\n",
    "ta = algo_logistic_regression(X_train, y_train, X_test, y_test)\n",
    "train_accuracy = ta.fit()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"-\" * 72)\n",
    "test_accuracy_baseline = ta.predict_dummy()\n",
    "print()\n",
    "print('Test Accuracy - Baseline:', test_accuracy_baseline,'\\n')\n",
    "print(\"Test confusion matrix - Baseline:\\n%s\\n\" % ta.test_confusion_matrix_baseline)\n",
    "\n",
    "# Add Test Accuracy - Baseline\n",
    "method_name = \"Baseline for Test-set (Random result)\"\n",
    "values.append((method_name, test_accuracy_baseline))\n",
    "# ----------------------------------------------------------------------------\n",
    "print()\n",
    "print(\"-\" * 72)\n",
    "print(\"Train Accuracy - Logistic Regression:\", train_accuracy,'\\n')\n",
    "print(\"Train confusion matrix:\\n%s\\n\" % ta.train_confusion_matrix)\n",
    "\n",
    "# Add Train Accuracy\n",
    "method_name = \"Train Accuracy - Logistic Regression\"\n",
    "values.append((method_name, train_accuracy))\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"-\" * 72)\n",
    "test_accuracy = ta.predict()\n",
    "print()\n",
    "print('Test Accuracy - LogisticRegression:', test_accuracy,'\\n')\n",
    "print(\"Test confusion matrix:\\n%s\\n\" % ta.test_confusion_matrix)\n",
    "\n",
    "# Add Test Accuracy\n",
    "method_name = \"Logistic Regression\"\n",
    "values.append((method_name, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:31:22.737371Z",
     "start_time": "2020-01-04T19:31:22.215762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity with L1 penalty: 93.66%\n",
      "Example run in 15.799 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "sparsity = np.mean(ta.classifier.coef_ == 0) * 100\n",
    "\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "\n",
    "coef = ta.classifier.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Class %i' % i)\n",
    "plt.suptitle('Classification vector for Logistic Regression')\n",
    "\n",
    "run_time = time.time() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:31:36.414500Z",
     "start_time": "2020-01-04T19:31:22.739380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "__init__ ran in: 3.5762786865234375e-06 sec\n",
      "Iteration 1, loss = 0.32062846\n",
      "Iteration 2, loss = 0.15380246\n",
      "Iteration 3, loss = 0.11584796\n",
      "Iteration 4, loss = 0.09353969\n",
      "Iteration 5, loss = 0.07927847\n",
      "Iteration 6, loss = 0.07134286\n",
      "Iteration 7, loss = 0.06220003\n",
      "Iteration 8, loss = 0.05481447\n",
      "Iteration 9, loss = 0.04937818\n",
      "Iteration 10, loss = 0.04596632\n",
      "fit ran in: 13.414228200912476 sec\n",
      "\n",
      "Train Accuracy - MLP: 98.77 \n",
      "\n",
      "Train confusion matrix:\n",
      "[[5878    0    6    2    0    9   10    5    6    7]\n",
      " [   1 6626   33   19    4    0   10   20   15   14]\n",
      " [   2    2 5862   42    5    1    6   26    8    4]\n",
      " [   0    0   14 6068    0   17    0    4   17   11]\n",
      " [   1    1    6    4 5729    1   19    9    1   71]\n",
      " [   4    0    1   11    0 5375   10    2    5   13]\n",
      " [   9    0    0    0    2    7 5892    1    7    0]\n",
      " [   0    2   14   23    4    1    1 6200    1   19]\n",
      " [   2    5    3   60    1   13    6    4 5719   38]\n",
      " [   3    0    0   11    2    2    0   17    3 5911]]\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "Classification report for classifier Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       1.00      0.98      0.99      1135\n",
      "           2       0.97      0.97      0.97      1032\n",
      "           3       0.94      0.97      0.95      1010\n",
      "           4       0.98      0.96      0.97       982\n",
      "           5       0.97      0.96      0.97       892\n",
      "           6       0.97      0.98      0.98       958\n",
      "           7       0.97      0.97      0.97      1028\n",
      "           8       0.98      0.94      0.96       974\n",
      "           9       0.94      0.98      0.96      1009\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n",
      "predict ran in: 0.24897122383117676 sec\n",
      "\n",
      "Test Accuracy - MLP: 97.04 \n",
      "\n",
      "Test confusion matrix:\n",
      "[[ 967    0    2    1    0    4    1    0    2    3]\n",
      " [   0 1117    3    5    0    0    3    1    6    0]\n",
      " [   3    0 1001    9    4    0    3    7    5    0]\n",
      " [   0    0    3  981    1   10    0    5    1    9]\n",
      " [   0    0    6    1  943    0    4    5    0   23]\n",
      " [   3    0    1   10    0  857    8    3    7    3]\n",
      " [   5    3    2    1    4    6  937    0    0    0]\n",
      " [   0    0    7   12    1    1    1  994    0   12]\n",
      " [   7    0    4   22    3    5    4    2  919    8]\n",
      " [   2    1    0    5    5    1    1    4    2  988]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 72)\n",
    "t0 = time.time()\n",
    "np.random.seed(31337)\n",
    "ta = algo_multilayer_perceptron(X_train, y_train, X_test, y_test)\n",
    "# ----------------------------------------------------------------------------\n",
    "train_accuracy = ta.fit()\n",
    "print()\n",
    "print(\"Train Accuracy - MLP:\", train_accuracy,'\\n')\n",
    "print(\"Train confusion matrix:\\n%s\\n\" % ta.train_confusion_matrix)\n",
    "\n",
    "# Add Train Accuracy\n",
    "method_name = \"Train Accuracy - MLP\"\n",
    "values.append((method_name, train_accuracy))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "print(\"-\" * 72)\n",
    "test_accuracy = ta.predict()\n",
    "print()\n",
    "print('Test Accuracy - MLP:', test_accuracy,'\\n')\n",
    "print(\"Test confusion matrix:\\n%s\\n\" % ta.test_confusion_matrix)\n",
    "\n",
    "# Add Test Accuracy\n",
    "method_name = \"Multilayer Perceptron\"\n",
    "values.append((method_name, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:32:41.646969Z",
     "start_time": "2020-01-04T19:31:36.417123Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setUp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ ran in: 2.6226043701171875e-06 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ran in: 15.77181339263916 sec\n",
      "setUp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:43: PendingDeprecationWarning: the matrix subclass is not the recommended way to represent matrices or deal with linear algebra (see https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html). Please adjust your code to use regular ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__ ran in: 1.9073486328125e-06 sec\n",
      "fit ran in: 15.43849515914917 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier Logistic Regression:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85       980\n",
      "           1       0.76      0.96      0.85      1135\n",
      "           2       0.80      0.62      0.69      1032\n",
      "           3       0.67      0.76      0.71      1010\n",
      "           4       0.71      0.76      0.74       982\n",
      "           5       0.62      0.41      0.50       892\n",
      "           6       0.69      0.84      0.76       958\n",
      "           7       0.74      0.81      0.77      1028\n",
      "           8       0.77      0.60      0.67       974\n",
      "           9       0.77      0.57      0.65      1009\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.73      0.73      0.72     10000\n",
      "weighted avg       0.73      0.73      0.72     10000\n",
      "\n",
      "\n",
      "predict ran in: 0.31712937355041504 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 65.206s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f3a843cf320>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestInput(unittest.TestCase):\n",
    "  \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # print('setupClass')   \n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls): \n",
    "        # print('teardownClass')\n",
    "        pass\n",
    "\n",
    "    def setUp(self):\n",
    "        print('setUp') \n",
    "        X, y = get_dataset_mnist()\n",
    "        splitRatio = 60000\n",
    "        self.X_train, self.y_train, self.X_test, self.y_test = split(X, y, splitRatio) \n",
    "        self.train_accuracy = 72.65\n",
    "        self.train_confusion_matrix = np.matrix([[5465,    7,   31,   25,   45,   18,  196,   59,   71,   6],\n",
    "                                                 [   3, 6481,   98,   45,    2,   29,   25,   40,   18,   1],\n",
    "                                                 [ 339,  478, 3603,  165,  286,   19,  689,  210,  115,  54],\n",
    "                                                 [ 143,  255,  224, 4521,   60,  256,  102,  146,  241, 183],\n",
    "                                                 [ 106,  136,   23,   51, 4521,  331,  198,  150,   84, 242],\n",
    "                                                 [ 414,  214,   85, 1032,  402, 2261,  292,  408,  186, 127],\n",
    "                                                 [ 189,   96,   91,   53,  157,  230, 5038,   29,   35,   0],\n",
    "                                                 [ 209,  203,  183,   38,  142,   16,   25, 5108,   50, 291],\n",
    "                                                 [  72,  770,  181,  692,   72,  191,  352,   48, 3317, 156],\n",
    "                                                 [ 167,  182,   48,  257,  625,  265,  120,  782,  227, 3276]])\n",
    "        \n",
    "        self.test_accuracy = 73.18\n",
    "        self.test_confusion_matrix = np.matrix([\n",
    "            [ 922,    2,    2,    3,    3,    1,   36,    4,    7,    0],\n",
    "            [   0, 1090,   18,   11,    0,    0,    5,    5,    6,    0],\n",
    "            [  66,   94,  636,   30,   40,    2,   98,   33,   25,    8],\n",
    "            [  21,   30,   31,  766,    7,   44,   18,   32,   38,   23],\n",
    "            [  15,   20,    2,    8,  748,   58,   45,   26,   18,   42],\n",
    "            [  61,   27,   12,  185,   74,  368,   58,   65,   23,   19],\n",
    "            [  36,   10,   16,    8,   26,   42,  804,    8,    7,    1],\n",
    "            [  23,   43,   47,    6,   16,    3,    6,  830,   10,   44],\n",
    "            [  14,   98,   27,  100,   10,   33,   70,    9,  583,   30],\n",
    "            [  21,   29,    9,   33,  129,   43,   28,  108,   38,  571]])\n",
    "\n",
    "        \n",
    "    def tearDown(self):\n",
    "        # print('tearDown')\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def test_fit(self):     \n",
    "        np.random.seed(31337)\n",
    "        self.ta = algo_logistic_regression(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.assertEqual(self.ta.fit(), self.train_accuracy) \n",
    "        self.assertEqual(self.ta.train_confusion_matrix.tolist(), self.train_confusion_matrix.tolist())  \n",
    "  \n",
    "\n",
    "    def test_predict(self):\n",
    "        np.random.seed(31337)\n",
    "        self.ta = algo_logistic_regression(self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "        self.ta.fit()\n",
    "        self.assertEqual(self.ta.predict(), self.test_accuracy)\n",
    "        self.assertEqual(self.ta.train_confusion_matrix.tolist(), self.train_confusion_matrix.tolist()) \n",
    "\n",
    "#run tests \n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:32:41.659364Z",
     "start_time": "2020-01-04T19:32:41.648554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testcase</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline for Test-set (Random result)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train Accuracy - Logistic Regression</td>\n",
       "      <td>72.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train Accuracy - MLP</td>\n",
       "      <td>98.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>97.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Testcase  Accuracy\n",
       "0  Baseline for Test-set (Random result)      0.00\n",
       "1   Train Accuracy - Logistic Regression     72.65\n",
       "2                    Logistic Regression     73.18\n",
       "3                   Train Accuracy - MLP     98.77\n",
       "4                  Multilayer Perceptron     97.04"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-\" * 36)\n",
    "df = pd.DataFrame(columns=COL_NAMES, data=values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network - Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:32:41.858922Z",
     "start_time": "2020-01-04T19:32:41.660866Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:32:41.898169Z",
     "start_time": "2020-01-04T19:32:41.862089Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1         # random seed (default: 1)\n",
    "no_cuda = False  # disables CUDA training\n",
    "batch_size = 32  # 64\n",
    "test_batch_size = 500\n",
    "lr = 1.0     # Learning Rate\n",
    "gamma = 0.7  # Learning rate step gamma (default: 0.7)\n",
    "epochs = 2   # number of epochs to train (default: 14)\n",
    "log_interval = 2 # how many batches to wait before logging training status\n",
    "save_model = True  # For Saving the current Model\n",
    "dir_data_path = \"./var/\"\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(dir_data_path, train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(dir_data_path, train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review some samples in Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:32:42.525106Z",
     "start_time": "2020-01-04T19:32:41.901421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example data - shape =  torch.Size([500, 1, 28, 28])\n",
      "\n",
      "This means we have 500 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one).\n",
      "We can plot some of them using matplotlib.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(\"Example data - shape = \", example_data.shape)\n",
    "\n",
    "# torch.Size([500, 1, 28, 28])\n",
    "print()\n",
    "print(\"This means we have 500 examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the one).\\nWe can plot some of them using matplotlib.\")\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Phase with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:34:39.979627Z",
     "start_time": "2020-01-04T19:32:42.527052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318299\n",
      "Train Epoch: 1 [64/60000 (0%)]\tLoss: 2.356526\n",
      "Train Epoch: 1 [128/60000 (0%)]\tLoss: 2.312636\n",
      "Train Epoch: 1 [192/60000 (0%)]\tLoss: 1.732391\n",
      "Train Epoch: 1 [256/60000 (0%)]\tLoss: 1.988776\n",
      "Train Epoch: 1 [320/60000 (1%)]\tLoss: 1.423388\n",
      "Train Epoch: 1 [384/60000 (1%)]\tLoss: 0.875975\n",
      "Train Epoch: 1 [448/60000 (1%)]\tLoss: 1.106432\n",
      "Train Epoch: 1 [512/60000 (1%)]\tLoss: 0.655509\n",
      "Train Epoch: 1 [576/60000 (1%)]\tLoss: 1.397822\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 1.419117\n",
      "Train Epoch: 1 [704/60000 (1%)]\tLoss: 1.099620\n",
      "Train Epoch: 1 [768/60000 (1%)]\tLoss: 0.776829\n",
      "Train Epoch: 1 [832/60000 (1%)]\tLoss: 0.749775\n",
      "Train Epoch: 1 [896/60000 (1%)]\tLoss: 0.527959\n",
      "Train Epoch: 1 [960/60000 (2%)]\tLoss: 0.594013\n",
      "Train Epoch: 1 [1024/60000 (2%)]\tLoss: 0.733167\n",
      "Train Epoch: 1 [1088/60000 (2%)]\tLoss: 0.655955\n",
      "Train Epoch: 1 [1152/60000 (2%)]\tLoss: 0.279198\n",
      "Train Epoch: 1 [1216/60000 (2%)]\tLoss: 0.875672\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.339386\n",
      "Train Epoch: 1 [1344/60000 (2%)]\tLoss: 0.241309\n",
      "Train Epoch: 1 [1408/60000 (2%)]\tLoss: 0.532624\n",
      "Train Epoch: 1 [1472/60000 (2%)]\tLoss: 0.646103\n",
      "Train Epoch: 1 [1536/60000 (3%)]\tLoss: 0.361729\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.163522\n",
      "Train Epoch: 1 [1664/60000 (3%)]\tLoss: 0.562877\n",
      "Train Epoch: 1 [1728/60000 (3%)]\tLoss: 0.359265\n",
      "Train Epoch: 1 [1792/60000 (3%)]\tLoss: 0.323614\n",
      "Train Epoch: 1 [1856/60000 (3%)]\tLoss: 0.646034\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.326121\n",
      "Train Epoch: 1 [1984/60000 (3%)]\tLoss: 0.461488\n",
      "Train Epoch: 1 [2048/60000 (3%)]\tLoss: 0.476587\n",
      "Train Epoch: 1 [2112/60000 (4%)]\tLoss: 0.999455\n",
      "Train Epoch: 1 [2176/60000 (4%)]\tLoss: 0.523617\n",
      "Train Epoch: 1 [2240/60000 (4%)]\tLoss: 0.363771\n",
      "Train Epoch: 1 [2304/60000 (4%)]\tLoss: 0.214505\n",
      "Train Epoch: 1 [2368/60000 (4%)]\tLoss: 0.288805\n",
      "Train Epoch: 1 [2432/60000 (4%)]\tLoss: 0.340442\n",
      "Train Epoch: 1 [2496/60000 (4%)]\tLoss: 0.380911\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.374806\n",
      "Train Epoch: 1 [2624/60000 (4%)]\tLoss: 0.178867\n",
      "Train Epoch: 1 [2688/60000 (4%)]\tLoss: 1.140804\n",
      "Train Epoch: 1 [2752/60000 (5%)]\tLoss: 0.437605\n",
      "Train Epoch: 1 [2816/60000 (5%)]\tLoss: 0.540368\n",
      "Train Epoch: 1 [2880/60000 (5%)]\tLoss: 0.187334\n",
      "Train Epoch: 1 [2944/60000 (5%)]\tLoss: 0.211509\n",
      "Train Epoch: 1 [3008/60000 (5%)]\tLoss: 0.353697\n",
      "Train Epoch: 1 [3072/60000 (5%)]\tLoss: 0.525333\n",
      "Train Epoch: 1 [3136/60000 (5%)]\tLoss: 0.504496\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.414569\n",
      "Train Epoch: 1 [3264/60000 (5%)]\tLoss: 0.272969\n",
      "Train Epoch: 1 [3328/60000 (6%)]\tLoss: 0.197131\n",
      "Train Epoch: 1 [3392/60000 (6%)]\tLoss: 0.155953\n",
      "Train Epoch: 1 [3456/60000 (6%)]\tLoss: 0.352913\n",
      "Train Epoch: 1 [3520/60000 (6%)]\tLoss: 0.248379\n",
      "Train Epoch: 1 [3584/60000 (6%)]\tLoss: 0.235308\n",
      "Train Epoch: 1 [3648/60000 (6%)]\tLoss: 0.285112\n",
      "Train Epoch: 1 [3712/60000 (6%)]\tLoss: 0.583486\n",
      "Train Epoch: 1 [3776/60000 (6%)]\tLoss: 0.172926\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.204157\n",
      "Train Epoch: 1 [3904/60000 (7%)]\tLoss: 0.066917\n",
      "Train Epoch: 1 [3968/60000 (7%)]\tLoss: 0.475939\n",
      "Train Epoch: 1 [4032/60000 (7%)]\tLoss: 0.191781\n",
      "Train Epoch: 1 [4096/60000 (7%)]\tLoss: 0.135697\n",
      "Train Epoch: 1 [4160/60000 (7%)]\tLoss: 0.174557\n",
      "Train Epoch: 1 [4224/60000 (7%)]\tLoss: 0.238206\n",
      "Train Epoch: 1 [4288/60000 (7%)]\tLoss: 0.134103\n",
      "Train Epoch: 1 [4352/60000 (7%)]\tLoss: 0.560604\n",
      "Train Epoch: 1 [4416/60000 (7%)]\tLoss: 0.458704\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.249980\n",
      "Train Epoch: 1 [4544/60000 (8%)]\tLoss: 0.403248\n",
      "Train Epoch: 1 [4608/60000 (8%)]\tLoss: 0.356272\n",
      "Train Epoch: 1 [4672/60000 (8%)]\tLoss: 0.394244\n",
      "Train Epoch: 1 [4736/60000 (8%)]\tLoss: 0.369791\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.058765\n",
      "Train Epoch: 1 [4864/60000 (8%)]\tLoss: 0.305916\n",
      "Train Epoch: 1 [4928/60000 (8%)]\tLoss: 0.209056\n",
      "Train Epoch: 1 [4992/60000 (8%)]\tLoss: 0.352070\n",
      "Train Epoch: 1 [5056/60000 (8%)]\tLoss: 0.123239\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.325804\n",
      "Train Epoch: 1 [5184/60000 (9%)]\tLoss: 0.316231\n",
      "Train Epoch: 1 [5248/60000 (9%)]\tLoss: 0.344189\n",
      "Train Epoch: 1 [5312/60000 (9%)]\tLoss: 0.238563\n",
      "Train Epoch: 1 [5376/60000 (9%)]\tLoss: 0.365042\n",
      "Train Epoch: 1 [5440/60000 (9%)]\tLoss: 0.189906\n",
      "Train Epoch: 1 [5504/60000 (9%)]\tLoss: 0.426145\n",
      "Train Epoch: 1 [5568/60000 (9%)]\tLoss: 0.090663\n",
      "Train Epoch: 1 [5632/60000 (9%)]\tLoss: 0.177993\n",
      "Train Epoch: 1 [5696/60000 (9%)]\tLoss: 0.196138\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.254932\n",
      "Train Epoch: 1 [5824/60000 (10%)]\tLoss: 0.456609\n",
      "Train Epoch: 1 [5888/60000 (10%)]\tLoss: 0.268547\n",
      "Train Epoch: 1 [5952/60000 (10%)]\tLoss: 0.053433\n",
      "Train Epoch: 1 [6016/60000 (10%)]\tLoss: 0.179400\n",
      "Train Epoch: 1 [6080/60000 (10%)]\tLoss: 0.182251\n",
      "Train Epoch: 1 [6144/60000 (10%)]\tLoss: 0.356261\n",
      "Train Epoch: 1 [6208/60000 (10%)]\tLoss: 0.117664\n",
      "Train Epoch: 1 [6272/60000 (10%)]\tLoss: 0.060347\n",
      "Train Epoch: 1 [6336/60000 (11%)]\tLoss: 0.167342\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.034733\n",
      "Train Epoch: 1 [6464/60000 (11%)]\tLoss: 0.120811\n",
      "Train Epoch: 1 [6528/60000 (11%)]\tLoss: 0.345732\n",
      "Train Epoch: 1 [6592/60000 (11%)]\tLoss: 0.368882\n",
      "Train Epoch: 1 [6656/60000 (11%)]\tLoss: 0.170327\n",
      "Train Epoch: 1 [6720/60000 (11%)]\tLoss: 0.112589\n",
      "Train Epoch: 1 [6784/60000 (11%)]\tLoss: 0.231850\n",
      "Train Epoch: 1 [6848/60000 (11%)]\tLoss: 0.301642\n",
      "Train Epoch: 1 [6912/60000 (12%)]\tLoss: 0.075346\n",
      "Train Epoch: 1 [6976/60000 (12%)]\tLoss: 0.354473\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.406649\n",
      "Train Epoch: 1 [7104/60000 (12%)]\tLoss: 0.262501\n",
      "Train Epoch: 1 [7168/60000 (12%)]\tLoss: 0.227208\n",
      "Train Epoch: 1 [7232/60000 (12%)]\tLoss: 0.200294\n",
      "Train Epoch: 1 [7296/60000 (12%)]\tLoss: 0.145417\n",
      "Train Epoch: 1 [7360/60000 (12%)]\tLoss: 0.250460\n",
      "Train Epoch: 1 [7424/60000 (12%)]\tLoss: 0.148006\n",
      "Train Epoch: 1 [7488/60000 (12%)]\tLoss: 0.238175\n",
      "Train Epoch: 1 [7552/60000 (13%)]\tLoss: 0.077844\n",
      "Train Epoch: 1 [7616/60000 (13%)]\tLoss: 0.096838\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.193523\n",
      "Train Epoch: 1 [7744/60000 (13%)]\tLoss: 0.050812\n",
      "Train Epoch: 1 [7808/60000 (13%)]\tLoss: 0.014785\n",
      "Train Epoch: 1 [7872/60000 (13%)]\tLoss: 0.341682\n",
      "Train Epoch: 1 [7936/60000 (13%)]\tLoss: 0.310557\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.528005\n",
      "Train Epoch: 1 [8064/60000 (13%)]\tLoss: 0.191449\n",
      "Train Epoch: 1 [8128/60000 (14%)]\tLoss: 0.196539\n",
      "Train Epoch: 1 [8192/60000 (14%)]\tLoss: 0.068238\n",
      "Train Epoch: 1 [8256/60000 (14%)]\tLoss: 0.053918\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.240146\n",
      "Train Epoch: 1 [8384/60000 (14%)]\tLoss: 0.159055\n",
      "Train Epoch: 1 [8448/60000 (14%)]\tLoss: 0.147061\n",
      "Train Epoch: 1 [8512/60000 (14%)]\tLoss: 0.341896\n",
      "Train Epoch: 1 [8576/60000 (14%)]\tLoss: 0.240680\n",
      "Train Epoch: 1 [8640/60000 (14%)]\tLoss: 0.252553\n",
      "Train Epoch: 1 [8704/60000 (15%)]\tLoss: 0.388281\n",
      "Train Epoch: 1 [8768/60000 (15%)]\tLoss: 0.174055\n",
      "Train Epoch: 1 [8832/60000 (15%)]\tLoss: 0.079981\n",
      "Train Epoch: 1 [8896/60000 (15%)]\tLoss: 0.326288\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.074881\n",
      "Train Epoch: 1 [9024/60000 (15%)]\tLoss: 0.241700\n",
      "Train Epoch: 1 [9088/60000 (15%)]\tLoss: 0.090524\n",
      "Train Epoch: 1 [9152/60000 (15%)]\tLoss: 0.397507\n",
      "Train Epoch: 1 [9216/60000 (15%)]\tLoss: 0.105592\n",
      "Train Epoch: 1 [9280/60000 (15%)]\tLoss: 0.131795\n",
      "Train Epoch: 1 [9344/60000 (16%)]\tLoss: 0.139701\n",
      "Train Epoch: 1 [9408/60000 (16%)]\tLoss: 0.142391\n",
      "Train Epoch: 1 [9472/60000 (16%)]\tLoss: 0.591632\n",
      "Train Epoch: 1 [9536/60000 (16%)]\tLoss: 0.025317\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.137636\n",
      "Train Epoch: 1 [9664/60000 (16%)]\tLoss: 0.188140\n",
      "Train Epoch: 1 [9728/60000 (16%)]\tLoss: 0.445003\n",
      "Train Epoch: 1 [9792/60000 (16%)]\tLoss: 0.113154\n",
      "Train Epoch: 1 [9856/60000 (16%)]\tLoss: 0.182779\n",
      "Train Epoch: 1 [9920/60000 (17%)]\tLoss: 0.627123\n",
      "Train Epoch: 1 [9984/60000 (17%)]\tLoss: 0.024845\n",
      "Train Epoch: 1 [10048/60000 (17%)]\tLoss: 0.150950\n",
      "Train Epoch: 1 [10112/60000 (17%)]\tLoss: 0.155549\n",
      "Train Epoch: 1 [10176/60000 (17%)]\tLoss: 0.180704\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.240336\n",
      "Train Epoch: 1 [10304/60000 (17%)]\tLoss: 0.086927\n",
      "Train Epoch: 1 [10368/60000 (17%)]\tLoss: 0.056282\n",
      "Train Epoch: 1 [10432/60000 (17%)]\tLoss: 0.144230\n",
      "Train Epoch: 1 [10496/60000 (17%)]\tLoss: 0.152263\n",
      "Train Epoch: 1 [10560/60000 (18%)]\tLoss: 0.306032\n",
      "Train Epoch: 1 [10624/60000 (18%)]\tLoss: 0.313113\n",
      "Train Epoch: 1 [10688/60000 (18%)]\tLoss: 0.275181\n",
      "Train Epoch: 1 [10752/60000 (18%)]\tLoss: 0.204116\n",
      "Train Epoch: 1 [10816/60000 (18%)]\tLoss: 0.143104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.236232\n",
      "Train Epoch: 1 [10944/60000 (18%)]\tLoss: 0.088301\n",
      "Train Epoch: 1 [11008/60000 (18%)]\tLoss: 0.366962\n",
      "Train Epoch: 1 [11072/60000 (18%)]\tLoss: 0.204014\n",
      "Train Epoch: 1 [11136/60000 (19%)]\tLoss: 0.143583\n",
      "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.210334\n",
      "Train Epoch: 1 [11264/60000 (19%)]\tLoss: 0.102122\n",
      "Train Epoch: 1 [11328/60000 (19%)]\tLoss: 0.102366\n",
      "Train Epoch: 1 [11392/60000 (19%)]\tLoss: 0.177041\n",
      "Train Epoch: 1 [11456/60000 (19%)]\tLoss: 0.138845\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.261450\n",
      "Train Epoch: 1 [11584/60000 (19%)]\tLoss: 0.197470\n",
      "Train Epoch: 1 [11648/60000 (19%)]\tLoss: 0.233641\n",
      "Train Epoch: 1 [11712/60000 (20%)]\tLoss: 0.135248\n",
      "Train Epoch: 1 [11776/60000 (20%)]\tLoss: 0.071971\n",
      "Train Epoch: 1 [11840/60000 (20%)]\tLoss: 0.447781\n",
      "Train Epoch: 1 [11904/60000 (20%)]\tLoss: 0.079275\n",
      "Train Epoch: 1 [11968/60000 (20%)]\tLoss: 0.199913\n",
      "Train Epoch: 1 [12032/60000 (20%)]\tLoss: 0.210040\n",
      "Train Epoch: 1 [12096/60000 (20%)]\tLoss: 0.247014\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.058979\n",
      "Train Epoch: 1 [12224/60000 (20%)]\tLoss: 0.120847\n",
      "Train Epoch: 1 [12288/60000 (20%)]\tLoss: 0.008831\n",
      "Train Epoch: 1 [12352/60000 (21%)]\tLoss: 0.235693\n",
      "Train Epoch: 1 [12416/60000 (21%)]\tLoss: 0.374022\n",
      "Train Epoch: 1 [12480/60000 (21%)]\tLoss: 0.620140\n",
      "Train Epoch: 1 [12544/60000 (21%)]\tLoss: 0.113381\n",
      "Train Epoch: 1 [12608/60000 (21%)]\tLoss: 0.196432\n",
      "Train Epoch: 1 [12672/60000 (21%)]\tLoss: 0.288825\n",
      "Train Epoch: 1 [12736/60000 (21%)]\tLoss: 0.044103\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.160585\n",
      "Train Epoch: 1 [12864/60000 (21%)]\tLoss: 0.266632\n",
      "Train Epoch: 1 [12928/60000 (22%)]\tLoss: 0.136541\n",
      "Train Epoch: 1 [12992/60000 (22%)]\tLoss: 0.315646\n",
      "Train Epoch: 1 [13056/60000 (22%)]\tLoss: 0.264910\n",
      "Train Epoch: 1 [13120/60000 (22%)]\tLoss: 0.243560\n",
      "Train Epoch: 1 [13184/60000 (22%)]\tLoss: 0.355334\n",
      "Train Epoch: 1 [13248/60000 (22%)]\tLoss: 0.389525\n",
      "Train Epoch: 1 [13312/60000 (22%)]\tLoss: 0.033934\n",
      "Train Epoch: 1 [13376/60000 (22%)]\tLoss: 0.087905\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.100344\n",
      "Train Epoch: 1 [13504/60000 (23%)]\tLoss: 0.039729\n",
      "Train Epoch: 1 [13568/60000 (23%)]\tLoss: 0.027408\n",
      "Train Epoch: 1 [13632/60000 (23%)]\tLoss: 0.097662\n",
      "Train Epoch: 1 [13696/60000 (23%)]\tLoss: 0.175982\n",
      "Train Epoch: 1 [13760/60000 (23%)]\tLoss: 0.061954\n",
      "Train Epoch: 1 [13824/60000 (23%)]\tLoss: 0.078620\n",
      "Train Epoch: 1 [13888/60000 (23%)]\tLoss: 0.058077\n",
      "Train Epoch: 1 [13952/60000 (23%)]\tLoss: 0.152776\n",
      "Train Epoch: 1 [14016/60000 (23%)]\tLoss: 0.129441\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.008109\n",
      "Train Epoch: 1 [14144/60000 (24%)]\tLoss: 0.058252\n",
      "Train Epoch: 1 [14208/60000 (24%)]\tLoss: 0.046281\n",
      "Train Epoch: 1 [14272/60000 (24%)]\tLoss: 0.250034\n",
      "Train Epoch: 1 [14336/60000 (24%)]\tLoss: 0.038841\n",
      "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.272399\n",
      "Train Epoch: 1 [14464/60000 (24%)]\tLoss: 0.507685\n",
      "Train Epoch: 1 [14528/60000 (24%)]\tLoss: 0.262481\n",
      "Train Epoch: 1 [14592/60000 (24%)]\tLoss: 0.163222\n",
      "Train Epoch: 1 [14656/60000 (24%)]\tLoss: 0.061701\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.133470\n",
      "Train Epoch: 1 [14784/60000 (25%)]\tLoss: 0.152760\n",
      "Train Epoch: 1 [14848/60000 (25%)]\tLoss: 0.213210\n",
      "Train Epoch: 1 [14912/60000 (25%)]\tLoss: 0.141383\n",
      "Train Epoch: 1 [14976/60000 (25%)]\tLoss: 0.306398\n",
      "Train Epoch: 1 [15040/60000 (25%)]\tLoss: 0.342514\n",
      "Train Epoch: 1 [15104/60000 (25%)]\tLoss: 0.143851\n",
      "Train Epoch: 1 [15168/60000 (25%)]\tLoss: 0.061079\n",
      "Train Epoch: 1 [15232/60000 (25%)]\tLoss: 0.138087\n",
      "Train Epoch: 1 [15296/60000 (25%)]\tLoss: 0.269929\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.138090\n",
      "Train Epoch: 1 [15424/60000 (26%)]\tLoss: 0.184929\n",
      "Train Epoch: 1 [15488/60000 (26%)]\tLoss: 0.323530\n",
      "Train Epoch: 1 [15552/60000 (26%)]\tLoss: 0.019822\n",
      "Train Epoch: 1 [15616/60000 (26%)]\tLoss: 0.025865\n",
      "Train Epoch: 1 [15680/60000 (26%)]\tLoss: 0.091403\n",
      "Train Epoch: 1 [15744/60000 (26%)]\tLoss: 0.045392\n",
      "Train Epoch: 1 [15808/60000 (26%)]\tLoss: 0.066866\n",
      "Train Epoch: 1 [15872/60000 (26%)]\tLoss: 0.249338\n",
      "Train Epoch: 1 [15936/60000 (27%)]\tLoss: 0.009293\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.173181\n",
      "Train Epoch: 1 [16064/60000 (27%)]\tLoss: 0.005199\n",
      "Train Epoch: 1 [16128/60000 (27%)]\tLoss: 0.129411\n",
      "Train Epoch: 1 [16192/60000 (27%)]\tLoss: 0.062061\n",
      "Train Epoch: 1 [16256/60000 (27%)]\tLoss: 0.055240\n",
      "Train Epoch: 1 [16320/60000 (27%)]\tLoss: 0.016246\n",
      "Train Epoch: 1 [16384/60000 (27%)]\tLoss: 0.064233\n",
      "Train Epoch: 1 [16448/60000 (27%)]\tLoss: 0.453777\n",
      "Train Epoch: 1 [16512/60000 (28%)]\tLoss: 0.270544\n",
      "Train Epoch: 1 [16576/60000 (28%)]\tLoss: 0.103912\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.192264\n",
      "Train Epoch: 1 [16704/60000 (28%)]\tLoss: 0.258785\n",
      "Train Epoch: 1 [16768/60000 (28%)]\tLoss: 0.134292\n",
      "Train Epoch: 1 [16832/60000 (28%)]\tLoss: 0.237424\n",
      "Train Epoch: 1 [16896/60000 (28%)]\tLoss: 0.190092\n",
      "Train Epoch: 1 [16960/60000 (28%)]\tLoss: 0.149115\n",
      "Train Epoch: 1 [17024/60000 (28%)]\tLoss: 0.332314\n",
      "Train Epoch: 1 [17088/60000 (28%)]\tLoss: 0.262408\n",
      "Train Epoch: 1 [17152/60000 (29%)]\tLoss: 0.018812\n",
      "Train Epoch: 1 [17216/60000 (29%)]\tLoss: 0.241018\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.023743\n",
      "Train Epoch: 1 [17344/60000 (29%)]\tLoss: 0.591148\n",
      "Train Epoch: 1 [17408/60000 (29%)]\tLoss: 0.079177\n",
      "Train Epoch: 1 [17472/60000 (29%)]\tLoss: 0.029548\n",
      "Train Epoch: 1 [17536/60000 (29%)]\tLoss: 0.374668\n",
      "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.663399\n",
      "Train Epoch: 1 [17664/60000 (29%)]\tLoss: 0.012916\n",
      "Train Epoch: 1 [17728/60000 (30%)]\tLoss: 0.562010\n",
      "Train Epoch: 1 [17792/60000 (30%)]\tLoss: 0.151089\n",
      "Train Epoch: 1 [17856/60000 (30%)]\tLoss: 0.278629\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.224533\n",
      "Train Epoch: 1 [17984/60000 (30%)]\tLoss: 0.186695\n",
      "Train Epoch: 1 [18048/60000 (30%)]\tLoss: 0.078102\n",
      "Train Epoch: 1 [18112/60000 (30%)]\tLoss: 0.159902\n",
      "Train Epoch: 1 [18176/60000 (30%)]\tLoss: 0.190306\n",
      "Train Epoch: 1 [18240/60000 (30%)]\tLoss: 0.029836\n",
      "Train Epoch: 1 [18304/60000 (31%)]\tLoss: 0.026778\n",
      "Train Epoch: 1 [18368/60000 (31%)]\tLoss: 0.094132\n",
      "Train Epoch: 1 [18432/60000 (31%)]\tLoss: 0.014194\n",
      "Train Epoch: 1 [18496/60000 (31%)]\tLoss: 0.157797\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.187287\n",
      "Train Epoch: 1 [18624/60000 (31%)]\tLoss: 0.435139\n",
      "Train Epoch: 1 [18688/60000 (31%)]\tLoss: 0.246062\n",
      "Train Epoch: 1 [18752/60000 (31%)]\tLoss: 0.037318\n",
      "Train Epoch: 1 [18816/60000 (31%)]\tLoss: 0.176998\n",
      "Train Epoch: 1 [18880/60000 (31%)]\tLoss: 0.257777\n",
      "Train Epoch: 1 [18944/60000 (32%)]\tLoss: 0.005282\n",
      "Train Epoch: 1 [19008/60000 (32%)]\tLoss: 0.016185\n",
      "Train Epoch: 1 [19072/60000 (32%)]\tLoss: 0.175583\n",
      "Train Epoch: 1 [19136/60000 (32%)]\tLoss: 0.177179\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.137899\n",
      "Train Epoch: 1 [19264/60000 (32%)]\tLoss: 0.365585\n",
      "Train Epoch: 1 [19328/60000 (32%)]\tLoss: 0.058693\n",
      "Train Epoch: 1 [19392/60000 (32%)]\tLoss: 0.250621\n",
      "Train Epoch: 1 [19456/60000 (32%)]\tLoss: 0.046571\n",
      "Train Epoch: 1 [19520/60000 (33%)]\tLoss: 0.199040\n",
      "Train Epoch: 1 [19584/60000 (33%)]\tLoss: 0.159693\n",
      "Train Epoch: 1 [19648/60000 (33%)]\tLoss: 0.026934\n",
      "Train Epoch: 1 [19712/60000 (33%)]\tLoss: 0.284098\n",
      "Train Epoch: 1 [19776/60000 (33%)]\tLoss: 0.235481\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.104028\n",
      "Train Epoch: 1 [19904/60000 (33%)]\tLoss: 0.067831\n",
      "Train Epoch: 1 [19968/60000 (33%)]\tLoss: 0.028783\n",
      "Train Epoch: 1 [20032/60000 (33%)]\tLoss: 0.083857\n",
      "Train Epoch: 1 [20096/60000 (33%)]\tLoss: 0.370408\n",
      "Train Epoch: 1 [20160/60000 (34%)]\tLoss: 0.091827\n",
      "Train Epoch: 1 [20224/60000 (34%)]\tLoss: 0.009270\n",
      "Train Epoch: 1 [20288/60000 (34%)]\tLoss: 0.247129\n",
      "Train Epoch: 1 [20352/60000 (34%)]\tLoss: 0.123203\n",
      "Train Epoch: 1 [20416/60000 (34%)]\tLoss: 0.057240\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.102516\n",
      "Train Epoch: 1 [20544/60000 (34%)]\tLoss: 0.088106\n",
      "Train Epoch: 1 [20608/60000 (34%)]\tLoss: 0.342712\n",
      "Train Epoch: 1 [20672/60000 (34%)]\tLoss: 0.190196\n",
      "Train Epoch: 1 [20736/60000 (35%)]\tLoss: 0.031119\n",
      "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.096514\n",
      "Train Epoch: 1 [20864/60000 (35%)]\tLoss: 0.244765\n",
      "Train Epoch: 1 [20928/60000 (35%)]\tLoss: 0.076927\n",
      "Train Epoch: 1 [20992/60000 (35%)]\tLoss: 0.301656\n",
      "Train Epoch: 1 [21056/60000 (35%)]\tLoss: 0.033601\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.025256\n",
      "Train Epoch: 1 [21184/60000 (35%)]\tLoss: 0.065553\n",
      "Train Epoch: 1 [21248/60000 (35%)]\tLoss: 0.034052\n",
      "Train Epoch: 1 [21312/60000 (36%)]\tLoss: 0.189312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [21376/60000 (36%)]\tLoss: 0.196032\n",
      "Train Epoch: 1 [21440/60000 (36%)]\tLoss: 0.040962\n",
      "Train Epoch: 1 [21504/60000 (36%)]\tLoss: 0.002538\n",
      "Train Epoch: 1 [21568/60000 (36%)]\tLoss: 0.059722\n",
      "Train Epoch: 1 [21632/60000 (36%)]\tLoss: 0.133829\n",
      "Train Epoch: 1 [21696/60000 (36%)]\tLoss: 0.126359\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.135060\n",
      "Train Epoch: 1 [21824/60000 (36%)]\tLoss: 0.619559\n",
      "Train Epoch: 1 [21888/60000 (36%)]\tLoss: 0.078371\n",
      "Train Epoch: 1 [21952/60000 (37%)]\tLoss: 0.208107\n",
      "Train Epoch: 1 [22016/60000 (37%)]\tLoss: 0.083521\n",
      "Train Epoch: 1 [22080/60000 (37%)]\tLoss: 0.393394\n",
      "Train Epoch: 1 [22144/60000 (37%)]\tLoss: 0.111635\n",
      "Train Epoch: 1 [22208/60000 (37%)]\tLoss: 0.090901\n",
      "Train Epoch: 1 [22272/60000 (37%)]\tLoss: 0.117206\n",
      "Train Epoch: 1 [22336/60000 (37%)]\tLoss: 0.099294\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.296972\n",
      "Train Epoch: 1 [22464/60000 (37%)]\tLoss: 0.234747\n",
      "Train Epoch: 1 [22528/60000 (38%)]\tLoss: 0.292073\n",
      "Train Epoch: 1 [22592/60000 (38%)]\tLoss: 0.220793\n",
      "Train Epoch: 1 [22656/60000 (38%)]\tLoss: 0.016038\n",
      "Train Epoch: 1 [22720/60000 (38%)]\tLoss: 0.130875\n",
      "Train Epoch: 1 [22784/60000 (38%)]\tLoss: 0.108996\n",
      "Train Epoch: 1 [22848/60000 (38%)]\tLoss: 0.259373\n",
      "Train Epoch: 1 [22912/60000 (38%)]\tLoss: 0.064907\n",
      "Train Epoch: 1 [22976/60000 (38%)]\tLoss: 0.070529\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.134460\n",
      "Train Epoch: 1 [23104/60000 (39%)]\tLoss: 0.029749\n",
      "Train Epoch: 1 [23168/60000 (39%)]\tLoss: 0.028403\n",
      "Train Epoch: 1 [23232/60000 (39%)]\tLoss: 0.205196\n",
      "Train Epoch: 1 [23296/60000 (39%)]\tLoss: 0.054125\n",
      "Train Epoch: 1 [23360/60000 (39%)]\tLoss: 0.125936\n",
      "Train Epoch: 1 [23424/60000 (39%)]\tLoss: 0.024346\n",
      "Train Epoch: 1 [23488/60000 (39%)]\tLoss: 0.194032\n",
      "Train Epoch: 1 [23552/60000 (39%)]\tLoss: 0.784388\n",
      "Train Epoch: 1 [23616/60000 (39%)]\tLoss: 0.044513\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.059257\n",
      "Train Epoch: 1 [23744/60000 (40%)]\tLoss: 0.031537\n",
      "Train Epoch: 1 [23808/60000 (40%)]\tLoss: 0.072001\n",
      "Train Epoch: 1 [23872/60000 (40%)]\tLoss: 0.224376\n",
      "Train Epoch: 1 [23936/60000 (40%)]\tLoss: 0.018979\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.316023\n",
      "Train Epoch: 1 [24064/60000 (40%)]\tLoss: 0.185893\n",
      "Train Epoch: 1 [24128/60000 (40%)]\tLoss: 0.049834\n",
      "Train Epoch: 1 [24192/60000 (40%)]\tLoss: 0.101852\n",
      "Train Epoch: 1 [24256/60000 (40%)]\tLoss: 0.236760\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.483365\n",
      "Train Epoch: 1 [24384/60000 (41%)]\tLoss: 0.013130\n",
      "Train Epoch: 1 [24448/60000 (41%)]\tLoss: 0.203332\n",
      "Train Epoch: 1 [24512/60000 (41%)]\tLoss: 0.101297\n",
      "Train Epoch: 1 [24576/60000 (41%)]\tLoss: 0.038823\n",
      "Train Epoch: 1 [24640/60000 (41%)]\tLoss: 0.391798\n",
      "Train Epoch: 1 [24704/60000 (41%)]\tLoss: 0.043581\n",
      "Train Epoch: 1 [24768/60000 (41%)]\tLoss: 0.037956\n",
      "Train Epoch: 1 [24832/60000 (41%)]\tLoss: 0.094167\n",
      "Train Epoch: 1 [24896/60000 (41%)]\tLoss: 0.645884\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.038153\n",
      "Train Epoch: 1 [25024/60000 (42%)]\tLoss: 0.248144\n",
      "Train Epoch: 1 [25088/60000 (42%)]\tLoss: 0.059346\n",
      "Train Epoch: 1 [25152/60000 (42%)]\tLoss: 0.055242\n",
      "Train Epoch: 1 [25216/60000 (42%)]\tLoss: 0.141255\n",
      "Train Epoch: 1 [25280/60000 (42%)]\tLoss: 0.066651\n",
      "Train Epoch: 1 [25344/60000 (42%)]\tLoss: 0.082239\n",
      "Train Epoch: 1 [25408/60000 (42%)]\tLoss: 0.190585\n",
      "Train Epoch: 1 [25472/60000 (42%)]\tLoss: 0.091030\n",
      "Train Epoch: 1 [25536/60000 (43%)]\tLoss: 0.330778\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.022202\n",
      "Train Epoch: 1 [25664/60000 (43%)]\tLoss: 0.008012\n",
      "Train Epoch: 1 [25728/60000 (43%)]\tLoss: 0.332988\n",
      "Train Epoch: 1 [25792/60000 (43%)]\tLoss: 0.016619\n",
      "Train Epoch: 1 [25856/60000 (43%)]\tLoss: 0.022900\n",
      "Train Epoch: 1 [25920/60000 (43%)]\tLoss: 0.003877\n",
      "Train Epoch: 1 [25984/60000 (43%)]\tLoss: 0.101347\n",
      "Train Epoch: 1 [26048/60000 (43%)]\tLoss: 0.392212\n",
      "Train Epoch: 1 [26112/60000 (44%)]\tLoss: 0.061342\n",
      "Train Epoch: 1 [26176/60000 (44%)]\tLoss: 0.057373\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.169360\n",
      "Train Epoch: 1 [26304/60000 (44%)]\tLoss: 0.146971\n",
      "Train Epoch: 1 [26368/60000 (44%)]\tLoss: 0.042839\n",
      "Train Epoch: 1 [26432/60000 (44%)]\tLoss: 0.204006\n",
      "Train Epoch: 1 [26496/60000 (44%)]\tLoss: 0.431120\n",
      "Train Epoch: 1 [26560/60000 (44%)]\tLoss: 0.009958\n",
      "Train Epoch: 1 [26624/60000 (44%)]\tLoss: 0.121026\n",
      "Train Epoch: 1 [26688/60000 (44%)]\tLoss: 0.098075\n",
      "Train Epoch: 1 [26752/60000 (45%)]\tLoss: 0.121285\n",
      "Train Epoch: 1 [26816/60000 (45%)]\tLoss: 0.110771\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.482104\n",
      "Train Epoch: 1 [26944/60000 (45%)]\tLoss: 0.182885\n",
      "Train Epoch: 1 [27008/60000 (45%)]\tLoss: 0.072010\n",
      "Train Epoch: 1 [27072/60000 (45%)]\tLoss: 0.783850\n",
      "Train Epoch: 1 [27136/60000 (45%)]\tLoss: 0.127065\n",
      "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.148949\n",
      "Train Epoch: 1 [27264/60000 (45%)]\tLoss: 0.508882\n",
      "Train Epoch: 1 [27328/60000 (46%)]\tLoss: 0.096537\n",
      "Train Epoch: 1 [27392/60000 (46%)]\tLoss: 0.081445\n",
      "Train Epoch: 1 [27456/60000 (46%)]\tLoss: 0.015245\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.035819\n",
      "Train Epoch: 1 [27584/60000 (46%)]\tLoss: 0.279484\n",
      "Train Epoch: 1 [27648/60000 (46%)]\tLoss: 0.254458\n",
      "Train Epoch: 1 [27712/60000 (46%)]\tLoss: 0.094803\n",
      "Train Epoch: 1 [27776/60000 (46%)]\tLoss: 0.060646\n",
      "Train Epoch: 1 [27840/60000 (46%)]\tLoss: 0.034278\n",
      "Train Epoch: 1 [27904/60000 (47%)]\tLoss: 0.060193\n",
      "Train Epoch: 1 [27968/60000 (47%)]\tLoss: 0.051731\n",
      "Train Epoch: 1 [28032/60000 (47%)]\tLoss: 0.080450\n",
      "Train Epoch: 1 [28096/60000 (47%)]\tLoss: 0.049841\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.027248\n",
      "Train Epoch: 1 [28224/60000 (47%)]\tLoss: 0.195149\n",
      "Train Epoch: 1 [28288/60000 (47%)]\tLoss: 0.210327\n",
      "Train Epoch: 1 [28352/60000 (47%)]\tLoss: 0.076029\n",
      "Train Epoch: 1 [28416/60000 (47%)]\tLoss: 0.141026\n",
      "Train Epoch: 1 [28480/60000 (47%)]\tLoss: 0.017105\n",
      "Train Epoch: 1 [28544/60000 (48%)]\tLoss: 0.196334\n",
      "Train Epoch: 1 [28608/60000 (48%)]\tLoss: 0.012263\n",
      "Train Epoch: 1 [28672/60000 (48%)]\tLoss: 0.216441\n",
      "Train Epoch: 1 [28736/60000 (48%)]\tLoss: 0.212963\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.172973\n",
      "Train Epoch: 1 [28864/60000 (48%)]\tLoss: 0.022916\n",
      "Train Epoch: 1 [28928/60000 (48%)]\tLoss: 0.034912\n",
      "Train Epoch: 1 [28992/60000 (48%)]\tLoss: 0.153936\n",
      "Train Epoch: 1 [29056/60000 (48%)]\tLoss: 0.148853\n",
      "Train Epoch: 1 [29120/60000 (49%)]\tLoss: 0.034218\n",
      "Train Epoch: 1 [29184/60000 (49%)]\tLoss: 0.022435\n",
      "Train Epoch: 1 [29248/60000 (49%)]\tLoss: 0.331735\n",
      "Train Epoch: 1 [29312/60000 (49%)]\tLoss: 0.013503\n",
      "Train Epoch: 1 [29376/60000 (49%)]\tLoss: 0.051407\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.105868\n",
      "Train Epoch: 1 [29504/60000 (49%)]\tLoss: 0.009633\n",
      "Train Epoch: 1 [29568/60000 (49%)]\tLoss: 0.088999\n",
      "Train Epoch: 1 [29632/60000 (49%)]\tLoss: 0.256312\n",
      "Train Epoch: 1 [29696/60000 (49%)]\tLoss: 0.094461\n",
      "Train Epoch: 1 [29760/60000 (50%)]\tLoss: 0.143557\n",
      "Train Epoch: 1 [29824/60000 (50%)]\tLoss: 0.530997\n",
      "Train Epoch: 1 [29888/60000 (50%)]\tLoss: 0.232101\n",
      "Train Epoch: 1 [29952/60000 (50%)]\tLoss: 0.063498\n",
      "Train Epoch: 1 [30016/60000 (50%)]\tLoss: 0.063166\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.005006\n",
      "Train Epoch: 1 [30144/60000 (50%)]\tLoss: 0.064404\n",
      "Train Epoch: 1 [30208/60000 (50%)]\tLoss: 0.021851\n",
      "Train Epoch: 1 [30272/60000 (50%)]\tLoss: 0.389834\n",
      "Train Epoch: 1 [30336/60000 (51%)]\tLoss: 0.287078\n",
      "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.029336\n",
      "Train Epoch: 1 [30464/60000 (51%)]\tLoss: 0.185166\n",
      "Train Epoch: 1 [30528/60000 (51%)]\tLoss: 0.173955\n",
      "Train Epoch: 1 [30592/60000 (51%)]\tLoss: 0.029842\n",
      "Train Epoch: 1 [30656/60000 (51%)]\tLoss: 0.014486\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.306964\n",
      "Train Epoch: 1 [30784/60000 (51%)]\tLoss: 0.214491\n",
      "Train Epoch: 1 [30848/60000 (51%)]\tLoss: 0.151802\n",
      "Train Epoch: 1 [30912/60000 (52%)]\tLoss: 0.177256\n",
      "Train Epoch: 1 [30976/60000 (52%)]\tLoss: 0.081309\n",
      "Train Epoch: 1 [31040/60000 (52%)]\tLoss: 0.024212\n",
      "Train Epoch: 1 [31104/60000 (52%)]\tLoss: 0.022689\n",
      "Train Epoch: 1 [31168/60000 (52%)]\tLoss: 0.058213\n",
      "Train Epoch: 1 [31232/60000 (52%)]\tLoss: 0.004326\n",
      "Train Epoch: 1 [31296/60000 (52%)]\tLoss: 0.239885\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.490398\n",
      "Train Epoch: 1 [31424/60000 (52%)]\tLoss: 0.084209\n",
      "Train Epoch: 1 [31488/60000 (52%)]\tLoss: 0.038082\n",
      "Train Epoch: 1 [31552/60000 (53%)]\tLoss: 0.023726\n",
      "Train Epoch: 1 [31616/60000 (53%)]\tLoss: 0.118842\n",
      "Train Epoch: 1 [31680/60000 (53%)]\tLoss: 0.147730\n",
      "Train Epoch: 1 [31744/60000 (53%)]\tLoss: 0.038789\n",
      "Train Epoch: 1 [31808/60000 (53%)]\tLoss: 0.002346\n",
      "Train Epoch: 1 [31872/60000 (53%)]\tLoss: 0.080165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [31936/60000 (53%)]\tLoss: 0.202033\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.087125\n",
      "Train Epoch: 1 [32064/60000 (53%)]\tLoss: 0.127368\n",
      "Train Epoch: 1 [32128/60000 (54%)]\tLoss: 0.055584\n",
      "Train Epoch: 1 [32192/60000 (54%)]\tLoss: 0.039045\n",
      "Train Epoch: 1 [32256/60000 (54%)]\tLoss: 0.029534\n",
      "Train Epoch: 1 [32320/60000 (54%)]\tLoss: 0.006232\n",
      "Train Epoch: 1 [32384/60000 (54%)]\tLoss: 0.155122\n",
      "Train Epoch: 1 [32448/60000 (54%)]\tLoss: 0.079358\n",
      "Train Epoch: 1 [32512/60000 (54%)]\tLoss: 0.014902\n",
      "Train Epoch: 1 [32576/60000 (54%)]\tLoss: 0.026243\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.309308\n",
      "Train Epoch: 1 [32704/60000 (55%)]\tLoss: 0.009900\n",
      "Train Epoch: 1 [32768/60000 (55%)]\tLoss: 0.149267\n",
      "Train Epoch: 1 [32832/60000 (55%)]\tLoss: 0.049524\n",
      "Train Epoch: 1 [32896/60000 (55%)]\tLoss: 0.023739\n",
      "Train Epoch: 1 [32960/60000 (55%)]\tLoss: 0.028310\n",
      "Train Epoch: 1 [33024/60000 (55%)]\tLoss: 0.014411\n",
      "Train Epoch: 1 [33088/60000 (55%)]\tLoss: 0.075543\n",
      "Train Epoch: 1 [33152/60000 (55%)]\tLoss: 0.155656\n",
      "Train Epoch: 1 [33216/60000 (55%)]\tLoss: 0.034110\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.076772\n",
      "Train Epoch: 1 [33344/60000 (56%)]\tLoss: 0.102355\n",
      "Train Epoch: 1 [33408/60000 (56%)]\tLoss: 0.008373\n",
      "Train Epoch: 1 [33472/60000 (56%)]\tLoss: 0.135175\n",
      "Train Epoch: 1 [33536/60000 (56%)]\tLoss: 0.230799\n",
      "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.048767\n",
      "Train Epoch: 1 [33664/60000 (56%)]\tLoss: 0.028404\n",
      "Train Epoch: 1 [33728/60000 (56%)]\tLoss: 0.102726\n",
      "Train Epoch: 1 [33792/60000 (56%)]\tLoss: 0.024084\n",
      "Train Epoch: 1 [33856/60000 (56%)]\tLoss: 0.006798\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.074005\n",
      "Train Epoch: 1 [33984/60000 (57%)]\tLoss: 0.117833\n",
      "Train Epoch: 1 [34048/60000 (57%)]\tLoss: 0.180955\n",
      "Train Epoch: 1 [34112/60000 (57%)]\tLoss: 0.041403\n",
      "Train Epoch: 1 [34176/60000 (57%)]\tLoss: 0.049633\n",
      "Train Epoch: 1 [34240/60000 (57%)]\tLoss: 0.076557\n",
      "Train Epoch: 1 [34304/60000 (57%)]\tLoss: 0.517646\n",
      "Train Epoch: 1 [34368/60000 (57%)]\tLoss: 0.199299\n",
      "Train Epoch: 1 [34432/60000 (57%)]\tLoss: 0.050574\n",
      "Train Epoch: 1 [34496/60000 (57%)]\tLoss: 0.078751\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.197961\n",
      "Train Epoch: 1 [34624/60000 (58%)]\tLoss: 0.068278\n",
      "Train Epoch: 1 [34688/60000 (58%)]\tLoss: 0.285048\n",
      "Train Epoch: 1 [34752/60000 (58%)]\tLoss: 0.143724\n",
      "Train Epoch: 1 [34816/60000 (58%)]\tLoss: 0.143154\n",
      "Train Epoch: 1 [34880/60000 (58%)]\tLoss: 0.291029\n",
      "Train Epoch: 1 [34944/60000 (58%)]\tLoss: 0.027512\n",
      "Train Epoch: 1 [35008/60000 (58%)]\tLoss: 0.190330\n",
      "Train Epoch: 1 [35072/60000 (58%)]\tLoss: 0.021791\n",
      "Train Epoch: 1 [35136/60000 (59%)]\tLoss: 0.218612\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.139994\n",
      "Train Epoch: 1 [35264/60000 (59%)]\tLoss: 0.304126\n",
      "Train Epoch: 1 [35328/60000 (59%)]\tLoss: 0.012884\n",
      "Train Epoch: 1 [35392/60000 (59%)]\tLoss: 0.023351\n",
      "Train Epoch: 1 [35456/60000 (59%)]\tLoss: 0.194267\n",
      "Train Epoch: 1 [35520/60000 (59%)]\tLoss: 0.021943\n",
      "Train Epoch: 1 [35584/60000 (59%)]\tLoss: 0.096186\n",
      "Train Epoch: 1 [35648/60000 (59%)]\tLoss: 0.005376\n",
      "Train Epoch: 1 [35712/60000 (60%)]\tLoss: 0.106226\n",
      "Train Epoch: 1 [35776/60000 (60%)]\tLoss: 0.007037\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.006152\n",
      "Train Epoch: 1 [35904/60000 (60%)]\tLoss: 0.005479\n",
      "Train Epoch: 1 [35968/60000 (60%)]\tLoss: 0.010909\n",
      "Train Epoch: 1 [36032/60000 (60%)]\tLoss: 0.040035\n",
      "Train Epoch: 1 [36096/60000 (60%)]\tLoss: 0.309125\n",
      "Train Epoch: 1 [36160/60000 (60%)]\tLoss: 0.148989\n",
      "Train Epoch: 1 [36224/60000 (60%)]\tLoss: 0.055278\n",
      "Train Epoch: 1 [36288/60000 (60%)]\tLoss: 0.132158\n",
      "Train Epoch: 1 [36352/60000 (61%)]\tLoss: 0.027847\n",
      "Train Epoch: 1 [36416/60000 (61%)]\tLoss: 0.439972\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.166590\n",
      "Train Epoch: 1 [36544/60000 (61%)]\tLoss: 0.022704\n",
      "Train Epoch: 1 [36608/60000 (61%)]\tLoss: 0.250836\n",
      "Train Epoch: 1 [36672/60000 (61%)]\tLoss: 0.177876\n",
      "Train Epoch: 1 [36736/60000 (61%)]\tLoss: 0.329051\n",
      "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.066159\n",
      "Train Epoch: 1 [36864/60000 (61%)]\tLoss: 0.110067\n",
      "Train Epoch: 1 [36928/60000 (62%)]\tLoss: 0.116918\n",
      "Train Epoch: 1 [36992/60000 (62%)]\tLoss: 0.010597\n",
      "Train Epoch: 1 [37056/60000 (62%)]\tLoss: 0.110811\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.099164\n",
      "Train Epoch: 1 [37184/60000 (62%)]\tLoss: 0.081615\n",
      "Train Epoch: 1 [37248/60000 (62%)]\tLoss: 0.468998\n",
      "Train Epoch: 1 [37312/60000 (62%)]\tLoss: 0.039833\n",
      "Train Epoch: 1 [37376/60000 (62%)]\tLoss: 0.045041\n",
      "Train Epoch: 1 [37440/60000 (62%)]\tLoss: 0.127920\n",
      "Train Epoch: 1 [37504/60000 (63%)]\tLoss: 0.158821\n",
      "Train Epoch: 1 [37568/60000 (63%)]\tLoss: 0.025281\n",
      "Train Epoch: 1 [37632/60000 (63%)]\tLoss: 0.166383\n",
      "Train Epoch: 1 [37696/60000 (63%)]\tLoss: 0.058000\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.406603\n",
      "Train Epoch: 1 [37824/60000 (63%)]\tLoss: 0.025037\n",
      "Train Epoch: 1 [37888/60000 (63%)]\tLoss: 0.097831\n",
      "Train Epoch: 1 [37952/60000 (63%)]\tLoss: 0.144721\n",
      "Train Epoch: 1 [38016/60000 (63%)]\tLoss: 0.053084\n",
      "Train Epoch: 1 [38080/60000 (63%)]\tLoss: 0.056092\n",
      "Train Epoch: 1 [38144/60000 (64%)]\tLoss: 0.039824\n",
      "Train Epoch: 1 [38208/60000 (64%)]\tLoss: 0.053850\n",
      "Train Epoch: 1 [38272/60000 (64%)]\tLoss: 0.006127\n",
      "Train Epoch: 1 [38336/60000 (64%)]\tLoss: 0.005847\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.099142\n",
      "Train Epoch: 1 [38464/60000 (64%)]\tLoss: 0.360611\n",
      "Train Epoch: 1 [38528/60000 (64%)]\tLoss: 0.023569\n",
      "Train Epoch: 1 [38592/60000 (64%)]\tLoss: 0.095667\n",
      "Train Epoch: 1 [38656/60000 (64%)]\tLoss: 0.165578\n",
      "Train Epoch: 1 [38720/60000 (65%)]\tLoss: 0.087579\n",
      "Train Epoch: 1 [38784/60000 (65%)]\tLoss: 0.052505\n",
      "Train Epoch: 1 [38848/60000 (65%)]\tLoss: 0.142787\n",
      "Train Epoch: 1 [38912/60000 (65%)]\tLoss: 0.132693\n",
      "Train Epoch: 1 [38976/60000 (65%)]\tLoss: 0.110314\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.050260\n",
      "Train Epoch: 1 [39104/60000 (65%)]\tLoss: 0.185530\n",
      "Train Epoch: 1 [39168/60000 (65%)]\tLoss: 0.235505\n",
      "Train Epoch: 1 [39232/60000 (65%)]\tLoss: 0.046935\n",
      "Train Epoch: 1 [39296/60000 (65%)]\tLoss: 0.126208\n",
      "Train Epoch: 1 [39360/60000 (66%)]\tLoss: 0.184010\n",
      "Train Epoch: 1 [39424/60000 (66%)]\tLoss: 0.017779\n",
      "Train Epoch: 1 [39488/60000 (66%)]\tLoss: 0.094519\n",
      "Train Epoch: 1 [39552/60000 (66%)]\tLoss: 0.083748\n",
      "Train Epoch: 1 [39616/60000 (66%)]\tLoss: 0.240323\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.007435\n",
      "Train Epoch: 1 [39744/60000 (66%)]\tLoss: 0.113403\n",
      "Train Epoch: 1 [39808/60000 (66%)]\tLoss: 0.373094\n",
      "Train Epoch: 1 [39872/60000 (66%)]\tLoss: 0.120749\n",
      "Train Epoch: 1 [39936/60000 (67%)]\tLoss: 0.046289\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.015436\n",
      "Train Epoch: 1 [40064/60000 (67%)]\tLoss: 0.218631\n",
      "Train Epoch: 1 [40128/60000 (67%)]\tLoss: 0.045677\n",
      "Train Epoch: 1 [40192/60000 (67%)]\tLoss: 0.039877\n",
      "Train Epoch: 1 [40256/60000 (67%)]\tLoss: 0.034343\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.124179\n",
      "Train Epoch: 1 [40384/60000 (67%)]\tLoss: 0.112606\n",
      "Train Epoch: 1 [40448/60000 (67%)]\tLoss: 0.034872\n",
      "Train Epoch: 1 [40512/60000 (68%)]\tLoss: 0.015650\n",
      "Train Epoch: 1 [40576/60000 (68%)]\tLoss: 0.272856\n",
      "Train Epoch: 1 [40640/60000 (68%)]\tLoss: 0.082482\n",
      "Train Epoch: 1 [40704/60000 (68%)]\tLoss: 0.201643\n",
      "Train Epoch: 1 [40768/60000 (68%)]\tLoss: 0.420132\n",
      "Train Epoch: 1 [40832/60000 (68%)]\tLoss: 0.355620\n",
      "Train Epoch: 1 [40896/60000 (68%)]\tLoss: 0.078499\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.035515\n",
      "Train Epoch: 1 [41024/60000 (68%)]\tLoss: 0.009619\n",
      "Train Epoch: 1 [41088/60000 (68%)]\tLoss: 0.040898\n",
      "Train Epoch: 1 [41152/60000 (69%)]\tLoss: 0.141637\n",
      "Train Epoch: 1 [41216/60000 (69%)]\tLoss: 0.036754\n",
      "Train Epoch: 1 [41280/60000 (69%)]\tLoss: 0.394915\n",
      "Train Epoch: 1 [41344/60000 (69%)]\tLoss: 0.381951\n",
      "Train Epoch: 1 [41408/60000 (69%)]\tLoss: 0.017769\n",
      "Train Epoch: 1 [41472/60000 (69%)]\tLoss: 0.017897\n",
      "Train Epoch: 1 [41536/60000 (69%)]\tLoss: 0.159271\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.054267\n",
      "Train Epoch: 1 [41664/60000 (69%)]\tLoss: 0.047821\n",
      "Train Epoch: 1 [41728/60000 (70%)]\tLoss: 0.043916\n",
      "Train Epoch: 1 [41792/60000 (70%)]\tLoss: 0.170748\n",
      "Train Epoch: 1 [41856/60000 (70%)]\tLoss: 0.309884\n",
      "Train Epoch: 1 [41920/60000 (70%)]\tLoss: 0.016709\n",
      "Train Epoch: 1 [41984/60000 (70%)]\tLoss: 0.233887\n",
      "Train Epoch: 1 [42048/60000 (70%)]\tLoss: 0.118496\n",
      "Train Epoch: 1 [42112/60000 (70%)]\tLoss: 0.040710\n",
      "Train Epoch: 1 [42176/60000 (70%)]\tLoss: 0.258830\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.024718\n",
      "Train Epoch: 1 [42304/60000 (71%)]\tLoss: 0.016988\n",
      "Train Epoch: 1 [42368/60000 (71%)]\tLoss: 0.064052\n",
      "Train Epoch: 1 [42432/60000 (71%)]\tLoss: 0.176155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [42496/60000 (71%)]\tLoss: 0.016517\n",
      "Train Epoch: 1 [42560/60000 (71%)]\tLoss: 0.030520\n",
      "Train Epoch: 1 [42624/60000 (71%)]\tLoss: 0.002443\n",
      "Train Epoch: 1 [42688/60000 (71%)]\tLoss: 0.168120\n",
      "Train Epoch: 1 [42752/60000 (71%)]\tLoss: 0.033562\n",
      "Train Epoch: 1 [42816/60000 (71%)]\tLoss: 0.001611\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.154334\n",
      "Train Epoch: 1 [42944/60000 (72%)]\tLoss: 0.054615\n",
      "Train Epoch: 1 [43008/60000 (72%)]\tLoss: 0.038937\n",
      "Train Epoch: 1 [43072/60000 (72%)]\tLoss: 0.074128\n",
      "Train Epoch: 1 [43136/60000 (72%)]\tLoss: 0.095028\n",
      "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.005325\n",
      "Train Epoch: 1 [43264/60000 (72%)]\tLoss: 0.011332\n",
      "Train Epoch: 1 [43328/60000 (72%)]\tLoss: 0.047807\n",
      "Train Epoch: 1 [43392/60000 (72%)]\tLoss: 0.047255\n",
      "Train Epoch: 1 [43456/60000 (72%)]\tLoss: 0.091860\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 0.003695\n",
      "Train Epoch: 1 [43584/60000 (73%)]\tLoss: 0.112970\n",
      "Train Epoch: 1 [43648/60000 (73%)]\tLoss: 0.404072\n",
      "Train Epoch: 1 [43712/60000 (73%)]\tLoss: 0.517816\n",
      "Train Epoch: 1 [43776/60000 (73%)]\tLoss: 0.133999\n",
      "Train Epoch: 1 [43840/60000 (73%)]\tLoss: 0.018460\n",
      "Train Epoch: 1 [43904/60000 (73%)]\tLoss: 0.171984\n",
      "Train Epoch: 1 [43968/60000 (73%)]\tLoss: 0.115679\n",
      "Train Epoch: 1 [44032/60000 (73%)]\tLoss: 0.087386\n",
      "Train Epoch: 1 [44096/60000 (73%)]\tLoss: 0.010674\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.049470\n",
      "Train Epoch: 1 [44224/60000 (74%)]\tLoss: 0.019576\n",
      "Train Epoch: 1 [44288/60000 (74%)]\tLoss: 0.181494\n",
      "Train Epoch: 1 [44352/60000 (74%)]\tLoss: 0.014204\n",
      "Train Epoch: 1 [44416/60000 (74%)]\tLoss: 0.081499\n",
      "Train Epoch: 1 [44480/60000 (74%)]\tLoss: 0.121314\n",
      "Train Epoch: 1 [44544/60000 (74%)]\tLoss: 0.035678\n",
      "Train Epoch: 1 [44608/60000 (74%)]\tLoss: 0.078700\n",
      "Train Epoch: 1 [44672/60000 (74%)]\tLoss: 0.004188\n",
      "Train Epoch: 1 [44736/60000 (75%)]\tLoss: 0.402004\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.005235\n",
      "Train Epoch: 1 [44864/60000 (75%)]\tLoss: 0.405708\n",
      "Train Epoch: 1 [44928/60000 (75%)]\tLoss: 0.006578\n",
      "Train Epoch: 1 [44992/60000 (75%)]\tLoss: 0.235549\n",
      "Train Epoch: 1 [45056/60000 (75%)]\tLoss: 0.105993\n",
      "Train Epoch: 1 [45120/60000 (75%)]\tLoss: 0.289570\n",
      "Train Epoch: 1 [45184/60000 (75%)]\tLoss: 0.037785\n",
      "Train Epoch: 1 [45248/60000 (75%)]\tLoss: 0.005442\n",
      "Train Epoch: 1 [45312/60000 (76%)]\tLoss: 0.068741\n",
      "Train Epoch: 1 [45376/60000 (76%)]\tLoss: 0.045935\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.020501\n",
      "Train Epoch: 1 [45504/60000 (76%)]\tLoss: 0.102545\n",
      "Train Epoch: 1 [45568/60000 (76%)]\tLoss: 0.004648\n",
      "Train Epoch: 1 [45632/60000 (76%)]\tLoss: 0.132255\n",
      "Train Epoch: 1 [45696/60000 (76%)]\tLoss: 0.036659\n",
      "Train Epoch: 1 [45760/60000 (76%)]\tLoss: 0.064421\n",
      "Train Epoch: 1 [45824/60000 (76%)]\tLoss: 0.129760\n",
      "Train Epoch: 1 [45888/60000 (76%)]\tLoss: 0.042646\n",
      "Train Epoch: 1 [45952/60000 (77%)]\tLoss: 0.324856\n",
      "Train Epoch: 1 [46016/60000 (77%)]\tLoss: 0.069847\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.188248\n",
      "Train Epoch: 1 [46144/60000 (77%)]\tLoss: 0.161094\n",
      "Train Epoch: 1 [46208/60000 (77%)]\tLoss: 0.010409\n",
      "Train Epoch: 1 [46272/60000 (77%)]\tLoss: 0.071770\n",
      "Train Epoch: 1 [46336/60000 (77%)]\tLoss: 0.018498\n",
      "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.200769\n",
      "Train Epoch: 1 [46464/60000 (77%)]\tLoss: 0.095424\n",
      "Train Epoch: 1 [46528/60000 (78%)]\tLoss: 0.029361\n",
      "Train Epoch: 1 [46592/60000 (78%)]\tLoss: 0.231278\n",
      "Train Epoch: 1 [46656/60000 (78%)]\tLoss: 0.044539\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.021822\n",
      "Train Epoch: 1 [46784/60000 (78%)]\tLoss: 0.633996\n",
      "Train Epoch: 1 [46848/60000 (78%)]\tLoss: 0.215062\n",
      "Train Epoch: 1 [46912/60000 (78%)]\tLoss: 0.024844\n",
      "Train Epoch: 1 [46976/60000 (78%)]\tLoss: 0.213024\n",
      "Train Epoch: 1 [47040/60000 (78%)]\tLoss: 0.155719\n",
      "Train Epoch: 1 [47104/60000 (79%)]\tLoss: 0.079829\n",
      "Train Epoch: 1 [47168/60000 (79%)]\tLoss: 0.018173\n",
      "Train Epoch: 1 [47232/60000 (79%)]\tLoss: 0.654176\n",
      "Train Epoch: 1 [47296/60000 (79%)]\tLoss: 0.063941\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.013200\n",
      "Train Epoch: 1 [47424/60000 (79%)]\tLoss: 0.254919\n",
      "Train Epoch: 1 [47488/60000 (79%)]\tLoss: 0.033308\n",
      "Train Epoch: 1 [47552/60000 (79%)]\tLoss: 0.190347\n",
      "Train Epoch: 1 [47616/60000 (79%)]\tLoss: 0.012440\n",
      "Train Epoch: 1 [47680/60000 (79%)]\tLoss: 0.046833\n",
      "Train Epoch: 1 [47744/60000 (80%)]\tLoss: 0.167749\n",
      "Train Epoch: 1 [47808/60000 (80%)]\tLoss: 0.155219\n",
      "Train Epoch: 1 [47872/60000 (80%)]\tLoss: 0.042709\n",
      "Train Epoch: 1 [47936/60000 (80%)]\tLoss: 0.041232\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.103978\n",
      "Train Epoch: 1 [48064/60000 (80%)]\tLoss: 0.093799\n",
      "Train Epoch: 1 [48128/60000 (80%)]\tLoss: 0.014370\n",
      "Train Epoch: 1 [48192/60000 (80%)]\tLoss: 0.003384\n",
      "Train Epoch: 1 [48256/60000 (80%)]\tLoss: 0.008334\n",
      "Train Epoch: 1 [48320/60000 (81%)]\tLoss: 0.035145\n",
      "Train Epoch: 1 [48384/60000 (81%)]\tLoss: 0.032080\n",
      "Train Epoch: 1 [48448/60000 (81%)]\tLoss: 0.040252\n",
      "Train Epoch: 1 [48512/60000 (81%)]\tLoss: 0.000720\n",
      "Train Epoch: 1 [48576/60000 (81%)]\tLoss: 0.225543\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.001584\n",
      "Train Epoch: 1 [48704/60000 (81%)]\tLoss: 0.077497\n",
      "Train Epoch: 1 [48768/60000 (81%)]\tLoss: 0.109467\n",
      "Train Epoch: 1 [48832/60000 (81%)]\tLoss: 0.012080\n",
      "Train Epoch: 1 [48896/60000 (81%)]\tLoss: 0.026428\n",
      "Train Epoch: 1 [48960/60000 (82%)]\tLoss: 0.011498\n",
      "Train Epoch: 1 [49024/60000 (82%)]\tLoss: 0.020314\n",
      "Train Epoch: 1 [49088/60000 (82%)]\tLoss: 0.176174\n",
      "Train Epoch: 1 [49152/60000 (82%)]\tLoss: 0.082924\n",
      "Train Epoch: 1 [49216/60000 (82%)]\tLoss: 0.204177\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.045607\n",
      "Train Epoch: 1 [49344/60000 (82%)]\tLoss: 0.372443\n",
      "Train Epoch: 1 [49408/60000 (82%)]\tLoss: 0.056477\n",
      "Train Epoch: 1 [49472/60000 (82%)]\tLoss: 0.357438\n",
      "Train Epoch: 1 [49536/60000 (83%)]\tLoss: 0.016835\n",
      "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.281244\n",
      "Train Epoch: 1 [49664/60000 (83%)]\tLoss: 0.005492\n",
      "Train Epoch: 1 [49728/60000 (83%)]\tLoss: 0.027408\n",
      "Train Epoch: 1 [49792/60000 (83%)]\tLoss: 0.027611\n",
      "Train Epoch: 1 [49856/60000 (83%)]\tLoss: 0.466228\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.011470\n",
      "Train Epoch: 1 [49984/60000 (83%)]\tLoss: 0.090626\n",
      "Train Epoch: 1 [50048/60000 (83%)]\tLoss: 0.007959\n",
      "Train Epoch: 1 [50112/60000 (84%)]\tLoss: 0.101257\n",
      "Train Epoch: 1 [50176/60000 (84%)]\tLoss: 0.303089\n",
      "Train Epoch: 1 [50240/60000 (84%)]\tLoss: 0.039114\n",
      "Train Epoch: 1 [50304/60000 (84%)]\tLoss: 0.026600\n",
      "Train Epoch: 1 [50368/60000 (84%)]\tLoss: 0.109580\n",
      "Train Epoch: 1 [50432/60000 (84%)]\tLoss: 0.060554\n",
      "Train Epoch: 1 [50496/60000 (84%)]\tLoss: 0.214150\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.020076\n",
      "Train Epoch: 1 [50624/60000 (84%)]\tLoss: 0.339787\n",
      "Train Epoch: 1 [50688/60000 (84%)]\tLoss: 0.033540\n",
      "Train Epoch: 1 [50752/60000 (85%)]\tLoss: 0.233759\n",
      "Train Epoch: 1 [50816/60000 (85%)]\tLoss: 0.011290\n",
      "Train Epoch: 1 [50880/60000 (85%)]\tLoss: 0.161774\n",
      "Train Epoch: 1 [50944/60000 (85%)]\tLoss: 0.160883\n",
      "Train Epoch: 1 [51008/60000 (85%)]\tLoss: 0.015004\n",
      "Train Epoch: 1 [51072/60000 (85%)]\tLoss: 0.266188\n",
      "Train Epoch: 1 [51136/60000 (85%)]\tLoss: 0.168995\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.036315\n",
      "Train Epoch: 1 [51264/60000 (85%)]\tLoss: 0.383134\n",
      "Train Epoch: 1 [51328/60000 (86%)]\tLoss: 0.552171\n",
      "Train Epoch: 1 [51392/60000 (86%)]\tLoss: 0.070204\n",
      "Train Epoch: 1 [51456/60000 (86%)]\tLoss: 0.122700\n",
      "Train Epoch: 1 [51520/60000 (86%)]\tLoss: 0.101994\n",
      "Train Epoch: 1 [51584/60000 (86%)]\tLoss: 0.067516\n",
      "Train Epoch: 1 [51648/60000 (86%)]\tLoss: 0.015589\n",
      "Train Epoch: 1 [51712/60000 (86%)]\tLoss: 0.245752\n",
      "Train Epoch: 1 [51776/60000 (86%)]\tLoss: 0.537097\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.009331\n",
      "Train Epoch: 1 [51904/60000 (87%)]\tLoss: 0.004966\n",
      "Train Epoch: 1 [51968/60000 (87%)]\tLoss: 0.369618\n",
      "Train Epoch: 1 [52032/60000 (87%)]\tLoss: 0.109113\n",
      "Train Epoch: 1 [52096/60000 (87%)]\tLoss: 0.064465\n",
      "Train Epoch: 1 [52160/60000 (87%)]\tLoss: 0.085177\n",
      "Train Epoch: 1 [52224/60000 (87%)]\tLoss: 0.065373\n",
      "Train Epoch: 1 [52288/60000 (87%)]\tLoss: 0.013585\n",
      "Train Epoch: 1 [52352/60000 (87%)]\tLoss: 0.036634\n",
      "Train Epoch: 1 [52416/60000 (87%)]\tLoss: 0.007366\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.009222\n",
      "Train Epoch: 1 [52544/60000 (88%)]\tLoss: 0.004407\n",
      "Train Epoch: 1 [52608/60000 (88%)]\tLoss: 0.102322\n",
      "Train Epoch: 1 [52672/60000 (88%)]\tLoss: 0.012317\n",
      "Train Epoch: 1 [52736/60000 (88%)]\tLoss: 0.377446\n",
      "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.088861\n",
      "Train Epoch: 1 [52864/60000 (88%)]\tLoss: 0.326942\n",
      "Train Epoch: 1 [52928/60000 (88%)]\tLoss: 0.056560\n",
      "Train Epoch: 1 [52992/60000 (88%)]\tLoss: 0.069780\n",
      "Train Epoch: 1 [53056/60000 (88%)]\tLoss: 0.040849\n",
      "Train Epoch: 1 [53120/60000 (89%)]\tLoss: 0.002001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [53184/60000 (89%)]\tLoss: 0.084001\n",
      "Train Epoch: 1 [53248/60000 (89%)]\tLoss: 0.013268\n",
      "Train Epoch: 1 [53312/60000 (89%)]\tLoss: 0.005875\n",
      "Train Epoch: 1 [53376/60000 (89%)]\tLoss: 0.265536\n",
      "Train Epoch: 1 [53440/60000 (89%)]\tLoss: 0.180840\n",
      "Train Epoch: 1 [53504/60000 (89%)]\tLoss: 0.077548\n",
      "Train Epoch: 1 [53568/60000 (89%)]\tLoss: 0.080701\n",
      "Train Epoch: 1 [53632/60000 (89%)]\tLoss: 0.004336\n",
      "Train Epoch: 1 [53696/60000 (89%)]\tLoss: 0.038625\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.034961\n",
      "Train Epoch: 1 [53824/60000 (90%)]\tLoss: 0.211494\n",
      "Train Epoch: 1 [53888/60000 (90%)]\tLoss: 0.020720\n",
      "Train Epoch: 1 [53952/60000 (90%)]\tLoss: 0.027224\n",
      "Train Epoch: 1 [54016/60000 (90%)]\tLoss: 0.010704\n",
      "Train Epoch: 1 [54080/60000 (90%)]\tLoss: 0.029158\n",
      "Train Epoch: 1 [54144/60000 (90%)]\tLoss: 0.112050\n",
      "Train Epoch: 1 [54208/60000 (90%)]\tLoss: 0.058935\n",
      "Train Epoch: 1 [54272/60000 (90%)]\tLoss: 0.117026\n",
      "Train Epoch: 1 [54336/60000 (91%)]\tLoss: 0.051199\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.042110\n",
      "Train Epoch: 1 [54464/60000 (91%)]\tLoss: 0.046236\n",
      "Train Epoch: 1 [54528/60000 (91%)]\tLoss: 0.227416\n",
      "Train Epoch: 1 [54592/60000 (91%)]\tLoss: 0.004690\n",
      "Train Epoch: 1 [54656/60000 (91%)]\tLoss: 0.004135\n",
      "Train Epoch: 1 [54720/60000 (91%)]\tLoss: 0.163319\n",
      "Train Epoch: 1 [54784/60000 (91%)]\tLoss: 0.092257\n",
      "Train Epoch: 1 [54848/60000 (91%)]\tLoss: 0.062859\n",
      "Train Epoch: 1 [54912/60000 (92%)]\tLoss: 0.115999\n",
      "Train Epoch: 1 [54976/60000 (92%)]\tLoss: 0.031442\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.400488\n",
      "Train Epoch: 1 [55104/60000 (92%)]\tLoss: 0.196473\n",
      "Train Epoch: 1 [55168/60000 (92%)]\tLoss: 0.069359\n",
      "Train Epoch: 1 [55232/60000 (92%)]\tLoss: 0.083824\n",
      "Train Epoch: 1 [55296/60000 (92%)]\tLoss: 0.228298\n",
      "Train Epoch: 1 [55360/60000 (92%)]\tLoss: 0.090117\n",
      "Train Epoch: 1 [55424/60000 (92%)]\tLoss: 0.000739\n",
      "Train Epoch: 1 [55488/60000 (92%)]\tLoss: 0.018456\n",
      "Train Epoch: 1 [55552/60000 (93%)]\tLoss: 0.030465\n",
      "Train Epoch: 1 [55616/60000 (93%)]\tLoss: 0.095887\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.017417\n",
      "Train Epoch: 1 [55744/60000 (93%)]\tLoss: 0.011118\n",
      "Train Epoch: 1 [55808/60000 (93%)]\tLoss: 0.148335\n",
      "Train Epoch: 1 [55872/60000 (93%)]\tLoss: 0.001953\n",
      "Train Epoch: 1 [55936/60000 (93%)]\tLoss: 0.111613\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.014064\n",
      "Train Epoch: 1 [56064/60000 (93%)]\tLoss: 0.012705\n",
      "Train Epoch: 1 [56128/60000 (94%)]\tLoss: 0.342535\n",
      "Train Epoch: 1 [56192/60000 (94%)]\tLoss: 0.214384\n",
      "Train Epoch: 1 [56256/60000 (94%)]\tLoss: 0.128217\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.054282\n",
      "Train Epoch: 1 [56384/60000 (94%)]\tLoss: 0.068334\n",
      "Train Epoch: 1 [56448/60000 (94%)]\tLoss: 0.023574\n",
      "Train Epoch: 1 [56512/60000 (94%)]\tLoss: 0.093619\n",
      "Train Epoch: 1 [56576/60000 (94%)]\tLoss: 0.008121\n",
      "Train Epoch: 1 [56640/60000 (94%)]\tLoss: 0.192987\n",
      "Train Epoch: 1 [56704/60000 (95%)]\tLoss: 0.106523\n",
      "Train Epoch: 1 [56768/60000 (95%)]\tLoss: 0.074190\n",
      "Train Epoch: 1 [56832/60000 (95%)]\tLoss: 0.093871\n",
      "Train Epoch: 1 [56896/60000 (95%)]\tLoss: 0.195222\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.020593\n",
      "Train Epoch: 1 [57024/60000 (95%)]\tLoss: 0.024449\n",
      "Train Epoch: 1 [57088/60000 (95%)]\tLoss: 0.015567\n",
      "Train Epoch: 1 [57152/60000 (95%)]\tLoss: 0.022270\n",
      "Train Epoch: 1 [57216/60000 (95%)]\tLoss: 0.006789\n",
      "Train Epoch: 1 [57280/60000 (95%)]\tLoss: 0.152801\n",
      "Train Epoch: 1 [57344/60000 (96%)]\tLoss: 0.184708\n",
      "Train Epoch: 1 [57408/60000 (96%)]\tLoss: 0.403155\n",
      "Train Epoch: 1 [57472/60000 (96%)]\tLoss: 0.148440\n",
      "Train Epoch: 1 [57536/60000 (96%)]\tLoss: 0.329833\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.031265\n",
      "Train Epoch: 1 [57664/60000 (96%)]\tLoss: 0.115460\n",
      "Train Epoch: 1 [57728/60000 (96%)]\tLoss: 0.246332\n",
      "Train Epoch: 1 [57792/60000 (96%)]\tLoss: 0.223478\n",
      "Train Epoch: 1 [57856/60000 (96%)]\tLoss: 0.143278\n",
      "Train Epoch: 1 [57920/60000 (97%)]\tLoss: 0.135698\n",
      "Train Epoch: 1 [57984/60000 (97%)]\tLoss: 0.229946\n",
      "Train Epoch: 1 [58048/60000 (97%)]\tLoss: 0.095597\n",
      "Train Epoch: 1 [58112/60000 (97%)]\tLoss: 0.031574\n",
      "Train Epoch: 1 [58176/60000 (97%)]\tLoss: 0.041749\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.028071\n",
      "Train Epoch: 1 [58304/60000 (97%)]\tLoss: 0.015034\n",
      "Train Epoch: 1 [58368/60000 (97%)]\tLoss: 0.116862\n",
      "Train Epoch: 1 [58432/60000 (97%)]\tLoss: 0.119486\n",
      "Train Epoch: 1 [58496/60000 (97%)]\tLoss: 0.032639\n",
      "Train Epoch: 1 [58560/60000 (98%)]\tLoss: 0.006926\n",
      "Train Epoch: 1 [58624/60000 (98%)]\tLoss: 0.025187\n",
      "Train Epoch: 1 [58688/60000 (98%)]\tLoss: 0.516139\n",
      "Train Epoch: 1 [58752/60000 (98%)]\tLoss: 0.371860\n",
      "Train Epoch: 1 [58816/60000 (98%)]\tLoss: 0.121659\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.217779\n",
      "Train Epoch: 1 [58944/60000 (98%)]\tLoss: 0.003065\n",
      "Train Epoch: 1 [59008/60000 (98%)]\tLoss: 0.344292\n",
      "Train Epoch: 1 [59072/60000 (98%)]\tLoss: 0.025475\n",
      "Train Epoch: 1 [59136/60000 (99%)]\tLoss: 0.039577\n",
      "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.130045\n",
      "Train Epoch: 1 [59264/60000 (99%)]\tLoss: 0.249257\n",
      "Train Epoch: 1 [59328/60000 (99%)]\tLoss: 0.138342\n",
      "Train Epoch: 1 [59392/60000 (99%)]\tLoss: 0.123425\n",
      "Train Epoch: 1 [59456/60000 (99%)]\tLoss: 0.032480\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.120720\n",
      "Train Epoch: 1 [59584/60000 (99%)]\tLoss: 0.070302\n",
      "Train Epoch: 1 [59648/60000 (99%)]\tLoss: 0.041060\n",
      "Train Epoch: 1 [59712/60000 (100%)]\tLoss: 0.017674\n",
      "Train Epoch: 1 [59776/60000 (100%)]\tLoss: 0.041632\n",
      "Train Epoch: 1 [59840/60000 (100%)]\tLoss: 0.250151\n",
      "Train Epoch: 1 [59904/60000 (100%)]\tLoss: 0.013142\n",
      "Train Epoch: 1 [59968/60000 (100%)]\tLoss: 0.032267\n",
      "\n",
      "Test set: Average loss: 0.0619, Accuracy: 9804/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.033175\n",
      "Train Epoch: 2 [64/60000 (0%)]\tLoss: 0.147091\n",
      "Train Epoch: 2 [128/60000 (0%)]\tLoss: 0.011107\n",
      "Train Epoch: 2 [192/60000 (0%)]\tLoss: 0.139467\n",
      "Train Epoch: 2 [256/60000 (0%)]\tLoss: 0.279032\n",
      "Train Epoch: 2 [320/60000 (1%)]\tLoss: 0.202469\n",
      "Train Epoch: 2 [384/60000 (1%)]\tLoss: 0.064530\n",
      "Train Epoch: 2 [448/60000 (1%)]\tLoss: 0.020003\n",
      "Train Epoch: 2 [512/60000 (1%)]\tLoss: 0.013842\n",
      "Train Epoch: 2 [576/60000 (1%)]\tLoss: 0.005833\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.046086\n",
      "Train Epoch: 2 [704/60000 (1%)]\tLoss: 0.090296\n",
      "Train Epoch: 2 [768/60000 (1%)]\tLoss: 0.019867\n",
      "Train Epoch: 2 [832/60000 (1%)]\tLoss: 0.002994\n",
      "Train Epoch: 2 [896/60000 (1%)]\tLoss: 0.078649\n",
      "Train Epoch: 2 [960/60000 (2%)]\tLoss: 0.004734\n",
      "Train Epoch: 2 [1024/60000 (2%)]\tLoss: 0.065801\n",
      "Train Epoch: 2 [1088/60000 (2%)]\tLoss: 0.097787\n",
      "Train Epoch: 2 [1152/60000 (2%)]\tLoss: 0.051659\n",
      "Train Epoch: 2 [1216/60000 (2%)]\tLoss: 0.470285\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.311630\n",
      "Train Epoch: 2 [1344/60000 (2%)]\tLoss: 0.048738\n",
      "Train Epoch: 2 [1408/60000 (2%)]\tLoss: 0.069282\n",
      "Train Epoch: 2 [1472/60000 (2%)]\tLoss: 0.184722\n",
      "Train Epoch: 2 [1536/60000 (3%)]\tLoss: 0.147998\n",
      "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.060933\n",
      "Train Epoch: 2 [1664/60000 (3%)]\tLoss: 0.011802\n",
      "Train Epoch: 2 [1728/60000 (3%)]\tLoss: 0.025280\n",
      "Train Epoch: 2 [1792/60000 (3%)]\tLoss: 0.094534\n",
      "Train Epoch: 2 [1856/60000 (3%)]\tLoss: 0.286896\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.018974\n",
      "Train Epoch: 2 [1984/60000 (3%)]\tLoss: 0.443532\n",
      "Train Epoch: 2 [2048/60000 (3%)]\tLoss: 0.170756\n",
      "Train Epoch: 2 [2112/60000 (4%)]\tLoss: 0.004440\n",
      "Train Epoch: 2 [2176/60000 (4%)]\tLoss: 0.035612\n",
      "Train Epoch: 2 [2240/60000 (4%)]\tLoss: 0.037763\n",
      "Train Epoch: 2 [2304/60000 (4%)]\tLoss: 0.040046\n",
      "Train Epoch: 2 [2368/60000 (4%)]\tLoss: 0.014539\n",
      "Train Epoch: 2 [2432/60000 (4%)]\tLoss: 0.150818\n",
      "Train Epoch: 2 [2496/60000 (4%)]\tLoss: 0.036047\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.032040\n",
      "Train Epoch: 2 [2624/60000 (4%)]\tLoss: 0.110289\n",
      "Train Epoch: 2 [2688/60000 (4%)]\tLoss: 0.134694\n",
      "Train Epoch: 2 [2752/60000 (5%)]\tLoss: 0.011330\n",
      "Train Epoch: 2 [2816/60000 (5%)]\tLoss: 0.058149\n",
      "Train Epoch: 2 [2880/60000 (5%)]\tLoss: 0.123757\n",
      "Train Epoch: 2 [2944/60000 (5%)]\tLoss: 0.002423\n",
      "Train Epoch: 2 [3008/60000 (5%)]\tLoss: 0.016106\n",
      "Train Epoch: 2 [3072/60000 (5%)]\tLoss: 0.007841\n",
      "Train Epoch: 2 [3136/60000 (5%)]\tLoss: 0.045648\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.032077\n",
      "Train Epoch: 2 [3264/60000 (5%)]\tLoss: 0.021656\n",
      "Train Epoch: 2 [3328/60000 (6%)]\tLoss: 0.087427\n",
      "Train Epoch: 2 [3392/60000 (6%)]\tLoss: 0.013555\n",
      "Train Epoch: 2 [3456/60000 (6%)]\tLoss: 0.001883\n",
      "Train Epoch: 2 [3520/60000 (6%)]\tLoss: 0.004342\n",
      "Train Epoch: 2 [3584/60000 (6%)]\tLoss: 0.081592\n",
      "Train Epoch: 2 [3648/60000 (6%)]\tLoss: 0.116363\n",
      "Train Epoch: 2 [3712/60000 (6%)]\tLoss: 0.028764\n",
      "Train Epoch: 2 [3776/60000 (6%)]\tLoss: 0.003344\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.104329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [3904/60000 (7%)]\tLoss: 0.000475\n",
      "Train Epoch: 2 [3968/60000 (7%)]\tLoss: 0.005051\n",
      "Train Epoch: 2 [4032/60000 (7%)]\tLoss: 0.075101\n",
      "Train Epoch: 2 [4096/60000 (7%)]\tLoss: 0.149000\n",
      "Train Epoch: 2 [4160/60000 (7%)]\tLoss: 0.016798\n",
      "Train Epoch: 2 [4224/60000 (7%)]\tLoss: 0.110540\n",
      "Train Epoch: 2 [4288/60000 (7%)]\tLoss: 0.112970\n",
      "Train Epoch: 2 [4352/60000 (7%)]\tLoss: 0.026143\n",
      "Train Epoch: 2 [4416/60000 (7%)]\tLoss: 0.038259\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.180694\n",
      "Train Epoch: 2 [4544/60000 (8%)]\tLoss: 0.036029\n",
      "Train Epoch: 2 [4608/60000 (8%)]\tLoss: 0.015407\n",
      "Train Epoch: 2 [4672/60000 (8%)]\tLoss: 0.111876\n",
      "Train Epoch: 2 [4736/60000 (8%)]\tLoss: 0.010923\n",
      "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.134081\n",
      "Train Epoch: 2 [4864/60000 (8%)]\tLoss: 0.349671\n",
      "Train Epoch: 2 [4928/60000 (8%)]\tLoss: 0.035728\n",
      "Train Epoch: 2 [4992/60000 (8%)]\tLoss: 0.053129\n",
      "Train Epoch: 2 [5056/60000 (8%)]\tLoss: 0.034205\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.067947\n",
      "Train Epoch: 2 [5184/60000 (9%)]\tLoss: 0.024013\n",
      "Train Epoch: 2 [5248/60000 (9%)]\tLoss: 0.022676\n",
      "Train Epoch: 2 [5312/60000 (9%)]\tLoss: 0.120761\n",
      "Train Epoch: 2 [5376/60000 (9%)]\tLoss: 0.014558\n",
      "Train Epoch: 2 [5440/60000 (9%)]\tLoss: 0.046871\n",
      "Train Epoch: 2 [5504/60000 (9%)]\tLoss: 0.037223\n",
      "Train Epoch: 2 [5568/60000 (9%)]\tLoss: 0.005821\n",
      "Train Epoch: 2 [5632/60000 (9%)]\tLoss: 0.026434\n",
      "Train Epoch: 2 [5696/60000 (9%)]\tLoss: 0.013444\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.007722\n",
      "Train Epoch: 2 [5824/60000 (10%)]\tLoss: 0.230272\n",
      "Train Epoch: 2 [5888/60000 (10%)]\tLoss: 0.529985\n",
      "Train Epoch: 2 [5952/60000 (10%)]\tLoss: 0.009994\n",
      "Train Epoch: 2 [6016/60000 (10%)]\tLoss: 0.045648\n",
      "Train Epoch: 2 [6080/60000 (10%)]\tLoss: 0.051601\n",
      "Train Epoch: 2 [6144/60000 (10%)]\tLoss: 0.030906\n",
      "Train Epoch: 2 [6208/60000 (10%)]\tLoss: 0.040927\n",
      "Train Epoch: 2 [6272/60000 (10%)]\tLoss: 0.080486\n",
      "Train Epoch: 2 [6336/60000 (11%)]\tLoss: 0.148200\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.018222\n",
      "Train Epoch: 2 [6464/60000 (11%)]\tLoss: 0.001694\n",
      "Train Epoch: 2 [6528/60000 (11%)]\tLoss: 0.123367\n",
      "Train Epoch: 2 [6592/60000 (11%)]\tLoss: 0.038797\n",
      "Train Epoch: 2 [6656/60000 (11%)]\tLoss: 0.026045\n",
      "Train Epoch: 2 [6720/60000 (11%)]\tLoss: 0.005416\n",
      "Train Epoch: 2 [6784/60000 (11%)]\tLoss: 0.002554\n",
      "Train Epoch: 2 [6848/60000 (11%)]\tLoss: 0.063717\n",
      "Train Epoch: 2 [6912/60000 (12%)]\tLoss: 0.002398\n",
      "Train Epoch: 2 [6976/60000 (12%)]\tLoss: 0.003235\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.076877\n",
      "Train Epoch: 2 [7104/60000 (12%)]\tLoss: 0.026992\n",
      "Train Epoch: 2 [7168/60000 (12%)]\tLoss: 0.084044\n",
      "Train Epoch: 2 [7232/60000 (12%)]\tLoss: 0.024964\n",
      "Train Epoch: 2 [7296/60000 (12%)]\tLoss: 0.001747\n",
      "Train Epoch: 2 [7360/60000 (12%)]\tLoss: 0.031146\n",
      "Train Epoch: 2 [7424/60000 (12%)]\tLoss: 0.244066\n",
      "Train Epoch: 2 [7488/60000 (12%)]\tLoss: 0.020731\n",
      "Train Epoch: 2 [7552/60000 (13%)]\tLoss: 0.015859\n",
      "Train Epoch: 2 [7616/60000 (13%)]\tLoss: 0.002468\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.082636\n",
      "Train Epoch: 2 [7744/60000 (13%)]\tLoss: 0.068993\n",
      "Train Epoch: 2 [7808/60000 (13%)]\tLoss: 0.044091\n",
      "Train Epoch: 2 [7872/60000 (13%)]\tLoss: 0.212898\n",
      "Train Epoch: 2 [7936/60000 (13%)]\tLoss: 0.000223\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.001959\n",
      "Train Epoch: 2 [8064/60000 (13%)]\tLoss: 0.114116\n",
      "Train Epoch: 2 [8128/60000 (14%)]\tLoss: 0.007141\n",
      "Train Epoch: 2 [8192/60000 (14%)]\tLoss: 0.225534\n",
      "Train Epoch: 2 [8256/60000 (14%)]\tLoss: 0.114130\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.146712\n",
      "Train Epoch: 2 [8384/60000 (14%)]\tLoss: 0.000238\n",
      "Train Epoch: 2 [8448/60000 (14%)]\tLoss: 0.085810\n",
      "Train Epoch: 2 [8512/60000 (14%)]\tLoss: 0.170308\n",
      "Train Epoch: 2 [8576/60000 (14%)]\tLoss: 0.167097\n",
      "Train Epoch: 2 [8640/60000 (14%)]\tLoss: 0.009433\n",
      "Train Epoch: 2 [8704/60000 (15%)]\tLoss: 0.033860\n",
      "Train Epoch: 2 [8768/60000 (15%)]\tLoss: 0.053053\n",
      "Train Epoch: 2 [8832/60000 (15%)]\tLoss: 0.167984\n",
      "Train Epoch: 2 [8896/60000 (15%)]\tLoss: 0.585364\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.009431\n",
      "Train Epoch: 2 [9024/60000 (15%)]\tLoss: 0.010367\n",
      "Train Epoch: 2 [9088/60000 (15%)]\tLoss: 0.057574\n",
      "Train Epoch: 2 [9152/60000 (15%)]\tLoss: 0.082213\n",
      "Train Epoch: 2 [9216/60000 (15%)]\tLoss: 0.016615\n",
      "Train Epoch: 2 [9280/60000 (15%)]\tLoss: 0.341548\n",
      "Train Epoch: 2 [9344/60000 (16%)]\tLoss: 0.049087\n",
      "Train Epoch: 2 [9408/60000 (16%)]\tLoss: 0.005991\n",
      "Train Epoch: 2 [9472/60000 (16%)]\tLoss: 0.216114\n",
      "Train Epoch: 2 [9536/60000 (16%)]\tLoss: 0.021518\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.030032\n",
      "Train Epoch: 2 [9664/60000 (16%)]\tLoss: 0.138178\n",
      "Train Epoch: 2 [9728/60000 (16%)]\tLoss: 0.079445\n",
      "Train Epoch: 2 [9792/60000 (16%)]\tLoss: 0.097523\n",
      "Train Epoch: 2 [9856/60000 (16%)]\tLoss: 0.059594\n",
      "Train Epoch: 2 [9920/60000 (17%)]\tLoss: 0.188812\n",
      "Train Epoch: 2 [9984/60000 (17%)]\tLoss: 0.053062\n",
      "Train Epoch: 2 [10048/60000 (17%)]\tLoss: 0.001582\n",
      "Train Epoch: 2 [10112/60000 (17%)]\tLoss: 0.068740\n",
      "Train Epoch: 2 [10176/60000 (17%)]\tLoss: 0.070599\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.010899\n",
      "Train Epoch: 2 [10304/60000 (17%)]\tLoss: 0.019204\n",
      "Train Epoch: 2 [10368/60000 (17%)]\tLoss: 0.005263\n",
      "Train Epoch: 2 [10432/60000 (17%)]\tLoss: 0.163358\n",
      "Train Epoch: 2 [10496/60000 (17%)]\tLoss: 0.095930\n",
      "Train Epoch: 2 [10560/60000 (18%)]\tLoss: 0.006140\n",
      "Train Epoch: 2 [10624/60000 (18%)]\tLoss: 0.010443\n",
      "Train Epoch: 2 [10688/60000 (18%)]\tLoss: 0.004871\n",
      "Train Epoch: 2 [10752/60000 (18%)]\tLoss: 0.324192\n",
      "Train Epoch: 2 [10816/60000 (18%)]\tLoss: 0.099166\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.065484\n",
      "Train Epoch: 2 [10944/60000 (18%)]\tLoss: 0.054744\n",
      "Train Epoch: 2 [11008/60000 (18%)]\tLoss: 0.022823\n",
      "Train Epoch: 2 [11072/60000 (18%)]\tLoss: 0.046796\n",
      "Train Epoch: 2 [11136/60000 (19%)]\tLoss: 0.031358\n",
      "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.028353\n",
      "Train Epoch: 2 [11264/60000 (19%)]\tLoss: 0.003420\n",
      "Train Epoch: 2 [11328/60000 (19%)]\tLoss: 0.007203\n",
      "Train Epoch: 2 [11392/60000 (19%)]\tLoss: 0.058621\n",
      "Train Epoch: 2 [11456/60000 (19%)]\tLoss: 0.052886\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.001278\n",
      "Train Epoch: 2 [11584/60000 (19%)]\tLoss: 0.020319\n",
      "Train Epoch: 2 [11648/60000 (19%)]\tLoss: 0.002525\n",
      "Train Epoch: 2 [11712/60000 (20%)]\tLoss: 0.123463\n",
      "Train Epoch: 2 [11776/60000 (20%)]\tLoss: 0.073487\n",
      "Train Epoch: 2 [11840/60000 (20%)]\tLoss: 0.039048\n",
      "Train Epoch: 2 [11904/60000 (20%)]\tLoss: 0.002734\n",
      "Train Epoch: 2 [11968/60000 (20%)]\tLoss: 0.092186\n",
      "Train Epoch: 2 [12032/60000 (20%)]\tLoss: 0.145632\n",
      "Train Epoch: 2 [12096/60000 (20%)]\tLoss: 0.033559\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.072212\n",
      "Train Epoch: 2 [12224/60000 (20%)]\tLoss: 0.008235\n",
      "Train Epoch: 2 [12288/60000 (20%)]\tLoss: 0.019125\n",
      "Train Epoch: 2 [12352/60000 (21%)]\tLoss: 0.010271\n",
      "Train Epoch: 2 [12416/60000 (21%)]\tLoss: 0.273806\n",
      "Train Epoch: 2 [12480/60000 (21%)]\tLoss: 0.004575\n",
      "Train Epoch: 2 [12544/60000 (21%)]\tLoss: 0.140681\n",
      "Train Epoch: 2 [12608/60000 (21%)]\tLoss: 0.117091\n",
      "Train Epoch: 2 [12672/60000 (21%)]\tLoss: 0.019590\n",
      "Train Epoch: 2 [12736/60000 (21%)]\tLoss: 0.159968\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.020580\n",
      "Train Epoch: 2 [12864/60000 (21%)]\tLoss: 0.089158\n",
      "Train Epoch: 2 [12928/60000 (22%)]\tLoss: 0.015421\n",
      "Train Epoch: 2 [12992/60000 (22%)]\tLoss: 0.051998\n",
      "Train Epoch: 2 [13056/60000 (22%)]\tLoss: 0.294902\n",
      "Train Epoch: 2 [13120/60000 (22%)]\tLoss: 0.134399\n",
      "Train Epoch: 2 [13184/60000 (22%)]\tLoss: 0.143806\n",
      "Train Epoch: 2 [13248/60000 (22%)]\tLoss: 0.022255\n",
      "Train Epoch: 2 [13312/60000 (22%)]\tLoss: 0.012143\n",
      "Train Epoch: 2 [13376/60000 (22%)]\tLoss: 0.159970\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.112749\n",
      "Train Epoch: 2 [13504/60000 (23%)]\tLoss: 0.018926\n",
      "Train Epoch: 2 [13568/60000 (23%)]\tLoss: 0.000530\n",
      "Train Epoch: 2 [13632/60000 (23%)]\tLoss: 0.001614\n",
      "Train Epoch: 2 [13696/60000 (23%)]\tLoss: 0.024180\n",
      "Train Epoch: 2 [13760/60000 (23%)]\tLoss: 0.034941\n",
      "Train Epoch: 2 [13824/60000 (23%)]\tLoss: 0.112988\n",
      "Train Epoch: 2 [13888/60000 (23%)]\tLoss: 0.000815\n",
      "Train Epoch: 2 [13952/60000 (23%)]\tLoss: 0.004969\n",
      "Train Epoch: 2 [14016/60000 (23%)]\tLoss: 0.034151\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.085182\n",
      "Train Epoch: 2 [14144/60000 (24%)]\tLoss: 0.018261\n",
      "Train Epoch: 2 [14208/60000 (24%)]\tLoss: 0.002417\n",
      "Train Epoch: 2 [14272/60000 (24%)]\tLoss: 0.001970\n",
      "Train Epoch: 2 [14336/60000 (24%)]\tLoss: 0.129346\n",
      "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.003594\n",
      "Train Epoch: 2 [14464/60000 (24%)]\tLoss: 0.028927\n",
      "Train Epoch: 2 [14528/60000 (24%)]\tLoss: 0.070324\n",
      "Train Epoch: 2 [14592/60000 (24%)]\tLoss: 0.011131\n",
      "Train Epoch: 2 [14656/60000 (24%)]\tLoss: 0.016052\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.258403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [14784/60000 (25%)]\tLoss: 0.031537\n",
      "Train Epoch: 2 [14848/60000 (25%)]\tLoss: 0.032364\n",
      "Train Epoch: 2 [14912/60000 (25%)]\tLoss: 0.036235\n",
      "Train Epoch: 2 [14976/60000 (25%)]\tLoss: 0.023629\n",
      "Train Epoch: 2 [15040/60000 (25%)]\tLoss: 0.049333\n",
      "Train Epoch: 2 [15104/60000 (25%)]\tLoss: 0.018808\n",
      "Train Epoch: 2 [15168/60000 (25%)]\tLoss: 0.035267\n",
      "Train Epoch: 2 [15232/60000 (25%)]\tLoss: 0.064214\n",
      "Train Epoch: 2 [15296/60000 (25%)]\tLoss: 0.003231\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.085413\n",
      "Train Epoch: 2 [15424/60000 (26%)]\tLoss: 0.098820\n",
      "Train Epoch: 2 [15488/60000 (26%)]\tLoss: 0.062950\n",
      "Train Epoch: 2 [15552/60000 (26%)]\tLoss: 0.001273\n",
      "Train Epoch: 2 [15616/60000 (26%)]\tLoss: 0.018883\n",
      "Train Epoch: 2 [15680/60000 (26%)]\tLoss: 0.018806\n",
      "Train Epoch: 2 [15744/60000 (26%)]\tLoss: 0.006671\n",
      "Train Epoch: 2 [15808/60000 (26%)]\tLoss: 0.039773\n",
      "Train Epoch: 2 [15872/60000 (26%)]\tLoss: 0.322432\n",
      "Train Epoch: 2 [15936/60000 (27%)]\tLoss: 0.077413\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.012562\n",
      "Train Epoch: 2 [16064/60000 (27%)]\tLoss: 0.000213\n",
      "Train Epoch: 2 [16128/60000 (27%)]\tLoss: 0.015540\n",
      "Train Epoch: 2 [16192/60000 (27%)]\tLoss: 0.023263\n",
      "Train Epoch: 2 [16256/60000 (27%)]\tLoss: 0.010225\n",
      "Train Epoch: 2 [16320/60000 (27%)]\tLoss: 0.098514\n",
      "Train Epoch: 2 [16384/60000 (27%)]\tLoss: 0.018611\n",
      "Train Epoch: 2 [16448/60000 (27%)]\tLoss: 0.004708\n",
      "Train Epoch: 2 [16512/60000 (28%)]\tLoss: 0.015240\n",
      "Train Epoch: 2 [16576/60000 (28%)]\tLoss: 0.108482\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.196231\n",
      "Train Epoch: 2 [16704/60000 (28%)]\tLoss: 0.009715\n",
      "Train Epoch: 2 [16768/60000 (28%)]\tLoss: 0.065432\n",
      "Train Epoch: 2 [16832/60000 (28%)]\tLoss: 0.011281\n",
      "Train Epoch: 2 [16896/60000 (28%)]\tLoss: 0.453089\n",
      "Train Epoch: 2 [16960/60000 (28%)]\tLoss: 0.009026\n",
      "Train Epoch: 2 [17024/60000 (28%)]\tLoss: 0.013736\n",
      "Train Epoch: 2 [17088/60000 (28%)]\tLoss: 0.052689\n",
      "Train Epoch: 2 [17152/60000 (29%)]\tLoss: 0.003919\n",
      "Train Epoch: 2 [17216/60000 (29%)]\tLoss: 0.024651\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.004981\n",
      "Train Epoch: 2 [17344/60000 (29%)]\tLoss: 0.034051\n",
      "Train Epoch: 2 [17408/60000 (29%)]\tLoss: 0.030303\n",
      "Train Epoch: 2 [17472/60000 (29%)]\tLoss: 0.022875\n",
      "Train Epoch: 2 [17536/60000 (29%)]\tLoss: 0.003793\n",
      "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.000425\n",
      "Train Epoch: 2 [17664/60000 (29%)]\tLoss: 0.025981\n",
      "Train Epoch: 2 [17728/60000 (30%)]\tLoss: 0.002468\n",
      "Train Epoch: 2 [17792/60000 (30%)]\tLoss: 0.063871\n",
      "Train Epoch: 2 [17856/60000 (30%)]\tLoss: 0.153915\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.025289\n",
      "Train Epoch: 2 [17984/60000 (30%)]\tLoss: 0.270849\n",
      "Train Epoch: 2 [18048/60000 (30%)]\tLoss: 0.056620\n",
      "Train Epoch: 2 [18112/60000 (30%)]\tLoss: 0.235990\n",
      "Train Epoch: 2 [18176/60000 (30%)]\tLoss: 0.199508\n",
      "Train Epoch: 2 [18240/60000 (30%)]\tLoss: 0.001735\n",
      "Train Epoch: 2 [18304/60000 (31%)]\tLoss: 0.296042\n",
      "Train Epoch: 2 [18368/60000 (31%)]\tLoss: 0.074296\n",
      "Train Epoch: 2 [18432/60000 (31%)]\tLoss: 0.001684\n",
      "Train Epoch: 2 [18496/60000 (31%)]\tLoss: 0.054373\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.032428\n",
      "Train Epoch: 2 [18624/60000 (31%)]\tLoss: 0.028535\n",
      "Train Epoch: 2 [18688/60000 (31%)]\tLoss: 0.002954\n",
      "Train Epoch: 2 [18752/60000 (31%)]\tLoss: 0.135480\n",
      "Train Epoch: 2 [18816/60000 (31%)]\tLoss: 0.003131\n",
      "Train Epoch: 2 [18880/60000 (31%)]\tLoss: 0.168854\n",
      "Train Epoch: 2 [18944/60000 (32%)]\tLoss: 0.043338\n",
      "Train Epoch: 2 [19008/60000 (32%)]\tLoss: 0.239845\n",
      "Train Epoch: 2 [19072/60000 (32%)]\tLoss: 0.062487\n",
      "Train Epoch: 2 [19136/60000 (32%)]\tLoss: 0.019617\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.028927\n",
      "Train Epoch: 2 [19264/60000 (32%)]\tLoss: 0.306944\n",
      "Train Epoch: 2 [19328/60000 (32%)]\tLoss: 0.213932\n",
      "Train Epoch: 2 [19392/60000 (32%)]\tLoss: 0.022383\n",
      "Train Epoch: 2 [19456/60000 (32%)]\tLoss: 0.117423\n",
      "Train Epoch: 2 [19520/60000 (33%)]\tLoss: 0.077494\n",
      "Train Epoch: 2 [19584/60000 (33%)]\tLoss: 0.111594\n",
      "Train Epoch: 2 [19648/60000 (33%)]\tLoss: 0.087818\n",
      "Train Epoch: 2 [19712/60000 (33%)]\tLoss: 0.140321\n",
      "Train Epoch: 2 [19776/60000 (33%)]\tLoss: 0.023902\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.004522\n",
      "Train Epoch: 2 [19904/60000 (33%)]\tLoss: 0.234554\n",
      "Train Epoch: 2 [19968/60000 (33%)]\tLoss: 0.038097\n",
      "Train Epoch: 2 [20032/60000 (33%)]\tLoss: 0.102607\n",
      "Train Epoch: 2 [20096/60000 (33%)]\tLoss: 0.141901\n",
      "Train Epoch: 2 [20160/60000 (34%)]\tLoss: 0.070423\n",
      "Train Epoch: 2 [20224/60000 (34%)]\tLoss: 0.339376\n",
      "Train Epoch: 2 [20288/60000 (34%)]\tLoss: 0.003044\n",
      "Train Epoch: 2 [20352/60000 (34%)]\tLoss: 0.014042\n",
      "Train Epoch: 2 [20416/60000 (34%)]\tLoss: 0.073410\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.009411\n",
      "Train Epoch: 2 [20544/60000 (34%)]\tLoss: 0.048846\n",
      "Train Epoch: 2 [20608/60000 (34%)]\tLoss: 0.383692\n",
      "Train Epoch: 2 [20672/60000 (34%)]\tLoss: 0.050961\n",
      "Train Epoch: 2 [20736/60000 (35%)]\tLoss: 0.040696\n",
      "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.004854\n",
      "Train Epoch: 2 [20864/60000 (35%)]\tLoss: 0.110558\n",
      "Train Epoch: 2 [20928/60000 (35%)]\tLoss: 0.007627\n",
      "Train Epoch: 2 [20992/60000 (35%)]\tLoss: 0.151839\n",
      "Train Epoch: 2 [21056/60000 (35%)]\tLoss: 0.055708\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.038889\n",
      "Train Epoch: 2 [21184/60000 (35%)]\tLoss: 0.052034\n",
      "Train Epoch: 2 [21248/60000 (35%)]\tLoss: 0.011848\n",
      "Train Epoch: 2 [21312/60000 (36%)]\tLoss: 0.045824\n",
      "Train Epoch: 2 [21376/60000 (36%)]\tLoss: 0.025974\n",
      "Train Epoch: 2 [21440/60000 (36%)]\tLoss: 0.046316\n",
      "Train Epoch: 2 [21504/60000 (36%)]\tLoss: 0.240055\n",
      "Train Epoch: 2 [21568/60000 (36%)]\tLoss: 0.053529\n",
      "Train Epoch: 2 [21632/60000 (36%)]\tLoss: 0.207346\n",
      "Train Epoch: 2 [21696/60000 (36%)]\tLoss: 0.025479\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.267478\n",
      "Train Epoch: 2 [21824/60000 (36%)]\tLoss: 0.045484\n",
      "Train Epoch: 2 [21888/60000 (36%)]\tLoss: 0.015724\n",
      "Train Epoch: 2 [21952/60000 (37%)]\tLoss: 0.022566\n",
      "Train Epoch: 2 [22016/60000 (37%)]\tLoss: 0.007237\n",
      "Train Epoch: 2 [22080/60000 (37%)]\tLoss: 0.202741\n",
      "Train Epoch: 2 [22144/60000 (37%)]\tLoss: 0.002107\n",
      "Train Epoch: 2 [22208/60000 (37%)]\tLoss: 0.012993\n",
      "Train Epoch: 2 [22272/60000 (37%)]\tLoss: 0.018430\n",
      "Train Epoch: 2 [22336/60000 (37%)]\tLoss: 0.103890\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.000542\n",
      "Train Epoch: 2 [22464/60000 (37%)]\tLoss: 0.085305\n",
      "Train Epoch: 2 [22528/60000 (38%)]\tLoss: 0.022650\n",
      "Train Epoch: 2 [22592/60000 (38%)]\tLoss: 0.130664\n",
      "Train Epoch: 2 [22656/60000 (38%)]\tLoss: 0.042242\n",
      "Train Epoch: 2 [22720/60000 (38%)]\tLoss: 0.089987\n",
      "Train Epoch: 2 [22784/60000 (38%)]\tLoss: 0.013952\n",
      "Train Epoch: 2 [22848/60000 (38%)]\tLoss: 0.228310\n",
      "Train Epoch: 2 [22912/60000 (38%)]\tLoss: 0.101758\n",
      "Train Epoch: 2 [22976/60000 (38%)]\tLoss: 0.146374\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.019524\n",
      "Train Epoch: 2 [23104/60000 (39%)]\tLoss: 0.084587\n",
      "Train Epoch: 2 [23168/60000 (39%)]\tLoss: 0.114542\n",
      "Train Epoch: 2 [23232/60000 (39%)]\tLoss: 0.002861\n",
      "Train Epoch: 2 [23296/60000 (39%)]\tLoss: 0.002774\n",
      "Train Epoch: 2 [23360/60000 (39%)]\tLoss: 0.047829\n",
      "Train Epoch: 2 [23424/60000 (39%)]\tLoss: 0.031400\n",
      "Train Epoch: 2 [23488/60000 (39%)]\tLoss: 0.002401\n",
      "Train Epoch: 2 [23552/60000 (39%)]\tLoss: 0.144729\n",
      "Train Epoch: 2 [23616/60000 (39%)]\tLoss: 0.004349\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.002452\n",
      "Train Epoch: 2 [23744/60000 (40%)]\tLoss: 0.063262\n",
      "Train Epoch: 2 [23808/60000 (40%)]\tLoss: 0.001715\n",
      "Train Epoch: 2 [23872/60000 (40%)]\tLoss: 0.057240\n",
      "Train Epoch: 2 [23936/60000 (40%)]\tLoss: 0.162606\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.075250\n",
      "Train Epoch: 2 [24064/60000 (40%)]\tLoss: 0.002336\n",
      "Train Epoch: 2 [24128/60000 (40%)]\tLoss: 0.123515\n",
      "Train Epoch: 2 [24192/60000 (40%)]\tLoss: 0.372506\n",
      "Train Epoch: 2 [24256/60000 (40%)]\tLoss: 0.222964\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.049572\n",
      "Train Epoch: 2 [24384/60000 (41%)]\tLoss: 0.006541\n",
      "Train Epoch: 2 [24448/60000 (41%)]\tLoss: 0.103187\n",
      "Train Epoch: 2 [24512/60000 (41%)]\tLoss: 0.170510\n",
      "Train Epoch: 2 [24576/60000 (41%)]\tLoss: 0.014256\n",
      "Train Epoch: 2 [24640/60000 (41%)]\tLoss: 0.033534\n",
      "Train Epoch: 2 [24704/60000 (41%)]\tLoss: 0.013858\n",
      "Train Epoch: 2 [24768/60000 (41%)]\tLoss: 0.108756\n",
      "Train Epoch: 2 [24832/60000 (41%)]\tLoss: 0.061350\n",
      "Train Epoch: 2 [24896/60000 (41%)]\tLoss: 0.007497\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.000580\n",
      "Train Epoch: 2 [25024/60000 (42%)]\tLoss: 0.026123\n",
      "Train Epoch: 2 [25088/60000 (42%)]\tLoss: 0.125908\n",
      "Train Epoch: 2 [25152/60000 (42%)]\tLoss: 0.050575\n",
      "Train Epoch: 2 [25216/60000 (42%)]\tLoss: 0.178677\n",
      "Train Epoch: 2 [25280/60000 (42%)]\tLoss: 0.111753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [25344/60000 (42%)]\tLoss: 0.313444\n",
      "Train Epoch: 2 [25408/60000 (42%)]\tLoss: 0.051958\n",
      "Train Epoch: 2 [25472/60000 (42%)]\tLoss: 0.011957\n",
      "Train Epoch: 2 [25536/60000 (43%)]\tLoss: 0.041214\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.221478\n",
      "Train Epoch: 2 [25664/60000 (43%)]\tLoss: 0.077417\n",
      "Train Epoch: 2 [25728/60000 (43%)]\tLoss: 0.042013\n",
      "Train Epoch: 2 [25792/60000 (43%)]\tLoss: 0.043337\n",
      "Train Epoch: 2 [25856/60000 (43%)]\tLoss: 0.200766\n",
      "Train Epoch: 2 [25920/60000 (43%)]\tLoss: 0.191324\n",
      "Train Epoch: 2 [25984/60000 (43%)]\tLoss: 0.032073\n",
      "Train Epoch: 2 [26048/60000 (43%)]\tLoss: 0.030216\n",
      "Train Epoch: 2 [26112/60000 (44%)]\tLoss: 0.069207\n",
      "Train Epoch: 2 [26176/60000 (44%)]\tLoss: 0.068335\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.142773\n",
      "Train Epoch: 2 [26304/60000 (44%)]\tLoss: 0.340306\n",
      "Train Epoch: 2 [26368/60000 (44%)]\tLoss: 0.141965\n",
      "Train Epoch: 2 [26432/60000 (44%)]\tLoss: 0.108962\n",
      "Train Epoch: 2 [26496/60000 (44%)]\tLoss: 0.026437\n",
      "Train Epoch: 2 [26560/60000 (44%)]\tLoss: 0.024040\n",
      "Train Epoch: 2 [26624/60000 (44%)]\tLoss: 0.040920\n",
      "Train Epoch: 2 [26688/60000 (44%)]\tLoss: 0.032422\n",
      "Train Epoch: 2 [26752/60000 (45%)]\tLoss: 0.036520\n",
      "Train Epoch: 2 [26816/60000 (45%)]\tLoss: 0.004154\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.030418\n",
      "Train Epoch: 2 [26944/60000 (45%)]\tLoss: 0.024072\n",
      "Train Epoch: 2 [27008/60000 (45%)]\tLoss: 0.041305\n",
      "Train Epoch: 2 [27072/60000 (45%)]\tLoss: 0.094145\n",
      "Train Epoch: 2 [27136/60000 (45%)]\tLoss: 0.119568\n",
      "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.050714\n",
      "Train Epoch: 2 [27264/60000 (45%)]\tLoss: 0.000418\n",
      "Train Epoch: 2 [27328/60000 (46%)]\tLoss: 0.000956\n",
      "Train Epoch: 2 [27392/60000 (46%)]\tLoss: 0.015914\n",
      "Train Epoch: 2 [27456/60000 (46%)]\tLoss: 0.007705\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.264558\n",
      "Train Epoch: 2 [27584/60000 (46%)]\tLoss: 0.036812\n",
      "Train Epoch: 2 [27648/60000 (46%)]\tLoss: 0.038706\n",
      "Train Epoch: 2 [27712/60000 (46%)]\tLoss: 0.031862\n",
      "Train Epoch: 2 [27776/60000 (46%)]\tLoss: 0.015206\n",
      "Train Epoch: 2 [27840/60000 (46%)]\tLoss: 0.043098\n",
      "Train Epoch: 2 [27904/60000 (47%)]\tLoss: 0.208153\n",
      "Train Epoch: 2 [27968/60000 (47%)]\tLoss: 0.017872\n",
      "Train Epoch: 2 [28032/60000 (47%)]\tLoss: 0.022031\n",
      "Train Epoch: 2 [28096/60000 (47%)]\tLoss: 0.006960\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.115951\n",
      "Train Epoch: 2 [28224/60000 (47%)]\tLoss: 0.003938\n",
      "Train Epoch: 2 [28288/60000 (47%)]\tLoss: 0.262489\n",
      "Train Epoch: 2 [28352/60000 (47%)]\tLoss: 0.037438\n",
      "Train Epoch: 2 [28416/60000 (47%)]\tLoss: 0.213970\n",
      "Train Epoch: 2 [28480/60000 (47%)]\tLoss: 0.004038\n",
      "Train Epoch: 2 [28544/60000 (48%)]\tLoss: 0.035270\n",
      "Train Epoch: 2 [28608/60000 (48%)]\tLoss: 0.132155\n",
      "Train Epoch: 2 [28672/60000 (48%)]\tLoss: 0.014746\n",
      "Train Epoch: 2 [28736/60000 (48%)]\tLoss: 0.026144\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.073004\n",
      "Train Epoch: 2 [28864/60000 (48%)]\tLoss: 0.000498\n",
      "Train Epoch: 2 [28928/60000 (48%)]\tLoss: 0.001040\n",
      "Train Epoch: 2 [28992/60000 (48%)]\tLoss: 0.005640\n",
      "Train Epoch: 2 [29056/60000 (48%)]\tLoss: 0.007313\n",
      "Train Epoch: 2 [29120/60000 (49%)]\tLoss: 0.121364\n",
      "Train Epoch: 2 [29184/60000 (49%)]\tLoss: 0.025367\n",
      "Train Epoch: 2 [29248/60000 (49%)]\tLoss: 0.008090\n",
      "Train Epoch: 2 [29312/60000 (49%)]\tLoss: 0.153405\n",
      "Train Epoch: 2 [29376/60000 (49%)]\tLoss: 0.104089\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.025221\n",
      "Train Epoch: 2 [29504/60000 (49%)]\tLoss: 0.082310\n",
      "Train Epoch: 2 [29568/60000 (49%)]\tLoss: 0.004842\n",
      "Train Epoch: 2 [29632/60000 (49%)]\tLoss: 0.027347\n",
      "Train Epoch: 2 [29696/60000 (49%)]\tLoss: 0.044188\n",
      "Train Epoch: 2 [29760/60000 (50%)]\tLoss: 0.132730\n",
      "Train Epoch: 2 [29824/60000 (50%)]\tLoss: 0.004923\n",
      "Train Epoch: 2 [29888/60000 (50%)]\tLoss: 0.028808\n",
      "Train Epoch: 2 [29952/60000 (50%)]\tLoss: 0.041992\n",
      "Train Epoch: 2 [30016/60000 (50%)]\tLoss: 0.000342\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.060041\n",
      "Train Epoch: 2 [30144/60000 (50%)]\tLoss: 0.050297\n",
      "Train Epoch: 2 [30208/60000 (50%)]\tLoss: 0.120175\n",
      "Train Epoch: 2 [30272/60000 (50%)]\tLoss: 0.017359\n",
      "Train Epoch: 2 [30336/60000 (51%)]\tLoss: 0.003359\n",
      "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.020055\n",
      "Train Epoch: 2 [30464/60000 (51%)]\tLoss: 0.016668\n",
      "Train Epoch: 2 [30528/60000 (51%)]\tLoss: 0.232098\n",
      "Train Epoch: 2 [30592/60000 (51%)]\tLoss: 0.159608\n",
      "Train Epoch: 2 [30656/60000 (51%)]\tLoss: 0.041949\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.002940\n",
      "Train Epoch: 2 [30784/60000 (51%)]\tLoss: 0.020608\n",
      "Train Epoch: 2 [30848/60000 (51%)]\tLoss: 0.010568\n",
      "Train Epoch: 2 [30912/60000 (52%)]\tLoss: 0.157961\n",
      "Train Epoch: 2 [30976/60000 (52%)]\tLoss: 0.053331\n",
      "Train Epoch: 2 [31040/60000 (52%)]\tLoss: 0.056547\n",
      "Train Epoch: 2 [31104/60000 (52%)]\tLoss: 0.036255\n",
      "Train Epoch: 2 [31168/60000 (52%)]\tLoss: 0.045005\n",
      "Train Epoch: 2 [31232/60000 (52%)]\tLoss: 0.005245\n",
      "Train Epoch: 2 [31296/60000 (52%)]\tLoss: 0.011469\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.010840\n",
      "Train Epoch: 2 [31424/60000 (52%)]\tLoss: 0.043509\n",
      "Train Epoch: 2 [31488/60000 (52%)]\tLoss: 0.009326\n",
      "Train Epoch: 2 [31552/60000 (53%)]\tLoss: 0.000683\n",
      "Train Epoch: 2 [31616/60000 (53%)]\tLoss: 0.001743\n",
      "Train Epoch: 2 [31680/60000 (53%)]\tLoss: 0.055999\n",
      "Train Epoch: 2 [31744/60000 (53%)]\tLoss: 0.036713\n",
      "Train Epoch: 2 [31808/60000 (53%)]\tLoss: 0.000509\n",
      "Train Epoch: 2 [31872/60000 (53%)]\tLoss: 0.062315\n",
      "Train Epoch: 2 [31936/60000 (53%)]\tLoss: 0.006455\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.237465\n",
      "Train Epoch: 2 [32064/60000 (53%)]\tLoss: 0.110540\n",
      "Train Epoch: 2 [32128/60000 (54%)]\tLoss: 0.005560\n",
      "Train Epoch: 2 [32192/60000 (54%)]\tLoss: 0.457330\n",
      "Train Epoch: 2 [32256/60000 (54%)]\tLoss: 0.002668\n",
      "Train Epoch: 2 [32320/60000 (54%)]\tLoss: 0.001613\n",
      "Train Epoch: 2 [32384/60000 (54%)]\tLoss: 0.019265\n",
      "Train Epoch: 2 [32448/60000 (54%)]\tLoss: 0.112899\n",
      "Train Epoch: 2 [32512/60000 (54%)]\tLoss: 0.049949\n",
      "Train Epoch: 2 [32576/60000 (54%)]\tLoss: 0.458914\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.037605\n",
      "Train Epoch: 2 [32704/60000 (55%)]\tLoss: 0.041230\n",
      "Train Epoch: 2 [32768/60000 (55%)]\tLoss: 0.001002\n",
      "Train Epoch: 2 [32832/60000 (55%)]\tLoss: 0.034562\n",
      "Train Epoch: 2 [32896/60000 (55%)]\tLoss: 0.069655\n",
      "Train Epoch: 2 [32960/60000 (55%)]\tLoss: 0.089996\n",
      "Train Epoch: 2 [33024/60000 (55%)]\tLoss: 0.001570\n",
      "Train Epoch: 2 [33088/60000 (55%)]\tLoss: 0.011776\n",
      "Train Epoch: 2 [33152/60000 (55%)]\tLoss: 0.101277\n",
      "Train Epoch: 2 [33216/60000 (55%)]\tLoss: 0.056195\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.009649\n",
      "Train Epoch: 2 [33344/60000 (56%)]\tLoss: 0.067743\n",
      "Train Epoch: 2 [33408/60000 (56%)]\tLoss: 0.003575\n",
      "Train Epoch: 2 [33472/60000 (56%)]\tLoss: 0.162504\n",
      "Train Epoch: 2 [33536/60000 (56%)]\tLoss: 0.266201\n",
      "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.031210\n",
      "Train Epoch: 2 [33664/60000 (56%)]\tLoss: 0.002392\n",
      "Train Epoch: 2 [33728/60000 (56%)]\tLoss: 0.004869\n",
      "Train Epoch: 2 [33792/60000 (56%)]\tLoss: 0.002415\n",
      "Train Epoch: 2 [33856/60000 (56%)]\tLoss: 0.042310\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.114667\n",
      "Train Epoch: 2 [33984/60000 (57%)]\tLoss: 0.058754\n",
      "Train Epoch: 2 [34048/60000 (57%)]\tLoss: 0.278313\n",
      "Train Epoch: 2 [34112/60000 (57%)]\tLoss: 0.158459\n",
      "Train Epoch: 2 [34176/60000 (57%)]\tLoss: 0.007586\n",
      "Train Epoch: 2 [34240/60000 (57%)]\tLoss: 0.032385\n",
      "Train Epoch: 2 [34304/60000 (57%)]\tLoss: 0.006720\n",
      "Train Epoch: 2 [34368/60000 (57%)]\tLoss: 0.073673\n",
      "Train Epoch: 2 [34432/60000 (57%)]\tLoss: 0.049506\n",
      "Train Epoch: 2 [34496/60000 (57%)]\tLoss: 0.029878\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.001457\n",
      "Train Epoch: 2 [34624/60000 (58%)]\tLoss: 0.116101\n",
      "Train Epoch: 2 [34688/60000 (58%)]\tLoss: 0.152502\n",
      "Train Epoch: 2 [34752/60000 (58%)]\tLoss: 0.019476\n",
      "Train Epoch: 2 [34816/60000 (58%)]\tLoss: 0.049964\n",
      "Train Epoch: 2 [34880/60000 (58%)]\tLoss: 0.013059\n",
      "Train Epoch: 2 [34944/60000 (58%)]\tLoss: 0.078260\n",
      "Train Epoch: 2 [35008/60000 (58%)]\tLoss: 0.076695\n",
      "Train Epoch: 2 [35072/60000 (58%)]\tLoss: 0.110242\n",
      "Train Epoch: 2 [35136/60000 (59%)]\tLoss: 0.002030\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.103087\n",
      "Train Epoch: 2 [35264/60000 (59%)]\tLoss: 0.004565\n",
      "Train Epoch: 2 [35328/60000 (59%)]\tLoss: 0.001684\n",
      "Train Epoch: 2 [35392/60000 (59%)]\tLoss: 0.024700\n",
      "Train Epoch: 2 [35456/60000 (59%)]\tLoss: 0.172434\n",
      "Train Epoch: 2 [35520/60000 (59%)]\tLoss: 0.018520\n",
      "Train Epoch: 2 [35584/60000 (59%)]\tLoss: 0.039567\n",
      "Train Epoch: 2 [35648/60000 (59%)]\tLoss: 0.025390\n",
      "Train Epoch: 2 [35712/60000 (60%)]\tLoss: 0.028632\n",
      "Train Epoch: 2 [35776/60000 (60%)]\tLoss: 0.026688\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.005614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [35904/60000 (60%)]\tLoss: 0.051300\n",
      "Train Epoch: 2 [35968/60000 (60%)]\tLoss: 0.058851\n",
      "Train Epoch: 2 [36032/60000 (60%)]\tLoss: 0.332901\n",
      "Train Epoch: 2 [36096/60000 (60%)]\tLoss: 0.001024\n",
      "Train Epoch: 2 [36160/60000 (60%)]\tLoss: 0.045919\n",
      "Train Epoch: 2 [36224/60000 (60%)]\tLoss: 0.032096\n",
      "Train Epoch: 2 [36288/60000 (60%)]\tLoss: 0.004994\n",
      "Train Epoch: 2 [36352/60000 (61%)]\tLoss: 0.059371\n",
      "Train Epoch: 2 [36416/60000 (61%)]\tLoss: 0.492651\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.149863\n",
      "Train Epoch: 2 [36544/60000 (61%)]\tLoss: 0.086781\n",
      "Train Epoch: 2 [36608/60000 (61%)]\tLoss: 0.046817\n",
      "Train Epoch: 2 [36672/60000 (61%)]\tLoss: 0.058718\n",
      "Train Epoch: 2 [36736/60000 (61%)]\tLoss: 0.140926\n",
      "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.002912\n",
      "Train Epoch: 2 [36864/60000 (61%)]\tLoss: 0.263623\n",
      "Train Epoch: 2 [36928/60000 (62%)]\tLoss: 0.054543\n",
      "Train Epoch: 2 [36992/60000 (62%)]\tLoss: 0.018298\n",
      "Train Epoch: 2 [37056/60000 (62%)]\tLoss: 0.018759\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.136152\n",
      "Train Epoch: 2 [37184/60000 (62%)]\tLoss: 0.001243\n",
      "Train Epoch: 2 [37248/60000 (62%)]\tLoss: 0.084773\n",
      "Train Epoch: 2 [37312/60000 (62%)]\tLoss: 0.080974\n",
      "Train Epoch: 2 [37376/60000 (62%)]\tLoss: 0.039121\n",
      "Train Epoch: 2 [37440/60000 (62%)]\tLoss: 0.003489\n",
      "Train Epoch: 2 [37504/60000 (63%)]\tLoss: 0.183017\n",
      "Train Epoch: 2 [37568/60000 (63%)]\tLoss: 0.128135\n",
      "Train Epoch: 2 [37632/60000 (63%)]\tLoss: 0.049444\n",
      "Train Epoch: 2 [37696/60000 (63%)]\tLoss: 0.020995\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.011331\n",
      "Train Epoch: 2 [37824/60000 (63%)]\tLoss: 0.050058\n",
      "Train Epoch: 2 [37888/60000 (63%)]\tLoss: 0.004615\n",
      "Train Epoch: 2 [37952/60000 (63%)]\tLoss: 0.314745\n",
      "Train Epoch: 2 [38016/60000 (63%)]\tLoss: 0.001251\n",
      "Train Epoch: 2 [38080/60000 (63%)]\tLoss: 0.022594\n",
      "Train Epoch: 2 [38144/60000 (64%)]\tLoss: 0.003772\n",
      "Train Epoch: 2 [38208/60000 (64%)]\tLoss: 0.046775\n",
      "Train Epoch: 2 [38272/60000 (64%)]\tLoss: 0.011936\n",
      "Train Epoch: 2 [38336/60000 (64%)]\tLoss: 0.001607\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.168743\n",
      "Train Epoch: 2 [38464/60000 (64%)]\tLoss: 0.273448\n",
      "Train Epoch: 2 [38528/60000 (64%)]\tLoss: 0.021073\n",
      "Train Epoch: 2 [38592/60000 (64%)]\tLoss: 0.024637\n",
      "Train Epoch: 2 [38656/60000 (64%)]\tLoss: 0.024406\n",
      "Train Epoch: 2 [38720/60000 (65%)]\tLoss: 0.042169\n",
      "Train Epoch: 2 [38784/60000 (65%)]\tLoss: 0.007965\n",
      "Train Epoch: 2 [38848/60000 (65%)]\tLoss: 0.091865\n",
      "Train Epoch: 2 [38912/60000 (65%)]\tLoss: 0.059924\n",
      "Train Epoch: 2 [38976/60000 (65%)]\tLoss: 0.037345\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.005311\n",
      "Train Epoch: 2 [39104/60000 (65%)]\tLoss: 0.108795\n",
      "Train Epoch: 2 [39168/60000 (65%)]\tLoss: 0.144846\n",
      "Train Epoch: 2 [39232/60000 (65%)]\tLoss: 0.003465\n",
      "Train Epoch: 2 [39296/60000 (65%)]\tLoss: 0.376419\n",
      "Train Epoch: 2 [39360/60000 (66%)]\tLoss: 0.043138\n",
      "Train Epoch: 2 [39424/60000 (66%)]\tLoss: 0.002260\n",
      "Train Epoch: 2 [39488/60000 (66%)]\tLoss: 0.104465\n",
      "Train Epoch: 2 [39552/60000 (66%)]\tLoss: 0.194745\n",
      "Train Epoch: 2 [39616/60000 (66%)]\tLoss: 0.011828\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.005972\n",
      "Train Epoch: 2 [39744/60000 (66%)]\tLoss: 0.122026\n",
      "Train Epoch: 2 [39808/60000 (66%)]\tLoss: 0.218747\n",
      "Train Epoch: 2 [39872/60000 (66%)]\tLoss: 0.005364\n",
      "Train Epoch: 2 [39936/60000 (67%)]\tLoss: 0.059091\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.113345\n",
      "Train Epoch: 2 [40064/60000 (67%)]\tLoss: 0.022138\n",
      "Train Epoch: 2 [40128/60000 (67%)]\tLoss: 0.004322\n",
      "Train Epoch: 2 [40192/60000 (67%)]\tLoss: 0.629738\n",
      "Train Epoch: 2 [40256/60000 (67%)]\tLoss: 0.019680\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.005946\n",
      "Train Epoch: 2 [40384/60000 (67%)]\tLoss: 0.036240\n",
      "Train Epoch: 2 [40448/60000 (67%)]\tLoss: 0.104437\n",
      "Train Epoch: 2 [40512/60000 (68%)]\tLoss: 0.004987\n",
      "Train Epoch: 2 [40576/60000 (68%)]\tLoss: 0.191784\n",
      "Train Epoch: 2 [40640/60000 (68%)]\tLoss: 0.122578\n",
      "Train Epoch: 2 [40704/60000 (68%)]\tLoss: 0.188025\n",
      "Train Epoch: 2 [40768/60000 (68%)]\tLoss: 0.005066\n",
      "Train Epoch: 2 [40832/60000 (68%)]\tLoss: 0.003116\n",
      "Train Epoch: 2 [40896/60000 (68%)]\tLoss: 0.135894\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.061005\n",
      "Train Epoch: 2 [41024/60000 (68%)]\tLoss: 0.004975\n",
      "Train Epoch: 2 [41088/60000 (68%)]\tLoss: 0.033428\n",
      "Train Epoch: 2 [41152/60000 (69%)]\tLoss: 0.003439\n",
      "Train Epoch: 2 [41216/60000 (69%)]\tLoss: 0.002851\n",
      "Train Epoch: 2 [41280/60000 (69%)]\tLoss: 0.005613\n",
      "Train Epoch: 2 [41344/60000 (69%)]\tLoss: 0.076241\n",
      "Train Epoch: 2 [41408/60000 (69%)]\tLoss: 0.002886\n",
      "Train Epoch: 2 [41472/60000 (69%)]\tLoss: 0.190642\n",
      "Train Epoch: 2 [41536/60000 (69%)]\tLoss: 0.097324\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.013725\n",
      "Train Epoch: 2 [41664/60000 (69%)]\tLoss: 0.061952\n",
      "Train Epoch: 2 [41728/60000 (70%)]\tLoss: 0.029426\n",
      "Train Epoch: 2 [41792/60000 (70%)]\tLoss: 0.020874\n",
      "Train Epoch: 2 [41856/60000 (70%)]\tLoss: 0.020237\n",
      "Train Epoch: 2 [41920/60000 (70%)]\tLoss: 0.096824\n",
      "Train Epoch: 2 [41984/60000 (70%)]\tLoss: 0.003992\n",
      "Train Epoch: 2 [42048/60000 (70%)]\tLoss: 0.028879\n",
      "Train Epoch: 2 [42112/60000 (70%)]\tLoss: 0.249345\n",
      "Train Epoch: 2 [42176/60000 (70%)]\tLoss: 0.013195\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.552964\n",
      "Train Epoch: 2 [42304/60000 (71%)]\tLoss: 0.004027\n",
      "Train Epoch: 2 [42368/60000 (71%)]\tLoss: 0.083920\n",
      "Train Epoch: 2 [42432/60000 (71%)]\tLoss: 0.035086\n",
      "Train Epoch: 2 [42496/60000 (71%)]\tLoss: 0.135850\n",
      "Train Epoch: 2 [42560/60000 (71%)]\tLoss: 0.003206\n",
      "Train Epoch: 2 [42624/60000 (71%)]\tLoss: 0.018427\n",
      "Train Epoch: 2 [42688/60000 (71%)]\tLoss: 0.001932\n",
      "Train Epoch: 2 [42752/60000 (71%)]\tLoss: 0.103792\n",
      "Train Epoch: 2 [42816/60000 (71%)]\tLoss: 0.076029\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.129294\n",
      "Train Epoch: 2 [42944/60000 (72%)]\tLoss: 0.035082\n",
      "Train Epoch: 2 [43008/60000 (72%)]\tLoss: 0.085460\n",
      "Train Epoch: 2 [43072/60000 (72%)]\tLoss: 0.031468\n",
      "Train Epoch: 2 [43136/60000 (72%)]\tLoss: 0.117590\n",
      "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.078373\n",
      "Train Epoch: 2 [43264/60000 (72%)]\tLoss: 0.056653\n",
      "Train Epoch: 2 [43328/60000 (72%)]\tLoss: 0.005855\n",
      "Train Epoch: 2 [43392/60000 (72%)]\tLoss: 0.029730\n",
      "Train Epoch: 2 [43456/60000 (72%)]\tLoss: 0.065688\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 0.507148\n",
      "Train Epoch: 2 [43584/60000 (73%)]\tLoss: 0.040391\n",
      "Train Epoch: 2 [43648/60000 (73%)]\tLoss: 0.013263\n",
      "Train Epoch: 2 [43712/60000 (73%)]\tLoss: 0.156869\n",
      "Train Epoch: 2 [43776/60000 (73%)]\tLoss: 0.077555\n",
      "Train Epoch: 2 [43840/60000 (73%)]\tLoss: 0.253284\n",
      "Train Epoch: 2 [43904/60000 (73%)]\tLoss: 0.591133\n",
      "Train Epoch: 2 [43968/60000 (73%)]\tLoss: 0.075667\n",
      "Train Epoch: 2 [44032/60000 (73%)]\tLoss: 0.010714\n",
      "Train Epoch: 2 [44096/60000 (73%)]\tLoss: 0.114645\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.082339\n",
      "Train Epoch: 2 [44224/60000 (74%)]\tLoss: 0.010879\n",
      "Train Epoch: 2 [44288/60000 (74%)]\tLoss: 0.204021\n",
      "Train Epoch: 2 [44352/60000 (74%)]\tLoss: 0.006378\n",
      "Train Epoch: 2 [44416/60000 (74%)]\tLoss: 0.092845\n",
      "Train Epoch: 2 [44480/60000 (74%)]\tLoss: 0.003979\n",
      "Train Epoch: 2 [44544/60000 (74%)]\tLoss: 0.028673\n",
      "Train Epoch: 2 [44608/60000 (74%)]\tLoss: 0.003024\n",
      "Train Epoch: 2 [44672/60000 (74%)]\tLoss: 0.033941\n",
      "Train Epoch: 2 [44736/60000 (75%)]\tLoss: 0.088129\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.223878\n",
      "Train Epoch: 2 [44864/60000 (75%)]\tLoss: 0.030402\n",
      "Train Epoch: 2 [44928/60000 (75%)]\tLoss: 0.055518\n",
      "Train Epoch: 2 [44992/60000 (75%)]\tLoss: 0.318240\n",
      "Train Epoch: 2 [45056/60000 (75%)]\tLoss: 0.053283\n",
      "Train Epoch: 2 [45120/60000 (75%)]\tLoss: 0.012092\n",
      "Train Epoch: 2 [45184/60000 (75%)]\tLoss: 0.279603\n",
      "Train Epoch: 2 [45248/60000 (75%)]\tLoss: 0.036139\n",
      "Train Epoch: 2 [45312/60000 (76%)]\tLoss: 0.112430\n",
      "Train Epoch: 2 [45376/60000 (76%)]\tLoss: 0.009179\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.239583\n",
      "Train Epoch: 2 [45504/60000 (76%)]\tLoss: 0.185298\n",
      "Train Epoch: 2 [45568/60000 (76%)]\tLoss: 0.025625\n",
      "Train Epoch: 2 [45632/60000 (76%)]\tLoss: 0.035212\n",
      "Train Epoch: 2 [45696/60000 (76%)]\tLoss: 0.040454\n",
      "Train Epoch: 2 [45760/60000 (76%)]\tLoss: 0.121276\n",
      "Train Epoch: 2 [45824/60000 (76%)]\tLoss: 0.074045\n",
      "Train Epoch: 2 [45888/60000 (76%)]\tLoss: 0.024631\n",
      "Train Epoch: 2 [45952/60000 (77%)]\tLoss: 0.011981\n",
      "Train Epoch: 2 [46016/60000 (77%)]\tLoss: 0.014084\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.046939\n",
      "Train Epoch: 2 [46144/60000 (77%)]\tLoss: 0.111213\n",
      "Train Epoch: 2 [46208/60000 (77%)]\tLoss: 0.000357\n",
      "Train Epoch: 2 [46272/60000 (77%)]\tLoss: 0.063954\n",
      "Train Epoch: 2 [46336/60000 (77%)]\tLoss: 0.088156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.155977\n",
      "Train Epoch: 2 [46464/60000 (77%)]\tLoss: 0.007192\n",
      "Train Epoch: 2 [46528/60000 (78%)]\tLoss: 0.001146\n",
      "Train Epoch: 2 [46592/60000 (78%)]\tLoss: 0.011712\n",
      "Train Epoch: 2 [46656/60000 (78%)]\tLoss: 0.016361\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.127432\n",
      "Train Epoch: 2 [46784/60000 (78%)]\tLoss: 0.060002\n",
      "Train Epoch: 2 [46848/60000 (78%)]\tLoss: 0.099834\n",
      "Train Epoch: 2 [46912/60000 (78%)]\tLoss: 0.073537\n",
      "Train Epoch: 2 [46976/60000 (78%)]\tLoss: 0.031607\n",
      "Train Epoch: 2 [47040/60000 (78%)]\tLoss: 0.009309\n",
      "Train Epoch: 2 [47104/60000 (79%)]\tLoss: 0.070596\n",
      "Train Epoch: 2 [47168/60000 (79%)]\tLoss: 0.005441\n",
      "Train Epoch: 2 [47232/60000 (79%)]\tLoss: 0.040860\n",
      "Train Epoch: 2 [47296/60000 (79%)]\tLoss: 0.045203\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.095416\n",
      "Train Epoch: 2 [47424/60000 (79%)]\tLoss: 0.039805\n",
      "Train Epoch: 2 [47488/60000 (79%)]\tLoss: 0.013368\n",
      "Train Epoch: 2 [47552/60000 (79%)]\tLoss: 0.010299\n",
      "Train Epoch: 2 [47616/60000 (79%)]\tLoss: 0.038184\n",
      "Train Epoch: 2 [47680/60000 (79%)]\tLoss: 0.134696\n",
      "Train Epoch: 2 [47744/60000 (80%)]\tLoss: 0.026200\n",
      "Train Epoch: 2 [47808/60000 (80%)]\tLoss: 0.146302\n",
      "Train Epoch: 2 [47872/60000 (80%)]\tLoss: 0.040601\n",
      "Train Epoch: 2 [47936/60000 (80%)]\tLoss: 0.028954\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.011328\n",
      "Train Epoch: 2 [48064/60000 (80%)]\tLoss: 0.097760\n",
      "Train Epoch: 2 [48128/60000 (80%)]\tLoss: 0.038523\n",
      "Train Epoch: 2 [48192/60000 (80%)]\tLoss: 0.038316\n",
      "Train Epoch: 2 [48256/60000 (80%)]\tLoss: 0.263364\n",
      "Train Epoch: 2 [48320/60000 (81%)]\tLoss: 0.016220\n",
      "Train Epoch: 2 [48384/60000 (81%)]\tLoss: 0.022579\n",
      "Train Epoch: 2 [48448/60000 (81%)]\tLoss: 0.062069\n",
      "Train Epoch: 2 [48512/60000 (81%)]\tLoss: 0.011239\n",
      "Train Epoch: 2 [48576/60000 (81%)]\tLoss: 0.111781\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.234101\n",
      "Train Epoch: 2 [48704/60000 (81%)]\tLoss: 0.007564\n",
      "Train Epoch: 2 [48768/60000 (81%)]\tLoss: 0.031894\n",
      "Train Epoch: 2 [48832/60000 (81%)]\tLoss: 0.021791\n",
      "Train Epoch: 2 [48896/60000 (81%)]\tLoss: 0.067335\n",
      "Train Epoch: 2 [48960/60000 (82%)]\tLoss: 0.345132\n",
      "Train Epoch: 2 [49024/60000 (82%)]\tLoss: 0.366437\n",
      "Train Epoch: 2 [49088/60000 (82%)]\tLoss: 0.135476\n",
      "Train Epoch: 2 [49152/60000 (82%)]\tLoss: 0.013189\n",
      "Train Epoch: 2 [49216/60000 (82%)]\tLoss: 0.092402\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.744946\n",
      "Train Epoch: 2 [49344/60000 (82%)]\tLoss: 0.079443\n",
      "Train Epoch: 2 [49408/60000 (82%)]\tLoss: 0.009065\n",
      "Train Epoch: 2 [49472/60000 (82%)]\tLoss: 0.000367\n",
      "Train Epoch: 2 [49536/60000 (83%)]\tLoss: 0.006361\n",
      "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.007369\n",
      "Train Epoch: 2 [49664/60000 (83%)]\tLoss: 0.123886\n",
      "Train Epoch: 2 [49728/60000 (83%)]\tLoss: 0.032242\n",
      "Train Epoch: 2 [49792/60000 (83%)]\tLoss: 0.155886\n",
      "Train Epoch: 2 [49856/60000 (83%)]\tLoss: 0.008397\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.023469\n",
      "Train Epoch: 2 [49984/60000 (83%)]\tLoss: 0.104254\n",
      "Train Epoch: 2 [50048/60000 (83%)]\tLoss: 0.066344\n",
      "Train Epoch: 2 [50112/60000 (84%)]\tLoss: 0.069185\n",
      "Train Epoch: 2 [50176/60000 (84%)]\tLoss: 0.278129\n",
      "Train Epoch: 2 [50240/60000 (84%)]\tLoss: 0.091909\n",
      "Train Epoch: 2 [50304/60000 (84%)]\tLoss: 0.009677\n",
      "Train Epoch: 2 [50368/60000 (84%)]\tLoss: 0.343561\n",
      "Train Epoch: 2 [50432/60000 (84%)]\tLoss: 0.054119\n",
      "Train Epoch: 2 [50496/60000 (84%)]\tLoss: 0.009622\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.081714\n",
      "Train Epoch: 2 [50624/60000 (84%)]\tLoss: 0.028554\n",
      "Train Epoch: 2 [50688/60000 (84%)]\tLoss: 0.149191\n",
      "Train Epoch: 2 [50752/60000 (85%)]\tLoss: 0.009117\n",
      "Train Epoch: 2 [50816/60000 (85%)]\tLoss: 0.037884\n",
      "Train Epoch: 2 [50880/60000 (85%)]\tLoss: 0.039888\n",
      "Train Epoch: 2 [50944/60000 (85%)]\tLoss: 0.262291\n",
      "Train Epoch: 2 [51008/60000 (85%)]\tLoss: 0.036542\n",
      "Train Epoch: 2 [51072/60000 (85%)]\tLoss: 0.088932\n",
      "Train Epoch: 2 [51136/60000 (85%)]\tLoss: 0.082277\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.029688\n",
      "Train Epoch: 2 [51264/60000 (85%)]\tLoss: 0.070565\n",
      "Train Epoch: 2 [51328/60000 (86%)]\tLoss: 0.104822\n",
      "Train Epoch: 2 [51392/60000 (86%)]\tLoss: 0.013628\n",
      "Train Epoch: 2 [51456/60000 (86%)]\tLoss: 0.502835\n",
      "Train Epoch: 2 [51520/60000 (86%)]\tLoss: 0.069805\n",
      "Train Epoch: 2 [51584/60000 (86%)]\tLoss: 0.022724\n",
      "Train Epoch: 2 [51648/60000 (86%)]\tLoss: 0.001010\n",
      "Train Epoch: 2 [51712/60000 (86%)]\tLoss: 0.088544\n",
      "Train Epoch: 2 [51776/60000 (86%)]\tLoss: 0.012531\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.028632\n",
      "Train Epoch: 2 [51904/60000 (87%)]\tLoss: 0.013170\n",
      "Train Epoch: 2 [51968/60000 (87%)]\tLoss: 0.258497\n",
      "Train Epoch: 2 [52032/60000 (87%)]\tLoss: 0.039952\n",
      "Train Epoch: 2 [52096/60000 (87%)]\tLoss: 0.015859\n",
      "Train Epoch: 2 [52160/60000 (87%)]\tLoss: 0.258634\n",
      "Train Epoch: 2 [52224/60000 (87%)]\tLoss: 0.076601\n",
      "Train Epoch: 2 [52288/60000 (87%)]\tLoss: 0.191014\n",
      "Train Epoch: 2 [52352/60000 (87%)]\tLoss: 0.082808\n",
      "Train Epoch: 2 [52416/60000 (87%)]\tLoss: 0.011861\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.035764\n",
      "Train Epoch: 2 [52544/60000 (88%)]\tLoss: 0.096721\n",
      "Train Epoch: 2 [52608/60000 (88%)]\tLoss: 0.063675\n",
      "Train Epoch: 2 [52672/60000 (88%)]\tLoss: 0.131871\n",
      "Train Epoch: 2 [52736/60000 (88%)]\tLoss: 0.092052\n",
      "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.102696\n",
      "Train Epoch: 2 [52864/60000 (88%)]\tLoss: 0.059128\n",
      "Train Epoch: 2 [52928/60000 (88%)]\tLoss: 0.136508\n",
      "Train Epoch: 2 [52992/60000 (88%)]\tLoss: 0.053935\n",
      "Train Epoch: 2 [53056/60000 (88%)]\tLoss: 0.389016\n",
      "Train Epoch: 2 [53120/60000 (89%)]\tLoss: 0.243536\n",
      "Train Epoch: 2 [53184/60000 (89%)]\tLoss: 0.090382\n",
      "Train Epoch: 2 [53248/60000 (89%)]\tLoss: 0.047891\n",
      "Train Epoch: 2 [53312/60000 (89%)]\tLoss: 0.294454\n",
      "Train Epoch: 2 [53376/60000 (89%)]\tLoss: 0.057928\n",
      "Train Epoch: 2 [53440/60000 (89%)]\tLoss: 0.389705\n",
      "Train Epoch: 2 [53504/60000 (89%)]\tLoss: 0.078049\n",
      "Train Epoch: 2 [53568/60000 (89%)]\tLoss: 0.037511\n",
      "Train Epoch: 2 [53632/60000 (89%)]\tLoss: 0.055146\n",
      "Train Epoch: 2 [53696/60000 (89%)]\tLoss: 0.023550\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.002051\n",
      "Train Epoch: 2 [53824/60000 (90%)]\tLoss: 0.049543\n",
      "Train Epoch: 2 [53888/60000 (90%)]\tLoss: 0.002780\n",
      "Train Epoch: 2 [53952/60000 (90%)]\tLoss: 0.159325\n",
      "Train Epoch: 2 [54016/60000 (90%)]\tLoss: 0.073973\n",
      "Train Epoch: 2 [54080/60000 (90%)]\tLoss: 0.135864\n",
      "Train Epoch: 2 [54144/60000 (90%)]\tLoss: 0.011385\n",
      "Train Epoch: 2 [54208/60000 (90%)]\tLoss: 0.009612\n",
      "Train Epoch: 2 [54272/60000 (90%)]\tLoss: 0.078992\n",
      "Train Epoch: 2 [54336/60000 (91%)]\tLoss: 0.027539\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.079290\n",
      "Train Epoch: 2 [54464/60000 (91%)]\tLoss: 0.007428\n",
      "Train Epoch: 2 [54528/60000 (91%)]\tLoss: 0.142652\n",
      "Train Epoch: 2 [54592/60000 (91%)]\tLoss: 0.039477\n",
      "Train Epoch: 2 [54656/60000 (91%)]\tLoss: 0.131068\n",
      "Train Epoch: 2 [54720/60000 (91%)]\tLoss: 0.027541\n",
      "Train Epoch: 2 [54784/60000 (91%)]\tLoss: 0.455527\n",
      "Train Epoch: 2 [54848/60000 (91%)]\tLoss: 0.045853\n",
      "Train Epoch: 2 [54912/60000 (92%)]\tLoss: 0.051173\n",
      "Train Epoch: 2 [54976/60000 (92%)]\tLoss: 0.038654\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.001076\n",
      "Train Epoch: 2 [55104/60000 (92%)]\tLoss: 0.024190\n",
      "Train Epoch: 2 [55168/60000 (92%)]\tLoss: 0.072056\n",
      "Train Epoch: 2 [55232/60000 (92%)]\tLoss: 0.019315\n",
      "Train Epoch: 2 [55296/60000 (92%)]\tLoss: 0.005405\n",
      "Train Epoch: 2 [55360/60000 (92%)]\tLoss: 0.227301\n",
      "Train Epoch: 2 [55424/60000 (92%)]\tLoss: 0.025306\n",
      "Train Epoch: 2 [55488/60000 (92%)]\tLoss: 0.165652\n",
      "Train Epoch: 2 [55552/60000 (93%)]\tLoss: 0.000538\n",
      "Train Epoch: 2 [55616/60000 (93%)]\tLoss: 0.213758\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.010866\n",
      "Train Epoch: 2 [55744/60000 (93%)]\tLoss: 0.180094\n",
      "Train Epoch: 2 [55808/60000 (93%)]\tLoss: 0.178804\n",
      "Train Epoch: 2 [55872/60000 (93%)]\tLoss: 0.229093\n",
      "Train Epoch: 2 [55936/60000 (93%)]\tLoss: 0.105773\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.008373\n",
      "Train Epoch: 2 [56064/60000 (93%)]\tLoss: 0.024661\n",
      "Train Epoch: 2 [56128/60000 (94%)]\tLoss: 0.068955\n",
      "Train Epoch: 2 [56192/60000 (94%)]\tLoss: 0.184029\n",
      "Train Epoch: 2 [56256/60000 (94%)]\tLoss: 0.033636\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.037014\n",
      "Train Epoch: 2 [56384/60000 (94%)]\tLoss: 0.147934\n",
      "Train Epoch: 2 [56448/60000 (94%)]\tLoss: 0.003267\n",
      "Train Epoch: 2 [56512/60000 (94%)]\tLoss: 0.437157\n",
      "Train Epoch: 2 [56576/60000 (94%)]\tLoss: 0.047246\n",
      "Train Epoch: 2 [56640/60000 (94%)]\tLoss: 0.028430\n",
      "Train Epoch: 2 [56704/60000 (95%)]\tLoss: 0.010535\n",
      "Train Epoch: 2 [56768/60000 (95%)]\tLoss: 0.012078\n",
      "Train Epoch: 2 [56832/60000 (95%)]\tLoss: 0.006670\n",
      "Train Epoch: 2 [56896/60000 (95%)]\tLoss: 0.010134\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.097282\n",
      "Train Epoch: 2 [57024/60000 (95%)]\tLoss: 0.012525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [57088/60000 (95%)]\tLoss: 0.002503\n",
      "Train Epoch: 2 [57152/60000 (95%)]\tLoss: 0.002392\n",
      "Train Epoch: 2 [57216/60000 (95%)]\tLoss: 0.051203\n",
      "Train Epoch: 2 [57280/60000 (95%)]\tLoss: 0.014912\n",
      "Train Epoch: 2 [57344/60000 (96%)]\tLoss: 0.049903\n",
      "Train Epoch: 2 [57408/60000 (96%)]\tLoss: 0.055304\n",
      "Train Epoch: 2 [57472/60000 (96%)]\tLoss: 0.002309\n",
      "Train Epoch: 2 [57536/60000 (96%)]\tLoss: 0.053464\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.009097\n",
      "Train Epoch: 2 [57664/60000 (96%)]\tLoss: 0.265256\n",
      "Train Epoch: 2 [57728/60000 (96%)]\tLoss: 0.003290\n",
      "Train Epoch: 2 [57792/60000 (96%)]\tLoss: 0.043840\n",
      "Train Epoch: 2 [57856/60000 (96%)]\tLoss: 0.132206\n",
      "Train Epoch: 2 [57920/60000 (97%)]\tLoss: 0.012922\n",
      "Train Epoch: 2 [57984/60000 (97%)]\tLoss: 0.010690\n",
      "Train Epoch: 2 [58048/60000 (97%)]\tLoss: 0.031770\n",
      "Train Epoch: 2 [58112/60000 (97%)]\tLoss: 0.157209\n",
      "Train Epoch: 2 [58176/60000 (97%)]\tLoss: 0.062201\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.094299\n",
      "Train Epoch: 2 [58304/60000 (97%)]\tLoss: 0.017061\n",
      "Train Epoch: 2 [58368/60000 (97%)]\tLoss: 0.175920\n",
      "Train Epoch: 2 [58432/60000 (97%)]\tLoss: 0.134604\n",
      "Train Epoch: 2 [58496/60000 (97%)]\tLoss: 0.240977\n",
      "Train Epoch: 2 [58560/60000 (98%)]\tLoss: 0.035245\n",
      "Train Epoch: 2 [58624/60000 (98%)]\tLoss: 0.307870\n",
      "Train Epoch: 2 [58688/60000 (98%)]\tLoss: 0.157359\n",
      "Train Epoch: 2 [58752/60000 (98%)]\tLoss: 0.000354\n",
      "Train Epoch: 2 [58816/60000 (98%)]\tLoss: 0.122991\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.036675\n",
      "Train Epoch: 2 [58944/60000 (98%)]\tLoss: 0.005889\n",
      "Train Epoch: 2 [59008/60000 (98%)]\tLoss: 0.241173\n",
      "Train Epoch: 2 [59072/60000 (98%)]\tLoss: 0.046229\n",
      "Train Epoch: 2 [59136/60000 (99%)]\tLoss: 0.034409\n",
      "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.057195\n",
      "Train Epoch: 2 [59264/60000 (99%)]\tLoss: 0.006843\n",
      "Train Epoch: 2 [59328/60000 (99%)]\tLoss: 0.014509\n",
      "Train Epoch: 2 [59392/60000 (99%)]\tLoss: 0.201179\n",
      "Train Epoch: 2 [59456/60000 (99%)]\tLoss: 0.015039\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.058815\n",
      "Train Epoch: 2 [59584/60000 (99%)]\tLoss: 0.195383\n",
      "Train Epoch: 2 [59648/60000 (99%)]\tLoss: 0.005493\n",
      "Train Epoch: 2 [59712/60000 (100%)]\tLoss: 0.017172\n",
      "Train Epoch: 2 [59776/60000 (100%)]\tLoss: 0.136170\n",
      "Train Epoch: 2 [59840/60000 (100%)]\tLoss: 0.147761\n",
      "Train Epoch: 2 [59904/60000 (100%)]\tLoss: 0.032825\n",
      "Train Epoch: 2 [59968/60000 (100%)]\tLoss: 0.002475\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9871/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "if save_model:\n",
    "    torch.save(model.state_dict(), dir_data_path + \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:34:39.995950Z",
     "start_time": "2020-01-04T19:34:39.981666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight', tensor([[[[ 0.0808, -0.0138, -0.0865],\n",
       "                        [-0.2602,  0.2034, -0.1033],\n",
       "                        [-0.0114, -0.1363,  0.1045]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2502,  0.1108, -0.2679],\n",
       "                        [-0.3166, -0.1286,  0.1146],\n",
       "                        [ 0.0654, -0.4153,  0.2533]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4790,  0.2735,  0.0808],\n",
       "                        [-0.2327, -0.3207,  0.1925],\n",
       "                        [ 0.2412,  0.0986, -0.4483]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1437,  0.1575,  0.1093],\n",
       "                        [ 0.1731, -0.1826, -0.2464],\n",
       "                        [-0.0443,  0.0023,  0.1469]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0151,  0.0198,  0.1989],\n",
       "                        [-0.2915,  0.3064, -0.3274],\n",
       "                        [-0.1967, -0.2246, -0.1181]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0644,  0.2313, -0.2676],\n",
       "                        [ 0.0334, -0.2822, -0.0016],\n",
       "                        [ 0.1714,  0.1260, -0.1072]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1467,  0.0791,  0.0827],\n",
       "                        [-0.2650,  0.1273, -0.2551],\n",
       "                        [-0.0901,  0.0403,  0.1516]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2148, -0.2153,  0.0883],\n",
       "                        [-0.1283,  0.0832, -0.2285],\n",
       "                        [-0.0792,  0.0372,  0.0980]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0244, -0.1465,  0.1342],\n",
       "                        [-0.0240, -0.2740,  0.1002],\n",
       "                        [ 0.0381,  0.0329, -0.1578]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1493, -0.1632,  0.0151],\n",
       "                        [-0.0975, -0.2756, -0.0126],\n",
       "                        [ 0.1190,  0.1839,  0.0873]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0682, -0.1530,  0.1441],\n",
       "                        [-0.0445, -0.3694, -0.0062],\n",
       "                        [ 0.2036,  0.0598,  0.0437]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2614,  0.0460,  0.1983],\n",
       "                        [ 0.0450,  0.1971, -0.0814],\n",
       "                        [ 0.2015, -0.0550, -0.1914]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0928,  0.0848, -0.2755],\n",
       "                        [-0.1172,  0.0170, -0.0294],\n",
       "                        [ 0.2571, -0.1277, -0.2576]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1496,  0.1571, -0.1819],\n",
       "                        [-0.0962,  0.1127, -0.1668],\n",
       "                        [-0.1556,  0.1089,  0.1445]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0870,  0.0211, -0.0913],\n",
       "                        [-0.1399, -0.0835, -0.1117],\n",
       "                        [-0.0738,  0.0029,  0.1516]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1079, -0.0177,  0.1168],\n",
       "                        [ 0.0067, -0.0370, -0.0809],\n",
       "                        [ 0.2197, -0.2973,  0.1308]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1861,  0.1232,  0.0742],\n",
       "                        [-0.1999, -0.3200, -0.0379],\n",
       "                        [ 0.2390, -0.3161,  0.0988]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0982,  0.2309,  0.0488],\n",
       "                        [-0.1368, -0.1867, -0.2397],\n",
       "                        [-0.0744,  0.0527,  0.0745]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2630,  0.1317, -0.4288],\n",
       "                        [-0.4375, -0.2021, -0.1357],\n",
       "                        [-0.0396, -0.0676,  0.3232]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0574,  0.1566, -0.2952],\n",
       "                        [ 0.2107, -0.2450,  0.1695],\n",
       "                        [-0.1751, -0.2227, -0.0057]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.4202,  0.0424, -0.2670],\n",
       "                        [-0.1236, -0.0763, -0.2047],\n",
       "                        [ 0.0887,  0.2315,  0.0210]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0484, -0.1801,  0.2564],\n",
       "                        [-0.1939, -0.3009,  0.2524],\n",
       "                        [-0.2403, -0.0790, -0.0222]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0215, -0.1928, -0.1828],\n",
       "                        [-0.3384, -0.0073, -0.3604],\n",
       "                        [ 0.1412,  0.1138,  0.0267]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3526, -0.2594, -0.2323],\n",
       "                        [ 0.1066, -0.1059, -0.0305],\n",
       "                        [-0.1443, -0.2961,  0.0101]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1185, -0.0710, -0.0586],\n",
       "                        [-0.0429,  0.1234,  0.1246],\n",
       "                        [ 0.2487, -0.3684,  0.0190]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0084,  0.1065,  0.1556],\n",
       "                        [ 0.0015, -0.0066,  0.0451],\n",
       "                        [-0.2651, -0.1923, -0.3567]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1344,  0.2504, -0.2744],\n",
       "                        [-0.2913,  0.1115,  0.1390],\n",
       "                        [ 0.1436, -0.2816,  0.0738]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1169, -0.1453,  0.1024],\n",
       "                        [-0.0379, -0.0434, -0.0285],\n",
       "                        [-0.1837,  0.0194,  0.2018]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0502, -0.1122,  0.1698],\n",
       "                        [-0.2436, -0.1593, -0.0128],\n",
       "                        [ 0.2292, -0.0371, -0.1365]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0250, -0.0592, -0.0117],\n",
       "                        [-0.2638,  0.1502, -0.2177],\n",
       "                        [ 0.1983, -0.2308, -0.0567]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0710, -0.1923, -0.0545],\n",
       "                        [ 0.1488, -0.3096,  0.2711],\n",
       "                        [-0.0140,  0.2442, -0.1513]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0477, -0.1876,  0.0879],\n",
       "                        [ 0.0473,  0.1285, -0.2908],\n",
       "                        [ 0.0512, -0.0409,  0.0397]]]])),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.3546, -0.3950, -0.2683, -0.2425, -0.2771, -0.3681, -0.3667, -0.2424,\n",
       "                      -0.1745, -0.2133, -0.2315, -0.4834, -0.4375, -0.2052, -0.1357, -0.2332,\n",
       "                      -0.3839, -0.2429, -0.2795, -0.1872, -0.3346, -0.3442, -0.3924, -0.3208,\n",
       "                      -0.4090, -0.2697, -0.1145, -0.2366, -0.1318, -0.2052, -0.2647, -0.1893])),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-7.5535e-02,  2.3248e-02,  8.2568e-02],\n",
       "                        [-1.1702e-02, -4.9127e-02, -4.4049e-03],\n",
       "                        [ 6.6011e-02,  5.6411e-02,  1.5143e-02]],\n",
       "              \n",
       "                       [[-5.9659e-02, -1.1124e-01, -1.6936e-01],\n",
       "                        [-3.4144e-02,  3.2649e-02, -4.0567e-02],\n",
       "                        [ 1.9325e-02, -5.1817e-04, -1.4351e-01]],\n",
       "              \n",
       "                       [[-1.4214e-03, -9.2244e-03,  3.4200e-02],\n",
       "                        [-2.5620e-02, -4.0302e-02,  5.6013e-02],\n",
       "                        [ 5.8548e-02, -3.8367e-02,  4.4059e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 5.6194e-02, -4.6831e-03, -5.1920e-03],\n",
       "                        [-2.7924e-03, -4.4646e-02,  5.4736e-02],\n",
       "                        [-6.2767e-02,  1.1669e-02,  6.0280e-02]],\n",
       "              \n",
       "                       [[ 1.3181e-02, -8.4531e-03, -6.3731e-02],\n",
       "                        [-2.3470e-02, -6.6161e-02, -8.7751e-03],\n",
       "                        [-2.7993e-02,  6.1610e-02,  3.8648e-02]],\n",
       "              \n",
       "                       [[-8.4959e-02, -9.8857e-02, -4.3266e-03],\n",
       "                        [ 3.1099e-02, -4.9818e-02, -2.8813e-02],\n",
       "                        [-6.4471e-02, -7.7713e-02, -2.5774e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 3.9482e-02, -9.5957e-02, -5.5438e-02],\n",
       "                        [-4.1035e-02, -2.7011e-04, -8.8640e-02],\n",
       "                        [-3.4467e-02, -1.7801e-02,  1.9584e-02]],\n",
       "              \n",
       "                       [[-4.1457e-02, -2.3881e-02,  4.3959e-02],\n",
       "                        [ 4.6312e-04,  1.5948e-01,  2.6167e-03],\n",
       "                        [ 9.7850e-02, -1.2480e-02,  3.5191e-02]],\n",
       "              \n",
       "                       [[ 1.8718e-01,  3.8990e-02, -2.9818e-02],\n",
       "                        [ 1.1177e-01, -4.4633e-02, -2.1276e-02],\n",
       "                        [-7.9262e-02, -5.5463e-04, -2.9257e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.9424e-02, -9.3783e-03, -2.4047e-02],\n",
       "                        [-4.8262e-02, -7.1565e-02,  1.0662e-02],\n",
       "                        [-3.7329e-02, -1.8439e-02, -1.8744e-03]],\n",
       "              \n",
       "                       [[ 1.2689e-02, -3.2205e-03,  4.7409e-02],\n",
       "                        [-5.7990e-02,  1.9521e-02,  9.4098e-02],\n",
       "                        [ 4.3869e-02,  1.0608e-01, -1.0428e-01]],\n",
       "              \n",
       "                       [[ 1.3910e-02,  6.7760e-02,  1.0584e-01],\n",
       "                        [-1.5169e-02, -9.8626e-02,  4.9845e-02],\n",
       "                        [-3.3212e-02, -3.9469e-03, -2.6077e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 6.8493e-02, -2.1726e-02, -5.6204e-02],\n",
       "                        [ 5.8337e-02,  3.7715e-02,  5.6642e-03],\n",
       "                        [-8.1125e-02,  1.1903e-02,  1.9011e-02]],\n",
       "              \n",
       "                       [[ 4.6669e-02,  4.9270e-04,  2.3071e-02],\n",
       "                        [ 4.5454e-02,  4.0448e-02, -1.2975e-01],\n",
       "                        [-1.0838e-01, -1.3654e-01, -7.9024e-02]],\n",
       "              \n",
       "                       [[-9.7198e-04,  7.4290e-03, -1.0342e-01],\n",
       "                        [-5.9697e-03,  5.1067e-02, -4.8098e-02],\n",
       "                        [ 4.1865e-02,  1.7511e-01,  7.7066e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.8869e-02, -6.0874e-02, -2.0271e-02],\n",
       "                        [-6.2014e-02,  3.4768e-02, -4.0347e-02],\n",
       "                        [-6.1829e-02, -7.1451e-02,  9.8481e-02]],\n",
       "              \n",
       "                       [[-7.0760e-02,  5.6072e-02,  7.8452e-02],\n",
       "                        [ 7.6344e-02,  3.8344e-02,  4.3578e-02],\n",
       "                        [-3.9760e-03,  1.6736e-02, -6.1470e-03]],\n",
       "              \n",
       "                       [[-4.7659e-02, -6.4144e-02, -4.3437e-02],\n",
       "                        [-2.6569e-02, -1.0558e-01, -3.0013e-02],\n",
       "                        [ 4.4533e-02, -5.9417e-02, -2.7872e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[-5.3338e-05,  2.8992e-02, -5.8211e-02],\n",
       "                        [-4.0081e-02, -7.3073e-02, -4.5075e-02],\n",
       "                        [-1.8578e-02, -8.0999e-02, -4.3756e-03]],\n",
       "              \n",
       "                       [[-4.7468e-02,  1.2187e-01,  7.7757e-02],\n",
       "                        [-5.5313e-02,  3.4684e-02,  1.3226e-01],\n",
       "                        [-1.6285e-02,  5.4379e-02,  3.0520e-02]],\n",
       "              \n",
       "                       [[-6.3552e-02, -4.7146e-02, -9.9145e-02],\n",
       "                        [-5.6439e-02, -5.9650e-02, -4.1388e-02],\n",
       "                        [-1.2414e-02, -7.2135e-02, -1.6673e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.6606e-02, -2.8989e-02,  1.6289e-02],\n",
       "                        [ 5.1508e-02, -4.5384e-02,  3.7124e-02],\n",
       "                        [-3.3544e-02,  1.7265e-02, -4.7395e-02]],\n",
       "              \n",
       "                       [[-7.5964e-02, -8.8446e-02, -3.2628e-02],\n",
       "                        [-6.9812e-02, -5.6072e-02, -1.3858e-01],\n",
       "                        [-8.6347e-02,  2.3999e-02, -1.3646e-01]],\n",
       "              \n",
       "                       [[ 4.3394e-02,  2.2671e-03,  2.0909e-02],\n",
       "                        [-3.9172e-02, -7.1640e-02,  8.7616e-03],\n",
       "                        [-5.9855e-02, -7.4552e-02, -2.8786e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.1516e-02, -6.6568e-02,  2.9845e-02],\n",
       "                        [ 6.8636e-02,  6.5422e-03, -4.5248e-02],\n",
       "                        [ 5.0217e-02, -2.4765e-02,  4.5586e-02]],\n",
       "              \n",
       "                       [[-1.0440e-02, -1.4175e-02, -1.4423e-01],\n",
       "                        [-9.7887e-02, -5.6605e-02,  1.9838e-02],\n",
       "                        [-4.3137e-03,  1.6095e-01, -8.0832e-02]],\n",
       "              \n",
       "                       [[ 2.0091e-01,  5.7718e-02,  2.1464e-01],\n",
       "                        [ 7.2649e-02,  1.5467e-01,  3.4703e-02],\n",
       "                        [ 8.3154e-02, -1.8077e-02,  2.2367e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.7505e-02, -8.2929e-03, -1.2381e-02],\n",
       "                        [ 5.9584e-02, -1.3483e-02,  1.2289e-02],\n",
       "                        [ 1.4296e-02,  4.2716e-03, -4.2829e-02]],\n",
       "              \n",
       "                       [[ 9.5692e-02,  2.0018e-01,  1.9820e-01],\n",
       "                        [ 1.3869e-01,  1.0095e-01,  1.0658e-02],\n",
       "                        [ 1.6340e-02, -3.2883e-02, -4.2408e-02]],\n",
       "              \n",
       "                       [[-3.5310e-02,  2.9449e-02, -4.6472e-02],\n",
       "                        [-5.5067e-02,  1.5546e-02,  3.9323e-02],\n",
       "                        [-3.1635e-02, -6.3176e-02,  1.4589e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 2.1954e-02, -4.3411e-02, -9.6498e-02],\n",
       "                        [ 4.8500e-02,  2.6477e-03, -5.2305e-02],\n",
       "                        [ 8.8036e-02,  6.5010e-02, -1.0062e-02]],\n",
       "              \n",
       "                       [[-1.0942e-01, -5.0130e-02, -9.1933e-02],\n",
       "                        [ 2.1093e-03, -1.0914e-01, -1.8450e-02],\n",
       "                        [-6.6369e-02, -1.2044e-01, -5.6942e-02]],\n",
       "              \n",
       "                       [[-4.7246e-02, -8.4128e-02, -1.0794e-01],\n",
       "                        [-6.1711e-02, -2.8371e-02, -2.9642e-02],\n",
       "                        [ 2.2259e-02,  5.2927e-02, -6.4779e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 2.8201e-02,  8.7048e-02,  5.6583e-02],\n",
       "                        [-4.8990e-02, -7.3182e-02, -1.1584e-03],\n",
       "                        [-2.0964e-02, -1.1400e-01, -1.3213e-01]],\n",
       "              \n",
       "                       [[ 4.3316e-02, -1.4873e-01, -9.2504e-02],\n",
       "                        [-5.1019e-02, -2.7184e-02,  7.9513e-02],\n",
       "                        [-8.7522e-02, -9.8668e-03,  4.2161e-02]],\n",
       "              \n",
       "                       [[-4.9123e-02, -9.5989e-03, -3.4871e-02],\n",
       "                        [ 5.2948e-02,  2.0882e-02, -9.2693e-03],\n",
       "                        [-4.5478e-02, -3.8346e-02, -6.5452e-02]]]])),\n",
       "             ('conv2.bias',\n",
       "              tensor([-6.0434e-02, -1.1418e-01, -5.5199e-02, -3.9564e-02, -2.6292e-02,\n",
       "                      -1.2476e-01, -3.4370e-03, -7.9524e-02, -8.3807e-02, -6.9042e-02,\n",
       "                      -1.8390e-02,  1.2290e-02, -5.5333e-02, -6.9705e-02, -5.3852e-02,\n",
       "                      -6.5255e-02, -3.6156e-02, -9.7215e-02, -8.4686e-02, -1.2129e-02,\n",
       "                       5.7865e-03, -2.7545e-03, -5.2874e-02, -6.4826e-02, -8.4183e-02,\n",
       "                      -7.3944e-02,  2.2541e-02, -2.8379e-02, -1.0191e-01, -1.3271e-01,\n",
       "                      -4.0015e-02, -1.1335e-01, -7.7614e-02,  9.4727e-03, -8.3372e-02,\n",
       "                       4.9101e-03, -8.2291e-02, -7.2916e-03, -4.3373e-02,  2.8251e-02,\n",
       "                      -7.4947e-02, -7.0453e-02, -3.0497e-02, -6.5774e-02,  1.8097e-02,\n",
       "                      -8.4009e-02,  4.8960e-02, -4.1650e-02, -3.8664e-02, -4.4474e-02,\n",
       "                      -3.0692e-02, -3.9603e-02, -3.6482e-02, -7.1881e-02, -2.1548e-02,\n",
       "                      -1.3871e-02, -7.2244e-02, -6.8546e-02,  1.7176e-03, -7.7117e-02,\n",
       "                      -9.8794e-03, -2.4013e-03, -1.0761e-01, -2.6554e-06])),\n",
       "             ('fc1.weight',\n",
       "              tensor([[ 2.2947e-02,  9.9141e-03,  1.9429e-02,  ...,  3.4588e-02,\n",
       "                        3.3350e-03,  8.5210e-03],\n",
       "                      [ 5.7146e-04,  1.0723e-02, -5.6621e-03,  ...,  1.7496e-02,\n",
       "                        1.9124e-02,  1.1409e-02],\n",
       "                      [ 2.4930e-03,  2.6352e-02,  1.4904e-02,  ...,  5.3877e-03,\n",
       "                        1.7916e-02,  1.8568e-02],\n",
       "                      ...,\n",
       "                      [-1.5478e-02,  4.9195e-05, -1.2106e-02,  ..., -1.1008e-03,\n",
       "                       -1.9559e-02, -8.1930e-03],\n",
       "                      [ 2.3099e-03,  1.0983e-02,  1.0582e-02,  ..., -9.7764e-03,\n",
       "                        5.9563e-03,  6.2410e-03],\n",
       "                      [ 7.6704e-04,  6.0056e-03,  5.4847e-03,  ...,  6.0019e-03,\n",
       "                        7.9047e-03, -5.9033e-03]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([-7.4833e-03,  2.3420e-02,  8.9481e-04,  1.6995e-02, -9.2458e-03,\n",
       "                      -1.5290e-02, -3.4651e-02, -1.0793e-02, -3.9931e-02,  4.8481e-02,\n",
       "                       3.4797e-02, -1.4192e-02,  3.9464e-03,  5.2670e-02,  5.7711e-02,\n",
       "                       8.0348e-02, -2.8558e-02, -1.4194e-02,  5.4810e-02, -6.9693e-02,\n",
       "                      -5.1182e-02, -3.7674e-02, -2.2420e-02,  2.4005e-02,  3.0392e-03,\n",
       "                      -5.7930e-02, -1.9340e-02,  1.2393e-02, -8.6512e-03,  1.7914e-02,\n",
       "                      -2.2801e-02, -4.8604e-02, -2.8131e-02,  6.3211e-03,  3.5270e-02,\n",
       "                      -5.5347e-03,  3.2155e-03,  2.4553e-03, -1.2229e-02,  5.3585e-02,\n",
       "                      -4.4888e-02, -7.4001e-02,  1.6490e-02, -2.1899e-02,  1.8015e-03,\n",
       "                      -2.1861e-02, -7.0015e-02,  2.4759e-02,  5.3536e-02, -9.6924e-02,\n",
       "                      -2.0195e-02, -2.8955e-02, -5.0551e-02,  2.3061e-02,  5.5163e-02,\n",
       "                       3.6539e-02,  2.1994e-02,  2.2393e-02,  4.4621e-02,  1.3293e-02,\n",
       "                       1.2885e-02,  1.5043e-02,  8.1891e-02, -2.7387e-02, -4.5085e-02,\n",
       "                      -2.8594e-02,  7.3954e-02,  1.8239e-02, -6.4900e-02, -2.5535e-02,\n",
       "                      -6.5961e-03, -6.9366e-02,  7.5008e-02, -1.9530e-02, -4.3304e-03,\n",
       "                      -5.0153e-02,  6.3281e-02, -5.2452e-02, -3.6309e-02,  3.7257e-02,\n",
       "                      -4.9245e-02, -1.4348e-02,  9.0620e-03,  2.5813e-02,  1.6934e-02,\n",
       "                       7.1028e-02, -7.4224e-03, -2.4321e-03, -3.6073e-02, -2.7485e-02,\n",
       "                      -8.1635e-02, -2.2486e-02, -1.5711e-02, -1.4143e-02, -7.4931e-05,\n",
       "                       8.5251e-02,  4.2324e-02, -3.9050e-02, -2.7720e-02,  8.4328e-02,\n",
       "                      -4.9597e-02, -3.0294e-03,  6.3266e-03, -2.9786e-02,  1.6149e-02,\n",
       "                      -3.4668e-02,  1.0022e-01,  5.7908e-02,  4.3734e-02,  2.3281e-02,\n",
       "                       8.0450e-03, -7.2820e-02,  1.3119e-02,  8.5829e-03,  3.2093e-02,\n",
       "                       2.7099e-02, -1.1237e-02, -1.1405e-02,  1.9141e-02, -1.1610e-02,\n",
       "                       3.1388e-02, -9.6889e-02,  1.3016e-03, -3.6274e-02,  3.0493e-02,\n",
       "                      -1.9224e-02,  3.4381e-02, -7.0993e-02])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.3140, -0.2482,  0.0409,  ..., -0.2867, -0.2151, -0.0130],\n",
       "                      [-0.0474,  0.0396, -0.1368,  ..., -0.2678, -0.2529, -0.3131],\n",
       "                      [-0.2080, -0.2237,  0.0371,  ...,  0.0119, -0.1426,  0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0730,  0.0617, -0.3228,  ..., -0.1553,  0.0921, -0.1119],\n",
       "                      [-0.2530, -0.2778, -0.2412,  ...,  0.0233, -0.2668,  0.0078],\n",
       "                      [ 0.0606, -0.2021, -0.2960,  ..., -0.2975,  0.0219, -0.0452]])),\n",
       "             ('fc2.bias',\n",
       "              tensor([-0.1505, -0.0070, -0.0527, -0.0502,  0.0239, -0.0280, -0.2330,  0.0295,\n",
       "                       0.0172, -0.0508]))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:34:40.004904Z",
     "start_time": "2020-01-04T19:34:39.997757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()\n",
    "\n",
    "# odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', \n",
    "#             'fc1.bias', 'fc2.weight', 'fc2.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:37:16.106291Z",
     "start_time": "2020-01-04T19:37:12.525723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  0 : 100 %\n",
      "Accuracy of  1 : 100 %\n",
      "Accuracy of  2 : 87 %\n",
      "Accuracy of  3 : 100 %\n",
      "Accuracy of  4 : 100 %\n",
      "Accuracy of  5 : 100 %\n",
      "Accuracy of  6 : 100 %\n",
      "Accuracy of  7 : 100 %\n",
      "Accuracy of  8 : 100 %\n",
      "Accuracy of  9 : 100 %\n",
      "Accuracy of CNN: 98.75\n"
     ]
    }
   ],
   "source": [
    "classes = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\")\n",
    "model.eval()\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "list_of_accuracies = []\n",
    "for i in range(10):\n",
    "    tmp_accuracy = round(100 * class_correct[i] / class_total[i], 2)\n",
    "    list_of_accuracies.append(tmp_accuracy)\n",
    "    print('Accuracy of %2s : %2d %%' % (classes[i], tmp_accuracy))\n",
    "    \n",
    "test_accuracy = round(np.mean(list_of_accuracies), 2)\n",
    "print(\"Accuracy of CNN:\", test_accuracy)\n",
    "\n",
    "# Add Test Accuracy\n",
    "method_name = \"Convolutional Neural Network\"\n",
    "values.append((method_name, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T19:38:17.175177Z",
     "start_time": "2020-01-04T19:38:17.165993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Testcase</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Baseline for Test-set (Random result)</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Train Accuracy - Logistic Regression</td>\n",
       "      <td>72.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>73.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Train Accuracy - MLP</td>\n",
       "      <td>98.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Multilayer Perceptron</td>\n",
       "      <td>97.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "      <td>98.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Testcase  Accuracy\n",
       "0  Baseline for Test-set (Random result)      0.00\n",
       "1   Train Accuracy - Logistic Regression     72.65\n",
       "2                    Logistic Regression     73.18\n",
       "3                   Train Accuracy - MLP     98.77\n",
       "4                  Multilayer Perceptron     97.04\n",
       "5           Convolutional Neural Network     98.75"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"-\" * 36)\n",
    "df = pd.DataFrame(columns=COL_NAMES, data=values)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above result, we could see that the accuracy score of method CNN is the best one and it is higher than MLP's accuracy score about 2%. \n",
    "\n",
    "In summary, in this notebook, as you see, we did:\n",
    "\n",
    "+ How to explore extensions to a baseline model to improve training and predicting capacity.\n",
    "\n",
    "+ How to use unit tests for the API and the model\n",
    "\n",
    "+ How to use unit tests for logging\n",
    "\n",
    "+ Can all of the unit tests be run with a single script and do all of the unit tests pass?\n",
    "\n",
    "+ How to monitor performance\n",
    "\n",
    "+ How to compare multiple models\n",
    "\n",
    "+ How to use visualizations for the EDA investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "218px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
